import{a as r,f as n}from"./BPxar_6W.js";import"./KFEA_jlE.js";import{I as o}from"./DXXdKfFK.js";var i=n('<h1>Skill Formation</h1> <p>How AI assistance affects learning and capability development.</p> <hr/> <h2>Sources</h2> <ul><li><a href="https://arxiv.org/pdf/2601.20245" rel="nofollow">Shen & Tamkin (2026). How AI Impacts Skill Formation. Anthropic.</a></li> <li><a href="https://www.anthropic.com/research/AI-assistance-coding-skills" rel="nofollow">Anthropic (2026). AI Assistance and Coding Skills.</a></li></ul> <hr/> <h2>Abstract</h2> <p>AI assistance produces productivity gains but impairs skill acquisition. In a randomized experiment with 52 software engineers learning a new Python library, AI users scored 17% lower on mastery assessments (d = 0.738, p = 0.01) without significant speed improvement. <span class="ev ev-moderate" title="RCT, n=52, Anthropic 2026">◐</span></p> <p>Six distinct interaction patterns emerged. Three preserved learning (65-86% scores): asking conceptual questions only, requesting explanations with code, or generating code then asking follow-up questions. Three impaired learning (24-39% scores): full delegation, progressive reliance, or iterative AI debugging.</p> <p>The control group encountered more errors and spent more time debugging — this is where learning happened. AI users who delegated finished fastest but learned least.</p> <hr/> <h2>Explanation</h2> <p>This is Anthropic’s own research confirming that <em>how</em> you use AI determines whether you learn.</p> <p><strong>The interaction patterns matter more than AI access itself.</strong></p> <table><thead><tr><th>Pattern</th><th>Score</th><th>What they did</th></tr></thead><tbody><tr><td>Generation-Then-Comprehension</td><td>86%</td><td>Generated code, then asked follow-up questions to understand it</td></tr><tr><td>Hybrid Code-Explanation</td><td>68%</td><td>Asked for explanations alongside code</td></tr><tr><td>Conceptual Inquiry</td><td>65%</td><td>Only asked conceptual questions, wrote code themselves</td></tr><tr><td>AI Delegation</td><td>39%</td><td>Asked AI to generate code, pasted it, moved on</td></tr><tr><td>Progressive Reliance</td><td>35%</td><td>Started writing code, gradually delegated more</td></tr><tr><td>Iterative Debugging</td><td>24%</td><td>Had AI fix errors repeatedly without understanding</td></tr></tbody></table> <p><strong>Why errors matter:</strong></p> <p>The control group (no AI) encountered median 3 errors vs 1 for AI users. <span class="ev ev-moderate" title="Same RCT, n=52">◐</span> Those errors — especially Trio-specific ones like TypeError and RuntimeWarning — forced understanding of how the library actually works.</p> <p>AI users who delegated never hit those errors. They completed the task without learning anything about async patterns, error handling, or the library’s execution model.</p> <p><strong>Participant self-awareness:</strong></p> <p>AI users knew something was wrong:</p> <ul><li>“I feel like I got lazy”</li> <li>“There are still a lot of gaps in my understanding”</li> <li>“I wish I’d taken the time to understand the explanations more”</li></ul> <p><strong>The implication:</strong></p> <p>Productivity and learning are not the same thing. The fastest completion (AI Delegation, 19.5 min) produced the worst learning (39%). The best learning (Generation-Then-Comprehension, 86%) took longer (24 min) but built actual capability.</p> <p>If you’re learning something new, how you interact with AI determines whether you come out more capable or just more dependent.</p>',1);function l(e){var t=i();o(46),r(e,t)}export{l as default};
