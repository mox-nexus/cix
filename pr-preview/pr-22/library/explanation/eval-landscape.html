<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../../_app/immutable/assets/0.CPJORcIU.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/CrossLinks.C4T6atsV.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/2.mFuDagjh.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/ArticleNav.D61W_PoL.css" rel="stylesheet">
		<link rel="modulepreload" href="../../_app/immutable/entry/start.CErVG62D.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DI1hKYpC.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/CDDJmPzr.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D5Xj3ZXT.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D0iwhpLH.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Cza-eoC2.js">
		<link rel="modulepreload" href="../../_app/immutable/entry/app.pJCGw2HY.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BtC_y5ln.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DzRGpBIR.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Bb8tKOVS.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/B4_FGh31.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/lNHfcz3e.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/do-Ws7ci.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Durb-G3s.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/CctXr6u4.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/0.B_unwFlx.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BWK-fzAV.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Bqd7FNqS.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/CEoIbbA0.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DJEkUMKa.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/2.CxMjylCU.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/8.G0f768n0.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/ByFkhAss.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/8OIdS8Aq.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Dp_ZLsHn.js"><!--12qhfyh--><meta name="description" content="Extensions that enhance human capability, not replace it."/><!----><!--1wa4r3o--><!----><title>Article — cix Library</title>
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><a href="#main" class="skip-link">Skip to content</a> <!--[--><nav class="site-nav svelte-qgym72" aria-label="Site navigation"><a href="../../" class="nav-wordmark svelte-qgym72">cix</a> <div class="nav-links svelte-qgym72"><!--[--><a href="../../ethos" class="nav-link svelte-qgym72">ethos</a><a href="../../catalog" class="nav-link svelte-qgym72">catalog</a><a href="../../library" class="nav-link svelte-qgym72">library</a><!--]--></div></nav><!--]--> <div class="page svelte-12qhfyh has-nav"><!--[--><!----><main id="main" class="library-layout svelte-12dzc7l"><!----><div class="article-layout svelte-1wa4r3o"><article class="library-prose svelte-1wa4r3o"><nav class="article-breadcrumb svelte-1wa4r3o"><a href="../../library" class="svelte-1wa4r3o">library</a> <span class="breadcrumb-sep svelte-1wa4r3o">/</span> <a href="../../library#explanation" class="svelte-1wa4r3o">explanation</a></nav> <!----><h1 id="understanding-the-llm-evaluation-landscape-february-2026">Understanding the LLM Evaluation Landscape (February 2026)</h1> <blockquote><p><strong>For</strong>: Someone who knows Python, has built Claude Code extensions, but hasn’t built evals before <strong>Why</strong>: To understand the full landscape before building ix (an experimentation platform) <strong>How</strong>: Start with examples, build to concepts, explain WHY things matter</p></blockquote> <hr/> <h2 id="what-this-document-is">What This Document Is</h2> <p>This is a teaching document, not a summary. It explains the evaluation landscape as of February 2026 from first principles, with examples and reasoning at every step. By the end, you’ll understand:</p> <ol><li>What evaluation actually means in the LLM world (it’s more complex than you think)</li> <li>What tools exist, what they’re good at, and what critical gaps remain</li> <li>What statistical methods matter and why (this is where most frameworks fail)</li> <li>What Anthropic specifically recommends (they’re the most relevant voice for evaluating Claude Code extensions)</li> <li>Where ix fits and what problems it uniquely solves</li></ol> <p>All claims are sourced. Traceability is critical.</p> <blockquote><p><strong>Source reports</strong> (detailed research backing this synthesis):</p> <ul><li><code>scratch/llm-eval-framework-landscape-2026-02-09.md</code> — 14 frameworks analyzed</li> <li><code>scratch/agent-eval-harness-landscape-2026-02-09.md</code> — Agent-specific tooling</li> <li><code>scratch/research-qos-experimentation-landscape-2026-02-09.md</code> — Statistical rigor and experimentation platforms</li> <li><code>scratch/anthropic-eval-research-sweep-2026-02-09.md</code> — 40+ Anthropic sources</li> <li><code>scratch/research-synthesis-ix-landscape-2026-02-09.md</code> — Unified synthesis</li></ul></blockquote> <hr/> <h2 id="part-1-what-is-evaluation-really">Part 1: What Is Evaluation, Really?</h2> <h3 id="the-simple-case-that-doesnt-exist">The Simple Case (That Doesn’t Exist)</h3> <p>Imagine you’re testing a function that adds two numbers:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> add</span><span style="color:#E1E4E8">(a, b):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> a </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> b</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">assert</span><span style="color:#E1E4E8"> add(</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 5</span><span style="color:#6A737D">  # Pass or fail. Done.</span></span></code></pre> <p>This is testing, not evaluation. The correct answer is unambiguous, deterministic, and instant to verify.</p> <p><strong>LLM evaluation is never this simple.</strong> Here’s why.</p> <h3 id="the-reality-non-determinism-judgment-and-cost">The Reality: Non-Determinism, Judgment, and Cost</h3> <p>When you ask Claude to write code, fix a bug, or answer a question, three problems emerge:</p> <p><strong>Problem 1: Non-Determinism</strong> Run the same prompt twice, get different outputs. Even with <code>temperature=0</code>, outputs vary. How do you measure quality when the answer changes?</p> <p><strong>Problem 2: Judgment Over Correctness</strong> There’s often no single “correct” answer. A bug fix might work but be inelegant. A summary might be accurate but miss the key point. Who decides what’s “good”?</p> <p><strong>Problem 3: Cost-Quality Tradeoffs</strong> An agent might get the right answer after 500 API calls costing $12. Another gets 90% accuracy in 3 calls costing $0.02. Which is better? It depends on your use case.</p> <p><strong>This is why evaluation is hard.</strong> You’re not checking correctness — you’re measuring quality, consistency, and cost across distributions of outcomes.</p> <hr/> <h2 id="part-2-the-four-dimensions-every-framework-must-handle">Part 2: The Four Dimensions Every Framework Must Handle</h2> <h3 id="dimension-1-state--does-anything-persist-between-steps">Dimension 1: State — Does Anything Persist Between Steps?</h3> <p>Consider evaluating a multi-step debugging agent:</p> <ol><li>Agent reads error message</li> <li>Agent searches codebase for the bug</li> <li>Agent proposes a fix</li> <li>Agent runs tests</li> <li>Agent iterates if tests fail</li></ol> <p>At step 3, the agent needs to remember what it found at step 2. That’s <strong>shared state</strong>.</p> <p><strong>Most frameworks treat each test as independent.</strong> They can’t check “does skill B interfere with skill A?” because they have no way to track whether adding B changed A’s internal state.</p> <p><strong>Only Inspect AI has a true state mechanism</strong> (the <code>store</code>, a Pydantic model passed through the solver chain). Every other framework reviewed treats samples as independent.</p> <p><strong>Why this matters for ix</strong>: ix needs to measure skill coexistence (“adding skill B breaks skill A”) and context degradation (“agent performance drops after 50 tool calls”). Both require tracking state across a session.</p> <p><strong>Source</strong>: LLM Framework Landscape report, “State Model” comparison matrix</p> <hr/> <h3 id="dimension-2-execution--how-are-tests-run">Dimension 2: Execution — How Are Tests Run?</h3> <p>You have 1,000 test cases. Do you run them:</p> <ul><li>One at a time (slow, but simple)</li> <li>All at once (fast, but overwhelms API rate limits)</li> <li>In batches of 20 (balanced, but requires orchestration)</li></ul> <p>And within a single test case with multiple graders, do you:</p> <ul><li>Run graders sequentially (A, then B, then C)</li> <li>Run graders in parallel (A, B, C at the same time)</li> <li>Run graders as a dependency graph (B depends on A’s output, C depends on both)</li></ul> <p><strong>Example</strong>: You’re evaluating a RAG system. You need to:</p> <ol><li>Check retrieval quality (did it fetch relevant documents?)</li> <li>Check answer accuracy (is the generated answer correct?)</li> <li>Compute a composite F1 score from #1 and #2</li></ol> <p>If graders run sequentially, this takes 3 LLM calls. If they run as a DAG, graders 1 and 2 can run in parallel, then grader 3 computes F1 from their results. That’s 2 LLM calls instead of 3.</p> <p><strong>Most frameworks run sequentially.</strong> DeepEval has a “DAG metric” (internal to a single metric), but no framework supports DAG orchestration across multiple metrics.</p> <p><strong>Why this matters for ix</strong>: ix’s probe-sensor-grader-scorer pipeline is inherently a DAG. Probes generate stimuli, sensors observe responses, graders score observations, scorers aggregate across trials. These should compose as a dependency graph, not a linear chain.</p> <p><strong>Source</strong>: LLM Framework Landscape report, “Execution Model” comparison matrix</p> <hr/> <h3 id="dimension-3-stochastic-handling--what-do-you-do-about-randomness">Dimension 3: Stochastic Handling — What Do You Do About Randomness?</h3> <p>Run the same prompt 10 times. You get 7 correct answers and 3 wrong ones. What’s your score?</p> <p><strong>Naive answer</strong>: 70% accuracy.</p> <p><strong>Better question</strong>: What’s your confidence interval? If you ran 10 more trials, would you get 60%? 80%? You need error bars.</p> <p><strong>Even better question</strong>: Does “one success out of 10 trials” matter, or do you need consistent success? These are different metrics:</p> <ul><li><strong>pass@k</strong>: Probability of at least one success in k trials. Use for code generation tools (one working solution is enough).</li> <li><strong>pass^k</strong>: Probability that all k trials succeed. Use for customer-facing agents (consistency matters).</li></ul> <p><strong>Example</strong>: An agent succeeds 75% of the time.</p> <ul><li>pass@3 = “at least one success in 3 tries” = 1 - (0.25)^3 = <strong>98.4%</strong></li> <li>pass^3 = “all 3 tries succeed” = (0.75)^3 = <strong>42.2%</strong></li></ul> <p>Same agent, wildly different performance depending on your metric.</p> <p><strong>Only two frameworks handle stochasticity properly:</strong></p> <ol><li><strong>Inspect AI</strong>: <code>epochs</code> parameter runs multiple trials per task, <code>reducers</code> aggregate scores, bootstrap computes standard errors</li> <li><strong>LM Harness</strong>: <code>bootstrap_iters</code> for confidence intervals</li></ol> <p><strong>Every other framework</strong> says “run it again and compare manually.”</p> <p><strong>Why this matters for ix</strong>: Without proper stochastic handling, you can’t tell if a 3-point score improvement is signal or noise. Anthropic’s research shows infrastructure configuration alone shifts scores by 6 percentage points — larger than most “improvements” you’ll measure.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://www.anthropic.com/engineering/infrastructure-noise" rel="nofollow">Anthropic “Infrastructure Noise”</a> — 6pp shifts from config alone</li> <li><a href="https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents" rel="nofollow">Anthropic “Demystifying Agents”</a> — pass@k vs pass^k</li></ul> <hr/> <h3 id="dimension-4-grading--who-decides-whats-good">Dimension 4: Grading — Who Decides What’s Good?</h3> <p>You have an agent that summarized a document. Is the summary good? Three approaches:</p> <p><strong>Code-based grading</strong> (deterministic):</p> <ul><li>Exact match: <code>output == expected_output</code></li> <li>ROUGE score: Overlap between output and reference</li> <li>JSON validity: <code>json.loads(output)</code> succeeds</li></ul> <p><strong>Pros</strong>: Fast, cheap, objective, reproducible <strong>Cons</strong>: Brittle to valid variations, lacks nuance</p> <p><strong>Model-based grading</strong> (LLM-as-judge):</p> <ul><li>Give Claude a rubric: “Rate this summary 1-5 on accuracy, clarity, conciseness”</li> <li>Claude grades the output</li> <li>You get scores with reasoning</li></ul> <p><strong>Pros</strong>: Flexible, scalable, captures nuance <strong>Cons</strong>: Non-deterministic, more expensive, requires calibration</p> <p><strong>Human grading</strong>:</p> <ul><li>Expert reads output, assigns score</li> <li>Gold standard quality</li></ul> <p><strong>Pros</strong>: Most accurate <strong>Cons</strong>: Expensive ($50-200/hour), slow, doesn’t scale</p> <p><strong>Anthropic’s hierarchy</strong> (use the highest tier feasible):</p> <ol><li>Code-based (if deterministic correctness exists)</li> <li>LLM-based (for nuance and subjective quality)</li> <li>Human (for calibration and spot-checks only)</li></ol> <p><strong>Critical insight from Anthropic</strong>: “Grade what the agent produced, not the path it took.” Don’t check for specific tool call sequences — agents find valid approaches designers didn’t anticipate.</p> <p><strong>Example</strong>: You’re evaluating a bug-fixing agent. Don’t check “did it call <code>git blame</code>, then <code>grep</code>, then <code>edit</code>?” Instead check “did it fix the bug?” The agent might use <code>rg</code> instead of <code>grep</code>, or skip <code>git blame</code> entirely. That’s fine.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents" rel="nofollow">Anthropic “Demystifying Agents”</a> — grader types table</li> <li><a href="https://platform.claude.com/docs/en/test-and-evaluate/develop-tests" rel="nofollow">Anthropic “Create Strong Empirical Tests”</a> — grading hierarchy</li></ul> <hr/> <h2 id="part-3-the-statistical-problem-everyone-ignores">Part 3: The Statistical Problem Everyone Ignores</h2> <h3 id="why-your-5-point-improvement-might-be-noise">Why Your 5-Point Improvement Might Be Noise</h3> <p>You run a measurement. Your agent scores 73%. You make a change. It scores 78%. Success?</p> <p><strong>Maybe. Maybe not.</strong></p> <p>Here’s what you need to know:</p> <p><strong>1. You Measured a Sample, Not the Universe</strong></p> <p>Your 1,000-question suite is a sample from an infinite universe of possible questions. The 73% you measured is an estimate of the true mean. How good is that estimate?</p> <p>Enter the <strong>Standard Error of the Mean (SEM)</strong>:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>SEM = standard_deviation / sqrt(n)</span></span></code></pre> <p>For 1,000 questions with std=0.45, SEM = 0.45 / sqrt(1000) = <strong>0.014</strong> (1.4 percentage points).</p> <p>Your <strong>95% confidence interval</strong> is:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>73% +/- 1.96 x 1.4% = 73% +/- 2.7% = [70.3%, 75.7%]</span></span></code></pre> <p>After your change, you scored 78%. That’s outside the confidence interval. <strong>Probably real.</strong></p> <p>But if you scored 75%? That’s [72.3%, 77.7%], which overlaps with the original interval. <strong>Might be noise.</strong></p> <p><strong>2. Your Questions Might Be Correlated</strong></p> <p>Imagine you’re measuring reading comprehension. You have 10 questions about one passage. A model that misunderstands the passage gets all 10 wrong. These questions are <strong>not independent</strong>.</p> <p>Standard error assumes independence. If questions cluster (reading comp, SQL injection, etc.), naive SEM <strong>underestimates uncertainty by up to 3x</strong>.</p> <p>You need <strong>clustered standard errors</strong>:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Naive SEM (wrong if clustered)</span></span>
<span class="line"><span style="color:#E1E4E8">sem_naive </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.std(scores) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> np.sqrt(</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(scores))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Clustered SEM (correct)</span></span>
<span class="line"><span style="color:#E1E4E8">cluster_means </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> df.groupby(</span><span style="color:#9ECBFF">'cluster_id'</span><span style="color:#E1E4E8">)[</span><span style="color:#9ECBFF">'score'</span><span style="color:#E1E4E8">].mean()</span></span>
<span class="line"><span style="color:#E1E4E8">sem_clustered </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.std(cluster_means) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> np.sqrt(</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(cluster_means))</span></span></code></pre> <p><strong>Evan Miller’s research</strong> (Anthropic) found clustered SEs <strong>3x larger</strong> than naive calculations on benchmarks like DROP and SQuAD.</p> <p><strong>3. Infrastructure Noise Is Real</strong></p> <p>From Anthropic’s infrastructure noise research (Feb 2026):</p> <ul><li>Changing from 1x to 3x resource headroom shifts scores by <strong>3.7 percentage points</strong> (p &lt; 0.001)</li> <li>Changing Docker config shifts scores by <strong>6 percentage points</strong> (p &lt; 0.01)</li></ul> <p><strong>Recommendation</strong>: “Leaderboard differences below 3 percentage points deserve skepticism without documented matching configurations.”</p> <p><strong>Your 5-point improvement might actually be 2 points of signal + 3 points of infrastructure noise.</strong></p> <p><strong>4. Paired Comparisons Are More Powerful</strong></p> <p>When comparing two models/agents on the same questions, don’t just compare their means. Use <strong>paired differences</strong>:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">differences </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> scores_after </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> scores_before</span></span>
<span class="line"><span style="color:#E1E4E8">mean_diff </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.mean(differences)</span></span>
<span class="line"><span style="color:#E1E4E8">se_diff </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.std(differences) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> np.sqrt(</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(differences))</span></span>
<span class="line"><span style="color:#E1E4E8">ci_diff </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> mean_diff </span><span style="color:#F97583">+/-</span><span style="color:#79B8FF"> 1.96</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> se_diff</span></span></code></pre> <p>Why? Models correlate on questions (r=0.3-0.7 on frontier measurements). Paired tests exploit that correlation for <strong>free variance reduction</strong>.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://arxiv.org/abs/2411.00640" rel="nofollow">Anthropic “Adding Error Bars”</a> — clustered SEs, paired differences, power analysis</li> <li><a href="https://www.anthropic.com/engineering/infrastructure-noise" rel="nofollow">Anthropic “Infrastructure Noise”</a> — 6pp config shifts, 3pp skepticism threshold</li></ul> <hr/> <h3 id="the-clt-is-dead-for-small-n-measurements">The CLT Is Dead for Small-N Measurements</h3> <p>The <strong>Central Limit Theorem</strong> says sample means are normally distributed for large N. Most statistics textbooks teach this.</p> <p>For LLM measurements with N &lt; 100, <strong>the CLT underestimates uncertainty</strong>. A 2025 ICML Spotlight paper (“Don’t Use the CLT”) proves this empirically across 12 benchmarks.</p> <p><strong>The fix</strong>: Use Bayesian methods. The <code>bayes_evals</code> library provides drop-in replacements for CLT-based confidence intervals.</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Wrong (CLT, underestimates uncertainty)</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> scipy </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> stats</span></span>
<span class="line"><span style="color:#E1E4E8">ci </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> stats.norm.interval(</span><span style="color:#79B8FF">0.95</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">loc</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">mean, </span><span style="color:#FFAB70">scale</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">sem)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Right (Bayesian, correct uncertainty)</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> bayes_evals </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> bayesian_ci</span></span>
<span class="line"><span style="color:#E1E4E8">ci </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> bayesian_ci(scores, </span><span style="color:#FFAB70">confidence</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.95</span><span style="color:#E1E4E8">)</span></span></code></pre> <p><strong>Why this matters</strong>: ix will run measurements with 20-50 tasks (Anthropic’s recommended starting point). The CLT breaks down completely at that scale. Bayesian methods are essential.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://arxiv.org/abs/2503.01747" rel="nofollow">“Don’t Use the CLT”</a> — ICML 2025 Spotlight</li> <li><a href="https://arxiv.org/abs/2601.20251" rel="nofollow">“Efficient Measurement with Statistical Guarantees”</a> — 5x sample efficiency via Bayesian factor models</li></ul> <hr/> <h2 id="part-4-what-frameworks-actually-exist">Part 4: What Frameworks Actually Exist</h2> <p>Now that you understand the hard parts, let’s survey the landscape. I’ll focus on what each framework is <strong>actually good at</strong>, not marketing claims.</p> <h3 id="category-1-frameworks-that-drive-tests-build-your-own">Category 1: Frameworks That Drive Tests (Build Your Own)</h3> <p>These are libraries/CLIs for defining and running evaluations.</p> <h4 id="inspect-ai-uk-ai-safety-institute">Inspect AI (UK AI Safety Institute)</h4> <p><strong>What it’s best at</strong>: Agent measurement with sandboxing and reproducibility</p> <p><strong>Architecture</strong>:</p> <ul><li><strong>Dataset</strong>: Test cases</li> <li><strong>Solver</strong>: Chain of operations (simple LLM call to complex agent)</li> <li><strong>Scorer</strong>: Grading logic</li> <li><strong>TaskState + store</strong>: Shared state across solver chain (unique among frameworks)</li></ul> <p><strong>Key capabilities</strong>:</p> <ul><li><strong>Agent Bridge</strong>: Drives external agents (Claude Code, LangChain, etc.) in Docker/K8s sandboxes</li> <li><strong>Epochs</strong>: First-class multi-trial support with reducers and bootstrap stderr</li> <li><strong>SWE-bench integration</strong>: Run Claude Code on SWE-bench in containers</li></ul> <p><strong>Stochastic handling</strong>: Best-in-class (epochs + reducers + bootstrap)</p> <p><strong>Why it’s relevant to ix</strong>: Inspect is the strongest existing foundation for agent measurement. Its TaskState + store is the only shared state mechanism. Its Agent Bridge shows how to drive Claude Code programmatically in isolated environments.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://inspect.aisi.org.uk/" rel="nofollow">Inspect AI docs</a></li> <li>Agent Harness Landscape report, “Inspect AI” section</li></ul> <hr/> <h4 id="promptfoo">Promptfoo</h4> <p><strong>What it’s best at</strong>: Declarative YAML test definitions, red-teaming, CI/CD integration</p> <p><strong>Architecture</strong>:</p> <ul><li>YAML config defines providers, prompts, test cases, assertions</li> <li>CLI runs matrix measurement (every prompt x every provider x every test)</li> <li>Built-in red-teaming (OWASP, NIST, MITRE ATLAS)</li></ul> <p><strong>Key capabilities</strong>:</p> <ul><li><strong>Claude Agent SDK provider</strong>: Dedicated integration for driving Claude Code sessions</li> <li><strong>Assertions</strong>: string match, regex, LLM-as-judge, cost, latency, ROUGE, BLEU, custom Python/JS</li> <li><strong>Side-effect management</strong>: Serial execution, hooks for env reset between trials</li></ul> <p><strong>Stochastic handling</strong>: Basic (<code>--repeat N</code>, but no CIs)</p> <p><strong>Why it’s relevant to ix</strong>: Anthropic uses Promptfoo internally for product measurements. Its YAML format could serve as a user-facing test definition layer for ix.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://www.promptfoo.dev/" rel="nofollow">Promptfoo docs</a></li> <li><a href="https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents" rel="nofollow">Anthropic “Demystifying Agents”</a> — “Anthropic uses a version for product evals”</li></ul> <hr/> <h4 id="deepeval">DeepEval</h4> <p><strong>What it’s best at</strong>: pytest-style testing with 50+ built-in metrics</p> <p><strong>Architecture</strong>:</p> <ul><li>Test cases as Python code (<code>@pytest.mark.deepeval</code>)</li> <li>50+ metrics (RAG, safety, conversational, agent)</li> <li>Optional cloud platform (Confident AI) for dataset management</li></ul> <p><strong>Key capabilities</strong>:</p> <ul><li><strong>DAG metric</strong>: Deterministic LLM-powered decision trees (most powerful metric in the ecosystem)</li> <li><strong>pytest integration</strong>: Familiar to Python developers</li> <li><strong>Concurrent execution</strong>: <code>max_concurrent=20</code> by default</li></ul> <p><strong>Stochastic handling</strong>: None (but DAG metrics are deterministic by design)</p> <p><strong>Why it’s relevant to ix</strong>: The DAG metric concept (composing LLM calls as a decision tree) is architecturally significant. It shows how to make LLM-based grading deterministic.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://deepeval.com/" rel="nofollow">DeepEval docs</a></li> <li>LLM Framework Landscape report, “DeepEval” section</li></ul> <hr/> <h4 id="lm-harness-eleutherai">LM Harness (EleutherAI)</h4> <p><strong>What it’s best at</strong>: Academic benchmarks (MMLU, HellaSwag, etc.)</p> <p><strong>Architecture</strong>:</p> <ul><li>60+ benchmarks, hundreds of subtasks</li> <li>Unified interface across HuggingFace, vLLM, APIs</li> <li>Backend for HuggingFace Open LLM Leaderboard</li></ul> <p><strong>Key capabilities</strong>:</p> <ul><li><strong>Bootstrap stderr</strong>: Mature statistical handling</li> <li><strong>Task versioning</strong>: Reproducibility of previously reported scores</li> <li><strong>Multi-backend</strong>: HuggingFace, vLLM, SGLang, OpenAI, Anthropic</li></ul> <p><strong>Stochastic handling</strong>: Excellent (bootstrap_iters for CIs)</p> <p><strong>Why it’s relevant to ix</strong>: Gold standard for model-level benchmarking. If ix ever needs to compare base model capabilities, LM Harness is the reference.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://www.eleuther.ai/artifacts/lm-evaluation-harness" rel="nofollow">LM Harness docs</a></li> <li>LLM Framework Landscape report, “LM Harness” section</li></ul> <hr/> <h3 id="category-2-platforms-that-observe--measure-saas-or-self-hosted">Category 2: Platforms That Observe + Measure (SaaS or Self-Hosted)</h3> <p>These combine tracing/observability with measurement. You instrument your code, they collect traces, you run tests against datasets.</p> <h4 id="langfuse-open-source">Langfuse (Open Source)</h4> <p><strong>What it’s best at</strong>: Self-hosted observability + measurement</p> <p><strong>Architecture</strong>:</p> <ul><li>Trace-based (traces, generations, spans, events)</li> <li>PostgreSQL backend (self-hostable Docker/K8s)</li> <li>Scores attach to traces</li></ul> <p><strong>Key capabilities</strong>:</p> <ul><li><strong>Most-starred</strong> in category (21.7k GitHub stars)</li> <li><strong>Open-source</strong> with full data ownership</li> <li><strong>50+ integrations</strong> (LangChain, LlamaIndex, OpenAI, Anthropic)</li></ul> <p><strong>Stochastic handling</strong>: Via experiment comparison (manual)</p> <p><strong>Why it’s relevant to ix</strong>: If ix needs observability, Langfuse is the best self-hosted option. But ix is an experimentation platform, not an observability tool.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://langfuse.com/" rel="nofollow">Langfuse docs</a></li> <li>LLM Framework Landscape report, “Langfuse” section</li></ul> <hr/> <h4 id="arize-phoenix">Arize Phoenix</h4> <p><strong>What it’s best at</strong>: OpenTelemetry-based observability</p> <p><strong>Architecture</strong>:</p> <ul><li>OTEL traces/spans with auto-instrumentation</li> <li>SQLite (dev) or PostgreSQL (prod)</li> <li>Runs anywhere (local, Docker, K8s, cloud)</li></ul> <p><strong>Key capabilities</strong>:</p> <ul><li><strong>Open-source</strong> (Elastic License 2.0, no feature gates)</li> <li><strong>20x speedup</strong> with built-in concurrency</li> <li><strong>Pre-built scorers</strong>: hallucination, toxicity, relevance</li></ul> <p><strong>Stochastic handling</strong>: Via experiment comparison (manual)</p> <p><strong>Why it’s relevant to ix</strong>: Phoenix shows how to integrate measurement with OTEL tracing. If ix produces traces, Phoenix could ingest them.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://phoenix.arize.com/" rel="nofollow">Arize Phoenix docs</a></li> <li>LLM Framework Landscape report, “Arize Phoenix” section</li></ul> <hr/> <h4 id="braintrust-proprietary-saas">Braintrust (Proprietary SaaS)</h4> <p><strong>What it’s best at</strong>: CI/CD integration with GitHub Actions</p> <p><strong>Architecture</strong>:</p> <ul><li>Experiments are the core organizational unit</li> <li>Git-like diffing for experiment comparison</li> <li>AutoScorers library (open-source scoring)</li></ul> <p><strong>Key capabilities</strong>:</p> <ul><li><strong>GitHub Actions</strong>: Block PRs if metrics regress</li> <li><strong>Loop AI</strong>: Generates custom scorers from natural language</li> <li><strong>AutoScorers</strong>: Levenshtein, BLEU, LLM-as-judge, pairwise</li></ul> <p><strong>Stochastic handling</strong>: Via experiment comparison (manual)</p> <p><strong>Why it’s relevant to ix</strong>: Braintrust shows the CI/CD pattern (quality gates, PR blocking). ix should support this.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://www.braintrust.dev/" rel="nofollow">Braintrust docs</a></li> <li>LLM Framework Landscape report, “Braintrust” section</li></ul> <hr/> <h3 id="category-3-benchmarks-measure-specific-capabilities">Category 3: Benchmarks (Measure Specific Capabilities)</h3> <p>These aren’t frameworks — they’re datasets with harnesses.</p> <h4 id="swe-bench-princeton">SWE-bench (Princeton)</h4> <p><strong>What it measures</strong>: Real-world GitHub bug fixes</p> <p><strong>Architecture</strong>:</p> <ul><li>2,294 tasks from 12 Python repos (original)</li> <li>500 human-verified subset (Verified)</li> <li>Dockerized harness: apply patch, run FAIL_TO_PASS + PASS_TO_PASS tests</li></ul> <p><strong>Current performance</strong>: ~75% on Verified (Jan 2026)</p> <p><strong>Why it’s relevant to ix</strong>: SWE-bench is the de facto standard for coding agent measurement. Inspect AI has SWE-bench integration. ix should too.</p> <p><strong>Source</strong>:</p> <ul><li><a href="https://arxiv.org/abs/2310.06770" rel="nofollow">SWE-bench paper</a></li> <li>Agent Harness Landscape report, “SWE-bench Family” section</li></ul> <hr/> <h4 id="webarena--agentbench--gaia">WebArena / AgentBench / GAIA</h4> <p><strong>What they measure</strong>:</p> <ul><li><strong>WebArena</strong>: Web agent tasks (e-commerce, forums, code)</li> <li><strong>AgentBench</strong>: 8 environments (OS, database, gaming, embodied AI)</li> <li><strong>GAIA</strong>: 466 human-curated multi-step reasoning tasks</li></ul> <p><strong>Current performance</strong>:</p> <ul><li>WebArena: 14% -> 60% (2023-2025)</li> <li>GAIA: 77% human-AI performance gap</li></ul> <p><strong>Why they’re relevant to ix</strong>: These show task complexity that exceeds current capabilities. If ix measures agents, these benchmarks are the ceiling.</p> <p><strong>Source</strong>:</p> <ul><li>Agent Harness Landscape report, “Academic Agent Benchmarks” section</li></ul> <hr/> <h2 id="part-5-what-anthropic-specifically-recommends">Part 5: What Anthropic Specifically Recommends</h2> <p>Anthropic is the most relevant voice for ix (measuring Claude Code extensions). Here’s their unified guidance extracted from 40+ sources.</p> <h3 id="core-terminology-anthropics-framework">Core Terminology (Anthropic’s Framework)</h3> <p>From <a href="https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents" rel="nofollow">“Demystifying Agents”</a>:</p> <ul><li><strong>Task</strong>: Single test with inputs + success criteria</li> <li><strong>Trial</strong>: One attempt at a task (multiple trials handle stochasticity)</li> <li><strong>Grader</strong>: Logic scoring agent performance</li> <li><strong>Transcript/Trace</strong>: Complete record (outputs, tool calls, reasoning)</li> <li><strong>Outcome</strong>: Final environment state (not agent’s claim)</li> <li><strong>Harness</strong>: Infrastructure running measurements end-to-end</li></ul> <p>This is the standard terminology. Use it.</p> <hr/> <h3 id="the-zero-to-one-roadmap-8-steps">The Zero-to-One Roadmap (8 Steps)</h3> <p>Anthropic’s practical guide from <a href="https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents" rel="nofollow">“Demystifying Agents”</a>:</p> <p><strong>0. Start early</strong>: 20-50 simple tasks from real failures = excellent starting point</p> <p><strong>1. Convert manual tests first</strong>: You already have manual checks. Automate them.</p> <p><strong>2. Write unambiguous tasks</strong>: Two domain experts should independently reach same pass/fail. If 0% pass@100, the task is broken (not the agent).</p> <p><strong>3. Build balanced problem sets</strong>: Test positive and negative cases. Avoid class imbalance.</p> <p><strong>4. Build robust harness</strong>: Each trial from clean environment. Prevent shared state, correlated failures.</p> <p><strong>5. Design graders thoughtfully</strong>:</p> <ul><li>Grade outcome, not path</li> <li>Support partial credit</li> <li>Calibrate LLM graders with humans</li> <li>Grade each dimension separately</li></ul> <p><strong>6. Check the transcripts</strong>: Scores without transcript analysis are unreliable</p> <p><strong>7. Monitor saturation</strong>: SWE-bench went 30% -> 80% in one year. Saturated benchmarks provide no improvement signal.</p> <p><strong>8. Keep suites healthy</strong>: Treat like unit tests. Dedicated teams own infrastructure.</p> <hr/> <h3 id="statistical-methods-from-evan-millers-research">Statistical Methods (From Evan Miller’s Research)</h3> <p>From <a href="https://arxiv.org/abs/2411.00640" rel="nofollow">“Adding Error Bars”</a>:</p> <p><strong>Always report</strong>:</p> <ul><li>Mean score</li> <li>Standard error (SEM)</li> <li>95% confidence interval: <code>mean +/- 1.96 x SEM</code></li></ul> <p><strong>Use clustered SEs</strong> when questions are non-independent (can be 3x larger than naive):</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Group by cluster (e.g., passage_id for reading comp)</span></span>
<span class="line"><span style="color:#E1E4E8">cluster_means </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> df.groupby(</span><span style="color:#9ECBFF">'cluster_id'</span><span style="color:#E1E4E8">)[</span><span style="color:#9ECBFF">'score'</span><span style="color:#E1E4E8">].mean()</span></span>
<span class="line"><span style="color:#E1E4E8">sem_clustered </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> cluster_means.std() </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> np.sqrt(</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(cluster_means))</span></span></code></pre> <p><strong>Use power analysis</strong> to determine sample sizes:</p> <ul><li>Small effects require large N</li> <li>Frontier model differences are often 2-5 percentage points</li> <li>Need 500+ samples to detect 3pp difference with 80% power</li></ul> <p><strong>Use paired differences</strong> when comparing models:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">differences </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> scores_model_b </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> scores_model_a</span></span>
<span class="line"><span style="color:#E1E4E8">mean_diff </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> differences.mean()</span></span>
<span class="line"><span style="color:#E1E4E8">se_diff </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> differences.std() </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> np.sqrt(</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(differences))</span></span></code></pre> <hr/> <h3 id="infrastructure-configuration-matters">Infrastructure Configuration Matters</h3> <p>From <a href="https://www.anthropic.com/engineering/infrastructure-noise" rel="nofollow">“Infrastructure Noise”</a>:</p> <p><strong>Key findings</strong>:</p> <ul><li>Resource allocation shifts scores by <strong>6 percentage points</strong></li> <li>Differences below <strong>3pp</strong> deserve skepticism without documented configs</li> <li>Use dual parameters: guaranteed allocation floor + hard kill threshold</li></ul> <p><strong>Recommendations</strong>:</p> <ul><li>Specify floor and ceiling, not single pinned value</li> <li>Run measurements at multiple times across multiple days</li> <li>Treat resource config as first-class experimental variable</li></ul> <hr/> <h3 id="grader-design-principles">Grader Design Principles</h3> <p>From <a href="https://platform.claude.com/docs/en/test-and-evaluate/develop-tests" rel="nofollow">“Create Strong Empirical Tests”</a>:</p> <p><strong>Hierarchy</strong> (use highest tier feasible):</p> <ol><li><strong>Code-based</strong>: Exact match, string match, regex. Fastest, most reliable.</li> <li><strong>LLM-based</strong>: Rubric scoring, binary classification. Fast, flexible, scalable.</li> <li><strong>Human</strong>: Expert review. Most accurate but expensive — use for calibration only.</li></ol> <p><strong>LLM grader best practices</strong>:</p> <ul><li>Use different model to grade than model that generated output</li> <li>Encourage reasoning: “Think first, then decide. Discard reasoning.”</li> <li>Provide “way out”: Return “Unknown” when lacking information</li> <li>Grade each dimension separately (not monolithic)</li> <li>Create clear rubrics with specific criteria</li> <li>Calibrate with human experts regularly</li></ul> <hr/> <h3 id="metrics-selection">Metrics Selection</h3> <p>From <a href="https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents" rel="nofollow">“Demystifying Agents”</a>:</p> <table><thead><tr><th>Situation</th><th>Metric</th><th>Formula</th><th>Why</th></tr></thead><tbody><tr><td>One success matters (coding)</td><td><strong>pass@k</strong></td><td>1 - (1-p)^k</td><td>Probability of at least one success</td></tr><tr><td>Consistency matters (customer-facing)</td><td><strong>pass^k</strong></td><td>p^k</td><td>Probability all trials succeed</td></tr><tr><td>Semantic similarity</td><td><strong>Cosine similarity</strong></td><td>SBERT embeddings</td><td>Captures meaning, not surface form</td></tr><tr><td>Summarization</td><td><strong>ROUGE-L</strong></td><td>LCS F1</td><td>Longest common subsequence</td></tr><tr><td>Subjective quality</td><td><strong>LLM-based Likert</strong></td><td>1-5 scale</td><td>Captures nuance</td></tr><tr><td>Binary classification</td><td><strong>LLM-based binary</strong></td><td>Yes/no</td><td>Context-aware</td></tr></tbody></table> <p><strong>Critical</strong>: (0.75)^3 = 42% (pass^3) vs 1 - (0.25)^3 = 98% (pass@3). Same agent, wildly different numbers.</p> <hr/> <h3 id="types-capability-vs-regression">Types: Capability vs Regression</h3> <p>From <a href="https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents" rel="nofollow">“Demystifying Agents”</a>:</p> <p><strong>Capability tests</strong>: “What can this do?”</p> <ul><li>Start at low pass rate</li> <li>Hills to climb</li> <li>Measure progress</li></ul> <p><strong>Regression tests</strong>: “Does it still work?”</p> <ul><li>Must maintain ~100% pass rate</li> <li>Protect against backsliding</li> <li>Catch breaking changes</li></ul> <p><strong>Graduation</strong>: High-pass capability tests graduate to regression suites.</p> <hr/> <h3 id="behavioral-measurement-bloom-architecture">Behavioral Measurement (Bloom Architecture)</h3> <p>From <a href="https://alignment.anthropic.com/2025/bloom-auto-evals/" rel="nofollow">Bloom research</a>:</p> <p><strong>Four-stage pipeline</strong>:</p> <ol><li><strong>Understanding</strong>: Agent reads behavior description, generates understanding of what to measure</li> <li><strong>Ideation</strong>: Generates scenarios designed to elicit target behavior</li> <li><strong>Rollout</strong>: Executes scenarios in parallel with simulated users and tools</li> <li><strong>Judgment</strong>: Judge model scores transcripts; meta-judge produces suite report</li></ol> <p><strong>Why it’s relevant to ix</strong>: Bloom’s pipeline (Understand -> Ideate -> Rollout -> Judge) maps to ix’s probe-sensor-grader-scorer ontology. Scenario generation is powerful.</p> <p><strong>Source</strong>: <a href="https://github.com/safety-research/bloom" rel="nofollow">Bloom GitHub</a></p> <hr/> <h3 id="the-swiss-cheese-model">The Swiss Cheese Model</h3> <p>From <a href="https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents" rel="nofollow">“Demystifying Agents”</a>:</p> <p><strong>No single method catches everything.</strong> Combine:</p> <ul><li>Automated tests (scale)</li> <li>Production monitoring (real user patterns)</li> <li>A/B testing (causal impact)</li> <li>User feedback (qualitative)</li> <li>Manual transcript review (deep understanding)</li> <li>Systematic human studies (gold standard)</li></ul> <p>Each layer has holes. Layering catches more.</p> <hr/> <h3 id="frameworks-anthropic-uses-internally">Frameworks Anthropic Uses Internally</h3> <p>From <a href="https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents" rel="nofollow">“Demystifying Agents”</a>:</p> <p><strong>Primary</strong>:</p> <ul><li><strong>Promptfoo</strong>: “Anthropic uses a version for product evals”</li> <li><strong>Harbor</strong>: Containerized agent environments (internal tool)</li></ul> <p><strong>Recommended</strong>:</p> <ul><li>Braintrust (offline + production, CI/CD)</li> <li>LangSmith (tracing, datasets, LangChain)</li> <li>Langfuse (self-hosted alternative)</li></ul> <hr/> <h2 id="part-6-the-critical-gaps-ix-must-fill">Part 6: The Critical Gaps ix Must Fill</h2> <p>Now that you understand the landscape, here’s what <strong>doesn’t exist</strong>.</p> <h3 id="gap-1-skill-activation-measurement">Gap 1: Skill Activation Measurement</h3> <p><strong>Question</strong>: “Given this context, does the agent activate the right skill/plugin?”</p> <p><strong>What exists</strong>: OpenAI’s blog post <a href="https://developers.openai.com/blog/eval-skills/" rel="nofollow">“Testing Agent Skills Systematically”</a> describes the pattern (use structured JSON output to verify skill invocation). But there’s no tooling.</p> <p><strong>Why it matters</strong>: You’re building a Claude Code skill marketplace. Users need to know “does the security review skill actually trigger when I paste vulnerable code?”</p> <p><strong>What ix provides</strong>:</p> <ul><li><strong>PromptProbe</strong>: Generates test inputs designed to trigger specific skills</li> <li><strong>ActivationSensor</strong>: Observes which skills activated</li> <li><strong>MatcherSensor</strong> (puma): Matches actual vs expected activations</li> <li><strong>ClassificationScorer</strong>: F1 score across activation cases</li></ul> <p><strong>Example</strong>:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Test case: Should this trigger the security-review skill?</span></span>
<span class="line"><span style="color:#E1E4E8">test_case </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#9ECBFF">    "input"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Review this code: cursor.execute(f'SELECT * FROM users WHERE id = </span><span style="color:#79B8FF">{user_id}</span><span style="color:#9ECBFF">')"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">    "expected_skills"</span><span style="color:#E1E4E8">: [</span><span style="color:#9ECBFF">"security-review"</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#9ECBFF">    "expected_findings"</span><span style="color:#E1E4E8">: [</span><span style="color:#9ECBFF">"SQL injection vulnerability"</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Probe generates input, sensor observes, scorer computes F1</span></span>
<span class="line"><span style="color:#E1E4E8">result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.run_activation_test(</span><span style="color:#FFAB70">test_cases</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[test_case])</span></span>
<span class="line"><span style="color:#6A737D"># Result: precision=0.92, recall=0.88, F1=0.90</span></span></code></pre> <p><strong>Source</strong>: Research Synthesis report, “Five Confirmed Gaps” section</p> <hr/> <h3 id="gap-2-coexistence--interference-measurement">Gap 2: Coexistence / Interference Measurement</h3> <p><strong>Question</strong>: “Does adding skill B change skill A’s behavior?”</p> <p><strong>What exists</strong>:</p> <ul><li><strong>CooperBench</strong> (Jan 2026): Multi-agent cooperation benchmark. Found frontier models achieve only 25% success when two agents collaborate — half the single-agent rate.</li> <li><strong>SWE-EVO</strong>: Measures plasticity (learning new tasks) and stability (retaining old tasks) with CL-F1 score.</li></ul> <p>But these measure task-level degradation, not skill-level interference within a single agent.</p> <p><strong>Why it matters</strong>: You add a “explain-code” skill to an agent that already has “write-tests”. Does the agent now over-explain instead of writing tests? This is invisible to single-skill tests.</p> <p><strong>What ix provides</strong>:</p> <ul><li><strong>Comparative experiment</strong>: Control (skill A alone) vs variant (skills A+B together)</li> <li><strong>ActivationSensor</strong>: Measures whether A’s activation changes when B is present</li> <li><strong>DeltaScorer</strong>: Computes delta F1, delta accuracy between control and variant</li></ul> <p><strong>Example</strong>:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Control: security-review skill alone</span></span>
<span class="line"><span style="color:#E1E4E8">control </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.Experiment(</span><span style="color:#FFAB70">skills</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"security-review"</span><span style="color:#E1E4E8">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Variant: security-review + code-optimization</span></span>
<span class="line"><span style="color:#E1E4E8">variant </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.Experiment(</span><span style="color:#FFAB70">skills</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"security-review"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"code-optimization"</span><span style="color:#E1E4E8">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Run same test cases through both</span></span>
<span class="line"><span style="color:#E1E4E8">result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.run_coexistence_test(control, variant, test_cases)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Result: security-review F1 dropped from 0.90 to 0.78 when code-optimization added</span></span>
<span class="line"><span style="color:#6A737D"># Interference detected: code-optimization generates explanations that suppress security warnings</span></span></code></pre> <p><strong>Source</strong>:</p> <ul><li><a href="https://arxiv.org/abs/2601.13295" rel="nofollow">CooperBench paper</a> — 75% cooperation failure rate</li> <li>Research Synthesis report, “Five Confirmed Gaps” section</li></ul> <hr/> <h3 id="gap-3-context-degradation-measurement">Gap 3: Context Degradation Measurement</h3> <p><strong>Question</strong>: “How does agent behavior degrade after N tool calls / crowded context?”</p> <p><strong>What exists</strong>: Nothing. Phil Schmid’s January 2026 thesis identified this as the critical gap for 2026:</p> <blockquote><p>“Task durability — how well a model follows instructions after its 50th or 100th tool call — is what matters but isn’t measured. A 1% leaderboard difference can’t detect reliability drift.”</p></blockquote> <p>Anthropic’s <a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents" rel="nofollow">“Effective Harnesses for Long-Running Agents”</a> describes the problem (longer context often makes things worse) but doesn’t provide tooling.</p> <p><strong>Why it matters</strong>: Your agent works great for the first 10 interactions. After 50 tool calls, it starts hallucinating. Standard benchmarks run 1-5 tool calls and declare victory.</p> <p><strong>What ix provides</strong>:</p> <ul><li><strong>PromptProbe with escalating context load</strong>: Systematically increases tool calls, context size</li> <li><strong>TimingSensor</strong>: Tracks latency as context grows</li> <li><strong>ActivationSensor</strong>: Measures whether correct skills still trigger at step 50</li> <li><strong>DegradationScorer</strong>: Plots accuracy, latency, activation F1 vs context size</li></ul> <p><strong>Example</strong>:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Test case: Does the agent maintain accuracy after 100 tool calls?</span></span>
<span class="line"><span style="color:#E1E4E8">degradation_test </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.ContextDegradationTest(</span></span>
<span class="line"><span style="color:#FFAB70">    skill</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"code-review"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    max_tool_calls</span><span style="color:#F97583">=</span><span style="color:#79B8FF">100</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    checkpoints</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">25</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">50</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">75</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">100</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.run_degradation_test(degradation_test)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Result: Accuracy at step 10: 92%, step 50: 78%, step 100: 61%</span></span>
<span class="line"><span style="color:#6A737D"># Context degradation detected: Agent loses focus after 50 calls</span></span></code></pre> <p><strong>Source</strong>:</p> <ul><li><a href="https://www.philschmid.de/agent-harness-2026" rel="nofollow">Phil Schmid “Agent Harness 2026”</a></li> <li>Research Synthesis report, “Five Confirmed Gaps” section</li></ul> <hr/> <h3 id="gap-4-dag-based-experiment-orchestration">Gap 4: DAG-Based Experiment Orchestration</h3> <p><strong>Question</strong>: “Can probes, sensors, scorers, reporters compose as a dependency graph?”</p> <p><strong>What exists</strong>:</p> <ul><li><strong>DeepEval</strong> has DAG metrics (scoring logic as a decision tree within a single metric)</li> <li><strong>Dagster</strong> has DAG orchestration (workflow management for data pipelines)</li> <li><strong>Nobody combines them</strong> for experimentation</li></ul> <p><strong>Why it matters</strong>: Your experiment has:</p> <ol><li>Retrieval sensor (measures doc retrieval quality)</li> <li>Generation sensor (measures answer quality)</li> <li>F1 scorer (computes F1 from retrieval precision + recall)</li> <li>Cost sensor (tracks API spend)</li> <li>Composite scorer (aggregates quality + cost as Pareto curve)</li></ol> <p>This is a dependency graph: Scorers 3 and 5 depend on sensors 1, 2, 4. Running them sequentially wastes time. Running them as a DAG is parallel where possible.</p> <p><strong>What ix provides</strong>:</p> <ul><li><strong>Matrix engine</strong>: Kind-agnostic DAG with parallel dispatch via Python’s <code>graphlib</code></li> <li><strong>Typed components</strong>: Probes, Sensors, Graders, Scorers, Reporters compose as nodes</li> <li><strong>Automatic parallelization</strong>: Independent branches execute concurrently</li></ul> <p><strong>Example</strong>:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Define DAG</span></span>
<span class="line"><span style="color:#E1E4E8">dag </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.ExperimentDAG()</span></span>
<span class="line"><span style="color:#E1E4E8">dag.add_probe(</span><span style="color:#9ECBFF">"prompt_probe"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">dag.add_sensor(</span><span style="color:#9ECBFF">"retrieval_sensor"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"prompt_probe"</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#E1E4E8">dag.add_sensor(</span><span style="color:#9ECBFF">"generation_sensor"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"prompt_probe"</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#E1E4E8">dag.add_scorer(</span><span style="color:#9ECBFF">"f1_scorer"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"retrieval_sensor"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"generation_sensor"</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#E1E4E8">dag.add_reporter(</span><span style="color:#9ECBFF">"console_reporter"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"f1_scorer"</span><span style="color:#E1E4E8">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Execute (automatic parallelization)</span></span>
<span class="line"><span style="color:#E1E4E8">result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.run(dag)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># retrieval_sensor and generation_sensor run in parallel</span></span>
<span class="line"><span style="color:#6A737D"># f1_scorer waits for both, then runs</span></span>
<span class="line"><span style="color:#6A737D"># console_reporter waits for f1_scorer, then prints</span></span></code></pre> <p><strong>Source</strong>: Research Synthesis report, “Five Confirmed Gaps” section</p> <hr/> <h3 id="gap-5-collaborative-intelligence-measurement">Gap 5: Collaborative Intelligence Measurement</h3> <p><strong>Question</strong>: “Does the extension make the human more capable, not dependent?”</p> <p><strong>What exists</strong>: Nothing. This requires longitudinal, human-in-the-loop measurement — a fundamentally different paradigm.</p> <p><strong>Why it matters</strong>: From the <a href="the-evidence.md">evidence base</a>:</p> <blockquote><p>“AI reliably improves immediate task performance while simultaneously degrading the cognitive foundations that enable long-term human capability.”</p> <ul><li>26% more tasks completed with AI (productivity gains are real)</li> <li>17% worse exam performance without AI (learning harm is real)</li> <li>20% skill reduction after 3 months (deskilling is measurable)</li></ul></blockquote> <p>Standard tests measure “does the AI produce good output?” Collaborative intelligence measurement asks “does the human get better over time?”</p> <p><strong>What ix provides</strong>: Not yet. This is M3+ territory. But ix’s architecture enables it:</p> <ul><li><strong>Longitudinal trials</strong>: Track human performance with and without agent over weeks</li> <li><strong>Transfer tests</strong>: After using agent for N tasks, measure human capability alone</li> <li><strong>Learning curves</strong>: Plot human skill acquisition vs agent usage intensity</li></ul> <p><strong>The hardest gap.</strong></p> <p><strong>Source</strong>:</p> <ul><li><a href="the-evidence.md">The Evidence</a> — CI research evidence base</li> <li>Research Synthesis report, “Five Confirmed Gaps” section</li></ul> <hr/> <h2 id="part-7-where-ix-fits-in-the-landscape">Part 7: Where ix Fits in the Landscape</h2> <h3 id="the-differentiation-map">The Differentiation Map</h3> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>                    Statistical Rigor</span></span>
<span class="line"><span>                         |</span></span>
<span class="line"><span>          Statsig *      |      * ix (target)</span></span>
<span class="line"><span>                         |     /</span></span>
<span class="line"><span>                         |    /</span></span>
<span class="line"><span>          Braintrust *   |   /</span></span>
<span class="line"><span>                         |  /</span></span>
<span class="line"><span>     --------------------+---------------------- Agent Testing</span></span>
<span class="line"><span>                         | /</span></span>
<span class="line"><span>          Promptfoo *    |/</span></span>
<span class="line"><span>                         |</span></span>
<span class="line"><span>          DeepEval *     |     * Inspect AI</span></span>
<span class="line"><span>                         |</span></span>
<span class="line"><span>                         |</span></span>
<span class="line"><span>                    Simple Testing</span></span></code></pre> <p><strong>ix lives in the upper-right quadrant</strong>: agent testing + statistical rigor + DAG orchestration.</p> <p>Nobody else is there.</p> <p><strong>Source</strong>: Research Synthesis report, “The ix Differentiation Map”</p> <hr/> <h3 id="what-ix-should-borrow">What ix Should Borrow</h3> <p>From the research synthesis:</p> <table><thead><tr><th>From</th><th>Take</th><th>Why</th></tr></thead><tbody><tr><td><strong>Inspect AI</strong></td><td>TaskState + store, epochs/reducers, async execution</td><td>Only framework with shared state and first-class stochastic handling</td></tr><tr><td><strong>LM Harness</strong></td><td>Bootstrap stderr, metric aggregation</td><td>Mature statistical infrastructure</td></tr><tr><td><strong>Promptfoo</strong></td><td>YAML test definitions, Claude Agent SDK provider</td><td>Best declarative DX, Anthropic uses it</td></tr><tr><td><strong>DeepEval</strong></td><td>DAG metric concept</td><td>Shows how to compose LLM calls as decision trees</td></tr><tr><td><strong>Anthropic</strong></td><td>Task/trial/transcript/outcome framework, pass@k</td><td>Canonical terminology and methodology</td></tr><tr><td><strong>Statsig</strong></td><td>CUPED variance reduction, sequential testing</td><td>Reduces sample size requirements 2-5x</td></tr><tr><td><strong>Bloom</strong></td><td>Understand -> Ideate -> Rollout -> Judge pipeline</td><td>Architecture for behavioral measurement generation</td></tr></tbody></table> <hr/> <h3 id="what-ix-should-not-build">What ix Should NOT Build</h3> <p>From the research synthesis:</p> <ul><li><strong>Observability</strong>: Langfuse, Opik, Phoenix already do this. ix produces results; they visualize.</li> <li><strong>Red-teaming</strong>: Promptfoo owns this (50+ vulnerability types, OWASP/NIST/MITRE).</li> <li><strong>Model benchmarking</strong>: LM Harness owns this (60+ benchmarks, HuggingFace leaderboard).</li> <li><strong>Prompt optimization</strong>: DSPy owns this. ix measures; DSPy optimizes.</li></ul> <p><strong>ix is an experimentation platform for cognitive extensions.</strong> Stay in that lane.</p> <hr/> <h3 id="the-ix-architecture-stack">The ix Architecture Stack</h3> <p>Based on the gaps and borrowing strategy:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>ix Experimentation Platform</span></span>
<span class="line"><span>+-- Core Engine</span></span>
<span class="line"><span>|   +-- Matrix (DAG orchestration via graphlib)</span></span>
<span class="line"><span>|   +-- Goal (experiment definition)</span></span>
<span class="line"><span>|   +-- Experiment (SLICK Workflow with state machine)</span></span>
<span class="line"><span>|</span></span>
<span class="line"><span>+-- SLICK Components (ontology)</span></span>
<span class="line"><span>|   +-- Probe (stimulus generation) -- SLICK Capability: tool</span></span>
<span class="line"><span>|   +-- Sensor (observation) -- SLICK Capability: resource</span></span>
<span class="line"><span>|   +-- Grader (scoring) -- domain-specific adapters for puma matchers</span></span>
<span class="line"><span>|   +-- Scorer (aggregation) -- F1, pass@k, distributions</span></span>
<span class="line"><span>|   +-- Reporter (output) -- console, JSON, HTML</span></span>
<span class="line"><span>|</span></span>
<span class="line"><span>+-- Statistical Layer (Bayesian, not CLT)</span></span>
<span class="line"><span>|   +-- bayes_evals library (drop-in replacement for CLT)</span></span>
<span class="line"><span>|   +-- Clustered standard errors</span></span>
<span class="line"><span>|   +-- Paired differences</span></span>
<span class="line"><span>|   +-- Power analysis</span></span>
<span class="line"><span>|</span></span>
<span class="line"><span>+-- Agent Harness (drive Claude Code)</span></span>
<span class="line"><span>|   +-- Inspect AI Agent Bridge (sandbox in Docker/K8s)</span></span>
<span class="line"><span>|   +-- Claude Agent SDK provider (Promptfoo pattern)</span></span>
<span class="line"><span>|   +-- Session management (resumable, isolated trials)</span></span>
<span class="line"><span>|</span></span>
<span class="line"><span>+-- Persistence (DuckDB)</span></span>
<span class="line"><span>|   +-- Trial-level results (scores, latency, cost, tokens)</span></span>
<span class="line"><span>|   +-- Historical tracking (regression detection over time)</span></span>
<span class="line"><span>|   +-- Export to observability platforms (Langfuse, Phoenix)</span></span>
<span class="line"><span>|</span></span>
<span class="line"><span>+-- Quality Gates (CI/CD)</span></span>
<span class="line"><span>    +-- Threshold checks (block PR if F1 &lt; 0.8)</span></span>
<span class="line"><span>    +-- Regression detection (alert if performance drops >5%)</span></span>
<span class="line"><span>    +-- Cost limits (fail if cost > $X per trial)</span></span></code></pre> <hr/> <h2 id="part-8-key-research-papers-that-change-how-we-think">Part 8: Key Research Papers That Change How We Think</h2> <p>These aren’t just citations — they fundamentally shift how you design experiments.</p> <h3 id="1-adding-error-bars-anthropic-nov-2024">1. “Adding Error Bars” (Anthropic, Nov 2024)</h3> <p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2411.00640" rel="nofollow">arXiv:2411.00640</a> <strong>Blog</strong>: <a href="https://www.anthropic.com/research/statistical-approach-to-model-evals" rel="nofollow">Statistical Approach to Model Measurements</a></p> <p><strong>Key contribution</strong>: Clustered standard errors can be <strong>3x larger</strong> than naive calculations. Most frameworks report naive SEM — wildly overconfident.</p> <p><strong>Actionable</strong>:</p> <ul><li>Always use clustered SEs when questions are non-independent</li> <li>Report paired differences with correlation coefficients</li> <li>Use power analysis to determine sample sizes</li></ul> <p><strong>Before this paper</strong>: “My agent scores 78% +/- 1%.” <strong>After this paper</strong>: “My agent scores 78% +/- 3% (clustered SE), CI [72%, 84%]. Paired difference vs baseline: +5% +/- 2%, CI [1%, 9%], correlation r=0.54.”</p> <hr/> <h3 id="2-dont-use-the-clt-icml-2025-spotlight">2. “Don’t Use the CLT” (ICML 2025 Spotlight)</h3> <p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2503.01747" rel="nofollow">arXiv:2503.01747</a></p> <p><strong>Key contribution</strong>: Central Limit Theorem underestimates uncertainty for N &lt; 100. Bayesian methods correct this.</p> <p><strong>Actionable</strong>: Use <code>bayes_evals</code> library for all confidence intervals in small-N settings (which is most experiments).</p> <p><strong>Before this paper</strong>: “I have 50 samples, CLT applies, SEM = sigma / sqrt(50).” <strong>After this paper</strong>: “I have 50 samples, CLT breaks down, use Bayesian credible intervals instead.”</p> <hr/> <h3 id="3-signal-and-noise-allen-ai-neurips-2025">3. “Signal and Noise” (Allen AI, NeurIPS 2025)</h3> <p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2508.13144" rel="nofollow">arXiv:2508.13144</a></p> <p><strong>Key contribution</strong>: Signal-to-noise ratio as a meta-metric for benchmark quality. If SNR &lt; 2, the benchmark doesn’t reliably measure what you think.</p> <p><strong>Actionable</strong>: Before investing effort in a benchmark, compute its SNR:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Signal = variance across models</span></span>
<span class="line"><span style="color:#6A737D"># Noise = variance within model (across trials)</span></span>
<span class="line"><span style="color:#79B8FF">SNR</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> var_between_models </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> var_within_model</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">if</span><span style="color:#79B8FF"> SNR</span><span style="color:#F97583"> &lt;</span><span style="color:#79B8FF"> 2</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"Benchmark is too noisy, redesign or get more samples"</span><span style="color:#E1E4E8">)</span></span></code></pre> <p><strong>Before this paper</strong>: “My test suite has high variance. I need more samples.” <strong>After this paper</strong>: “High variance might be high signal (models differ a lot) or high noise (trials differ a lot). SNR tells me which.”</p> <hr/> <h3 id="4-ai-agents-that-matter-princeton-jul-2024">4. “AI Agents That Matter” (Princeton, Jul 2024)</h3> <p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2407.01502" rel="nofollow">arXiv:2407.01502</a></p> <p><strong>Key contribution</strong>: Always report cost alongside accuracy. Pareto curves reveal the real tradeoffs.</p> <p><strong>Example</strong>: LATS costs 50x more than baselines but papers don’t report this. When you add cost to the graph, LATS is Pareto-dominated by cheaper methods.</p> <p><strong>Actionable</strong>: Every ix scorer must track:</p> <ul><li>Quality metrics (accuracy, F1, ROUGE, etc.)</li> <li>Cost (API spend, latency, token count)</li> <li>Pareto frontier (max quality per dollar)</li></ul> <p><strong>Before this paper</strong>: “Agent A: 85% accuracy. Agent B: 82% accuracy. A wins.” <strong>After this paper</strong>: “Agent A: 85% @ $2.50/task. Agent B: 82% @ $0.03/task. B wins for production, A wins for research.”</p> <hr/> <h3 id="5-cooperbench-jan-2026">5. CooperBench (Jan 2026)</h3> <p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2601.13295" rel="nofollow">arXiv:2601.13295</a></p> <p><strong>Key contribution</strong>: First benchmark measuring multi-agent cooperation. Frontier models achieve only <strong>25% success</strong> when two agents collaborate — roughly half single-agent rate.</p> <p><strong>Three failure modes</strong>:</p> <ol><li>Agents fail to communicate actionable information</li> <li>Agents deviate from commitments</li> <li>Agents hold incorrect expectations about partners</li></ol> <p><strong>Actionable for ix</strong>: Coexistence tests must check for these failure modes. When skill B is added:</p> <ul><li>Does skill A receive actionable information from B?</li> <li>Does A’s behavior change when B is present?</li> <li>Do A and B have correct expectations about each other’s activation?</li></ul> <hr/> <h3 id="6-swe-evo-jan-2026">6. SWE-EVO (Jan 2026)</h3> <p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2512.18470" rel="nofollow">arXiv:2512.18470</a></p> <p><strong>Key contribution</strong>: Introduces <strong>CL-F1</strong> (continual learning F1):</p> <ul><li><strong>CL-Plasticity</strong>: Ability to learn new tasks</li> <li><strong>CL-Stability</strong>: Retention of performance on previous tasks</li> <li><strong>CL-F1</strong>: Harmonic mean of plasticity and stability</li></ul> <p><strong>Actionable for ix</strong>: Coexistence testing is a special case of continual learning:</p> <ul><li>Plasticity = “can the agent use skill B?”</li> <li>Stability = “does skill A still work after adding B?”</li> <li>CL-F1 = overall coexistence score</li></ul> <p><strong>Example</strong>:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Before: security-review skill scores F1=0.90</span></span>
<span class="line"><span style="color:#6A737D"># After adding code-optimization skill:</span></span>
<span class="line"><span style="color:#E1E4E8">plasticity </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.92</span><span style="color:#6A737D">  # code-optimization works well</span></span>
<span class="line"><span style="color:#E1E4E8">stability </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.78</span><span style="color:#6A737D">   # security-review degraded</span></span>
<span class="line"><span style="color:#E1E4E8">cl_f1 </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 2</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> (plasticity </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> stability) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> (plasticity </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> stability)  </span><span style="color:#6A737D"># = 0.84</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Interpretation: Code-optimization added successfully, but interfered with security-review</span></span></code></pre> <hr/> <h2 id="part-9-putting-it-all-together--a-worked-example">Part 9: Putting It All Together — A Worked Example</h2> <p>Let’s walk through a realistic ix experiment from start to finish.</p> <h3 id="scenario">Scenario</h3> <p>You’ve built a Claude Code skill called <code>security-review</code>. It analyzes code for vulnerabilities. You want to:</p> <ol><li>Measure skill activation accuracy (“does it trigger when it should?“)</li> <li>Detect coexistence interference (“does adding <code>code-style</code> break it?“)</li> <li>Track context degradation (“does it work after 50 file reviews?“)</li> <li>Ensure statistical rigor (Bayesian CIs, not naive CLT)</li></ol> <hr/> <h3 id="step-1-define-tasks-anthropics-framework">Step 1: Define Tasks (Anthropic’s Framework)</h3> <p><strong>Task</strong>: Test case with inputs + success criteria</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">tasks </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span></span>
<span class="line"><span style="color:#9ECBFF">        "id"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"sql_injection_1"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "input"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"cursor.execute(f'SELECT * FROM users WHERE id = </span><span style="color:#79B8FF">{user_id}</span><span style="color:#9ECBFF">')"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "expected_skill"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"security-review"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "expected_finding"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"SQL injection vulnerability"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "cluster"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"sql_injection"</span><span style="color:#6A737D">  # For clustered SEs</span></span>
<span class="line"><span style="color:#E1E4E8">    },</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span></span>
<span class="line"><span style="color:#9ECBFF">        "id"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"sql_injection_2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "input"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"query = 'DELETE FROM logs WHERE ' + user_input; execute(query)"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "expected_skill"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"security-review"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "expected_finding"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"SQL injection via concatenation"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "cluster"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"sql_injection"</span></span>
<span class="line"><span style="color:#E1E4E8">    },</span></span>
<span class="line"><span style="color:#6A737D">    # ... 48 more tasks across 5 clusters</span></span>
<span class="line"><span style="color:#E1E4E8">]</span></span></code></pre> <p><strong>Note</strong>: 50 tasks, 5 clusters (10 tasks per cluster) for clustered SE calculation.</p> <hr/> <h3 id="step-2-define-experiment-dag">Step 2: Define Experiment DAG</h3> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">experiment </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.Experiment(</span></span>
<span class="line"><span style="color:#FFAB70">    name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"security-review-activation"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    trials_per_task</span><span style="color:#F97583">=</span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># pass@10 and pass^10 metrics</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># DAG nodes</span></span>
<span class="line"><span style="color:#E1E4E8">experiment.add_probe(</span><span style="color:#9ECBFF">"prompt_probe"</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># Generates test inputs</span></span>
<span class="line"><span style="color:#E1E4E8">experiment.add_sensor(</span><span style="color:#9ECBFF">"activation_sensor"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"prompt_probe"</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># Observes skill activation</span></span>
<span class="line"><span style="color:#E1E4E8">experiment.add_sensor(</span><span style="color:#9ECBFF">"finding_sensor"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"prompt_probe"</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># Extracts security findings</span></span>
<span class="line"><span style="color:#E1E4E8">experiment.add_grader(</span><span style="color:#9ECBFF">"activation_grader"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"activation_sensor"</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># Matches expected vs actual</span></span>
<span class="line"><span style="color:#E1E4E8">experiment.add_grader(</span><span style="color:#9ECBFF">"finding_grader"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"finding_sensor"</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># Matches expected finding</span></span>
<span class="line"><span style="color:#E1E4E8">experiment.add_scorer(</span><span style="color:#9ECBFF">"classification_scorer"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"activation_grader"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"finding_grader"</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># F1</span></span>
<span class="line"><span style="color:#E1E4E8">experiment.add_scorer(</span><span style="color:#9ECBFF">"stochastic_scorer"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"classification_scorer"</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># pass@k, pass^k</span></span>
<span class="line"><span style="color:#E1E4E8">experiment.add_scorer(</span><span style="color:#9ECBFF">"cost_scorer"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"activation_sensor"</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># Tracks API spend</span></span>
<span class="line"><span style="color:#E1E4E8">experiment.add_reporter(</span><span style="color:#9ECBFF">"console_reporter"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">depends_on</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"stochastic_scorer"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"cost_scorer"</span><span style="color:#E1E4E8">])</span></span></code></pre> <p><strong>DAG parallelization</strong>:</p> <ul><li><code>activation_sensor</code> and <code>finding_sensor</code> run in parallel (both depend only on probe)</li> <li><code>activation_grader</code> and <code>finding_grader</code> run in parallel</li> <li><code>classification_scorer</code> waits for both graders</li> <li><code>stochastic_scorer</code> computes pass@k from classification results</li> <li><code>cost_scorer</code> runs independently</li> <li><code>console_reporter</code> waits for both scorers</li></ul> <hr/> <h3 id="step-3-run-experiment-skill-activation">Step 3: Run Experiment (Skill Activation)</h3> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.run(experiment, </span><span style="color:#FFAB70">tasks</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tasks)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Result structure:</span></span>
<span class="line"><span style="color:#E1E4E8">{</span></span>
<span class="line"><span style="color:#9ECBFF">    "mean_f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.88</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">    "se_clustered"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.03</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># 3x larger than naive SE=0.01</span></span>
<span class="line"><span style="color:#9ECBFF">    "ci_95"</span><span style="color:#E1E4E8">: [</span><span style="color:#79B8FF">0.82</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.94</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#9ECBFF">    "pass_at_10"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.98</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># At least one success in 10 trials</span></span>
<span class="line"><span style="color:#9ECBFF">    "pass_pow_10"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.28</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># All 10 trials succeed</span></span>
<span class="line"><span style="color:#9ECBFF">    "total_cost"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">4.23</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># USD</span></span>
<span class="line"><span style="color:#9ECBFF">    "cost_per_task"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.085</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">    "transcripts"</span><span style="color:#E1E4E8">: [</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">]  </span><span style="color:#6A737D"># Full traces for inspection</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre> <p><strong>Interpretation</strong>:</p> <ul><li>F1 = 0.88 means precision/recall are high (skill activates correctly)</li> <li>Clustered SE = 0.03 (not 0.01) because SQL injection tasks correlate</li> <li>pass@10 = 98% means “at least once in 10 tries” almost always works</li> <li>pass^10 = 28% means “all 10 tries” rarely works — stochastic variance is high</li> <li>Cost = $0.085/task is reasonable for security review</li></ul> <hr/> <h3 id="step-4-coexistence-test-does-adding-code-style-break-it">Step 4: Coexistence Test (Does Adding <code>code-style</code> Break It?)</h3> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Control: security-review alone</span></span>
<span class="line"><span style="color:#E1E4E8">control </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.Experiment(</span></span>
<span class="line"><span style="color:#FFAB70">    name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"control"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    skills</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"security-review"</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Variant: security-review + code-style</span></span>
<span class="line"><span style="color:#E1E4E8">variant </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.Experiment(</span></span>
<span class="line"><span style="color:#FFAB70">    name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"variant"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    skills</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"security-review"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"code-style"</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Run both on same tasks</span></span>
<span class="line"><span style="color:#E1E4E8">coexistence_result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.run_paired([control, variant], </span><span style="color:#FFAB70">tasks</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tasks)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Result:</span></span>
<span class="line"><span style="color:#E1E4E8">{</span></span>
<span class="line"><span style="color:#9ECBFF">    "control_f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.88</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">    "variant_f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.76</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Dropped!</span></span>
<span class="line"><span style="color:#9ECBFF">    "delta_f1"</span><span style="color:#E1E4E8">: </span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.12</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">    "se_paired"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.02</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Paired SE (smaller than independent)</span></span>
<span class="line"><span style="color:#9ECBFF">    "ci_95"</span><span style="color:#E1E4E8">: [</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.16</span><span style="color:#E1E4E8">, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.08</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#9ECBFF">    "correlation"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.61</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Tasks correlate across experiments</span></span>
<span class="line"><span style="color:#9ECBFF">    "interpretation"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Code-style skill interferes with security-review. F1 dropped 12 points (p &lt; 0.001)."</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre> <p><strong>Interpretation</strong>:</p> <ul><li>F1 dropped from 0.88 to 0.76 when code-style added</li> <li>Paired CI = [-0.16, -0.08] doesn’t include zero -> <strong>significant degradation</strong></li> <li>Transcript analysis shows: code-style generates explanations that suppress security warnings</li> <li><strong>Coexistence interference detected</strong></li></ul> <hr/> <h3 id="step-5-context-degradation-test">Step 5: Context Degradation Test</h3> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#E1E4E8">degradation_experiment </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.ContextDegradationExperiment(</span></span>
<span class="line"><span style="color:#FFAB70">    skill</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"security-review"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    max_tool_calls</span><span style="color:#F97583">=</span><span style="color:#79B8FF">100</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    checkpoints</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">25</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">50</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">75</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">100</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">degradation_result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.run(degradation_experiment, </span><span style="color:#FFAB70">tasks</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tasks)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Result:</span></span>
<span class="line"><span style="color:#E1E4E8">{</span></span>
<span class="line"><span style="color:#9ECBFF">    "checkpoints"</span><span style="color:#E1E4E8">: {</span></span>
<span class="line"><span style="color:#79B8FF">        10</span><span style="color:#E1E4E8">:  {</span><span style="color:#9ECBFF">"f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.88</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"latency_p50"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">1.2</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"cost"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.09</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#79B8FF">        25</span><span style="color:#E1E4E8">:  {</span><span style="color:#9ECBFF">"f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.85</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"latency_p50"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">1.4</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"cost"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.11</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#79B8FF">        50</span><span style="color:#E1E4E8">:  {</span><span style="color:#9ECBFF">"f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.78</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"latency_p50"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">1.9</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"cost"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.14</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#79B8FF">        75</span><span style="color:#E1E4E8">:  {</span><span style="color:#9ECBFF">"f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.71</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"latency_p50"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">2.5</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"cost"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.19</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#79B8FF">        100</span><span style="color:#E1E4E8">: {</span><span style="color:#9ECBFF">"f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.63</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"latency_p50"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">3.2</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"cost"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.26</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#E1E4E8">    },</span></span>
<span class="line"><span style="color:#9ECBFF">    "degradation_rate"</span><span style="color:#E1E4E8">: </span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.0025</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># F1 drops 0.25 points per 10 tool calls</span></span>
<span class="line"><span style="color:#9ECBFF">    "interpretation"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Context degradation detected. Agent loses focus after 50 calls."</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre> <p><strong>Interpretation</strong>:</p> <ul><li>F1 at step 10: 88% (baseline)</li> <li>F1 at step 50: 78% (10-point drop)</li> <li>F1 at step 100: 63% (25-point drop)</li> <li>Latency and cost also increase (crowded context is expensive)</li> <li><strong>Context degradation confirmed</strong></li></ul> <hr/> <h3 id="step-6-statistical-rigor-bayesian-cis">Step 6: Statistical Rigor (Bayesian CIs)</h3> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># ix automatically uses bayes_evals for small-N</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> bayes_evals </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> bayesian_ci</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Instead of CLT (wrong):</span></span>
<span class="line"><span style="color:#E1E4E8">sem_clt </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.std(f1_scores) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> np.sqrt(</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(f1_scores))</span></span>
<span class="line"><span style="color:#E1E4E8">ci_clt </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [mean </span><span style="color:#F97583">-</span><span style="color:#79B8FF"> 1.96</span><span style="color:#F97583">*</span><span style="color:#E1E4E8">sem_clt, mean </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> 1.96</span><span style="color:#F97583">*</span><span style="color:#E1E4E8">sem_clt]  </span><span style="color:#6A737D"># Underestimates uncertainty</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Bayesian (correct):</span></span>
<span class="line"><span style="color:#E1E4E8">ci_bayes </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> bayesian_ci(f1_scores, </span><span style="color:#FFAB70">confidence</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.95</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># Wider, more honest</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Result:</span></span>
<span class="line"><span style="color:#6A737D"># CLT CI: [0.85, 0.91] (overconfident)</span></span>
<span class="line"><span style="color:#6A737D"># Bayes CI: [0.82, 0.94] (realistic)</span></span></code></pre> <p><strong>Why this matters</strong>: With 50 tasks, CLT breaks down. Bayesian methods give you honest uncertainty.</p> <hr/> <h3 id="step-7-pareto-analysis-cost-vs-quality">Step 7: Pareto Analysis (Cost vs Quality)</h3> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Collect quality and cost across variants</span></span>
<span class="line"><span style="color:#E1E4E8">variants </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"name"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"security-review-base"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.88</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"cost"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.085</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"name"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"security-review-cot"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.91</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"cost"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.14</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"name"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"security-review-lightweight"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"f1"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.82</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"cost"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0.03</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#E1E4E8">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">pareto_result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.compute_pareto(variants, </span><span style="color:#FFAB70">quality_metric</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"f1"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">cost_metric</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"cost"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Result: Pareto frontier</span></span>
<span class="line"><span style="color:#6A737D"># - Lightweight: 0.82 F1 @ $0.03 (best for cost-sensitive)</span></span>
<span class="line"><span style="color:#6A737D"># - Base: 0.88 F1 @ $0.085 (balanced)</span></span>
<span class="line"><span style="color:#6A737D"># - CoT: 0.91 F1 @ $0.14 (best for quality-sensitive)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># CoT is NOT Pareto-optimal -- it costs 1.65x more for only 3-point improvement</span></span>
<span class="line"><span style="color:#6A737D"># For production: use base or lightweight depending on budget</span></span></code></pre> <p><strong>Interpretation</strong>: Always show cost-quality tradeoffs. Pure accuracy rankings hide the real decision.</p> <p><strong>Source</strong>: <a href="https://arxiv.org/abs/2407.01502" rel="nofollow">AI Agents That Matter</a></p> <hr/> <h3 id="step-8-quality-gate-cicd-integration">Step 8: Quality Gate (CI/CD Integration)</h3> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span style="color:#6A737D"># Define thresholds</span></span>
<span class="line"><span style="color:#E1E4E8">quality_gate </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> ix.QualityGate(</span></span>
<span class="line"><span style="color:#FFAB70">    min_f1</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.80</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    max_degradation</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.05</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Alert if F1 drops >5pp vs baseline</span></span>
<span class="line"><span style="color:#FFAB70">    max_cost_per_task</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.10</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Check results</span></span>
<span class="line"><span style="color:#E1E4E8">gate_result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> quality_gate.check(result)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> gate_result.passed:</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"Quality gate passed. Deploy to production."</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">else</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Quality gate failed: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">gate_result.failures</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    sys.exit(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span></code></pre> <p><strong>Integration</strong>: Run this in GitHub Actions on every PR. Block merge if quality regresses.</p> <p><strong>Source</strong>: <a href="https://www.braintrust.dev/docs/core/functions/scorers" rel="nofollow">Braintrust CI/CD patterns</a></p> <hr/> <h2 id="part-10-final-mental-model--what-youve-learned">Part 10: Final Mental Model — What You’ve Learned</h2> <p>You started knowing Python and Claude Code skills. Now you know:</p> <h3 id="1-measurement-under-uncertainty">1. Measurement Under Uncertainty</h3> <p>It’s not pass/fail. It’s distributions, confidence intervals, and tradeoffs.</p> <h3 id="2-state-execution-stochasticity-grading">2. State, Execution, Stochasticity, Grading</h3> <p>Every framework makes choices on these four dimensions. Most make bad choices (no state, sequential execution, no stochastic handling, brittle grading).</p> <h3 id="3-statistics-matter-more-than-you-think">3. Statistics Matter More Than You Think</h3> <ul><li>Clustered SEs can be 3x larger than naive</li> <li>CLT breaks down for small N (use Bayesian)</li> <li>Infrastructure noise is 3-6 percentage points</li> <li>Paired differences are more powerful than independent comparisons</li></ul> <h3 id="4-anthropics-guidance-is-canonical">4. Anthropic’s Guidance Is Canonical</h3> <ul><li>Task/trial/transcript/outcome terminology</li> <li>pass@k vs pass^k metrics</li> <li>Grade outcome, not path</li> <li>20-50 tasks from real failures = starting point</li> <li>Code > LLM > Human grading hierarchy</li></ul> <h3 id="5-the-gaps-ix-fills-are-real">5. The Gaps ix Fills Are Real</h3> <ul><li>Skill activation: No tool tests “right skill for right context”</li> <li>Coexistence: No tool tests “adding B breaks A”</li> <li>Context degradation: No benchmark measures “works after 50 calls”</li> <li>DAG orchestration: No framework composes components as dependency graph</li></ul> <h3 id="6-ix-is-unique">6. ix Is Unique</h3> <p>Upper-right quadrant: agent experimentation + statistical rigor + DAG orchestration.</p> <p>Nobody else is there.</p> <hr/> <h2 id="appendix-a-complete-source-index">Appendix A: Complete Source Index</h2> <p>All sources organized by tier and category.</p> <h3 id="tier-1-anthropic-official-research">[TIER 1] Anthropic Official Research</h3> <table><thead><tr><th>Source</th><th>URL</th><th>Key Contribution</th></tr></thead><tbody><tr><td>Adding Error Bars</td><td><a href="https://arxiv.org/abs/2411.00640" rel="nofollow">arXiv:2411.00640</a></td><td>Clustered SEs 3x larger than naive</td></tr><tr><td>Demystifying Agents</td><td><a href="https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents" rel="nofollow">anthropic.com</a></td><td>Zero-to-one roadmap, grader types</td></tr><tr><td>Infrastructure Noise</td><td><a href="https://www.anthropic.com/engineering/infrastructure-noise" rel="nofollow">anthropic.com</a></td><td>6pp shifts from config alone</td></tr><tr><td>Create Strong Empirical Tests</td><td><a href="https://platform.claude.com/docs/en/test-and-evaluate/develop-tests" rel="nofollow">docs</a></td><td>Code examples for every grading type</td></tr><tr><td>Bloom</td><td><a href="https://github.com/safety-research/bloom" rel="nofollow">GitHub</a></td><td>Understand -> Ideate -> Rollout -> Judge pipeline</td></tr><tr><td>Model-Written Tests</td><td><a href="https://arxiv.org/abs/2212.09251" rel="nofollow">arXiv:2212.09251</a></td><td>Automated test generation, 154 datasets</td></tr><tr><td>Constitutional AI</td><td><a href="https://arxiv.org/abs/2212.08073" rel="nofollow">arXiv:2212.08073</a></td><td>Self-assessment methodology</td></tr><tr><td>Measuring CoT Faithfulness</td><td><a href="https://arxiv.org/abs/2307.13702" rel="nofollow">arXiv:2307.13702</a></td><td>CoT is unreliable for grading</td></tr><tr><td>Sabotage Tests</td><td><a href="https://arxiv.org/abs/2410.21514" rel="nofollow">arXiv:2410.21514</a></td><td>Four sabotage test types</td></tr><tr><td>Alignment Faking</td><td><a href="https://arxiv.org/abs/2412.14093" rel="nofollow">arXiv:2412.14093</a></td><td>Scratchpad methodology, 12% faking rate</td></tr><tr><td>Claude Code Headless</td><td><a href="https://code.claude.com/docs/en/headless" rel="nofollow">docs</a></td><td>Programmatic Claude Code for harnesses</td></tr></tbody></table> <h3 id="tier-1-academic-papers-non-anthropic">[TIER 1] Academic Papers (Non-Anthropic)</h3> <table><thead><tr><th>Source</th><th>URL</th><th>Key Contribution</th></tr></thead><tbody><tr><td>Don’t Use the CLT</td><td><a href="https://arxiv.org/abs/2503.01747" rel="nofollow">arXiv:2503.01747</a></td><td>CLT underestimates uncertainty, use Bayesian</td></tr><tr><td>Signal and Noise</td><td><a href="https://arxiv.org/abs/2508.13144" rel="nofollow">arXiv:2508.13144</a></td><td>SNR as benchmark quality metric</td></tr><tr><td>AI Agents That Matter</td><td><a href="https://arxiv.org/abs/2407.01502" rel="nofollow">arXiv:2407.01502</a></td><td>Always report cost, Pareto curves</td></tr><tr><td>CooperBench</td><td><a href="https://arxiv.org/abs/2601.13295" rel="nofollow">arXiv:2601.13295</a></td><td>Multi-agent cooperation fails 75%</td></tr><tr><td>SWE-EVO</td><td><a href="https://arxiv.org/abs/2512.18470" rel="nofollow">arXiv:2512.18470</a></td><td>CL-F1 (plasticity + stability)</td></tr><tr><td>SWE-bench</td><td><a href="https://arxiv.org/abs/2310.06770" rel="nofollow">arXiv:2310.06770</a></td><td>Real-world GitHub bug fixes</td></tr></tbody></table> <h3 id="tier-1-official-framework-docs">[TIER 1] Official Framework Docs</h3> <table><thead><tr><th>Framework</th><th>Docs</th><th>GitHub</th><th>Stars</th></tr></thead><tbody><tr><td>Inspect AI</td><td><a href="https://inspect.aisi.org.uk/" rel="nofollow">docs</a></td><td><a href="https://github.com/UKGovernmentBEIS/inspect_ai" rel="nofollow">GitHub</a></td><td>1.7k</td></tr><tr><td>Promptfoo</td><td><a href="https://www.promptfoo.dev/" rel="nofollow">docs</a></td><td><a href="https://github.com/promptfoo/promptfoo" rel="nofollow">GitHub</a></td><td>10.4k</td></tr><tr><td>DeepEval</td><td><a href="https://deepeval.com/" rel="nofollow">docs</a></td><td><a href="https://github.com/confident-ai/deepeval" rel="nofollow">GitHub</a></td><td>13.6k</td></tr><tr><td>LM Harness</td><td><a href="https://www.eleuther.ai/artifacts/lm-evaluation-harness" rel="nofollow">docs</a></td><td><a href="https://github.com/EleutherAI/lm-evaluation-harness" rel="nofollow">GitHub</a></td><td>11.4k</td></tr><tr><td>Langfuse</td><td><a href="https://langfuse.com/" rel="nofollow">docs</a></td><td><a href="https://github.com/langfuse/langfuse" rel="nofollow">GitHub</a></td><td>21.7k</td></tr><tr><td>Arize Phoenix</td><td><a href="https://phoenix.arize.com/" rel="nofollow">docs</a></td><td><a href="https://github.com/Arize-ai/phoenix" rel="nofollow">GitHub</a></td><td>8.5k</td></tr><tr><td>Braintrust</td><td><a href="https://www.braintrust.dev/" rel="nofollow">docs</a></td><td><a href="https://github.com/braintrustdata/braintrust-sdk" rel="nofollow">GitHub</a></td><td>114</td></tr></tbody></table> <h3 id="tier-2-practitioner-analysis">[TIER 2] Practitioner Analysis</h3> <table><thead><tr><th>Source</th><th>URL</th><th>Key Insight</th></tr></thead><tbody><tr><td>Phil Schmid “Agent Harness 2026”</td><td><a href="https://www.philschmid.de/agent-harness-2026" rel="nofollow">philschmid.de</a></td><td>Task durability (50+ tool calls) is unmeasured</td></tr><tr><td>Hamel Husain “Inspect AI Review”</td><td><a href="https://hamel.dev/notes/llm/evals/inspect.html" rel="nofollow">hamel.dev</a></td><td>Practitioner review of Inspect AI</td></tr></tbody></table> <h3 id="tier-3-research-syntheses-this-document">[TIER 3] Research Syntheses (This Document)</h3> <table><thead><tr><th>Source</th><th>Location</th><th>Contents</th></tr></thead><tbody><tr><td>Unified Synthesis</td><td><code>scratch/research-synthesis-ix-landscape-2026-02-09.md</code></td><td>Framework comparison, gaps analysis</td></tr><tr><td>LLM Framework Landscape</td><td><code>scratch/llm-eval-framework-landscape-2026-02-09.md</code></td><td>14 frameworks analyzed</td></tr><tr><td>Agent Harness Landscape</td><td><code>scratch/agent-eval-harness-landscape-2026-02-09.md</code></td><td>Agent-specific tooling</td></tr><tr><td>Anthropic Research Sweep</td><td><code>scratch/anthropic-eval-research-sweep-2026-02-09.md</code></td><td>40+ Anthropic sources</td></tr></tbody></table> <hr/> <h2 id="appendix-b-key-numbers-to-remember">Appendix B: Key Numbers to Remember</h2> <table><thead><tr><th>Number</th><th>Source</th><th>What It Means</th></tr></thead><tbody><tr><td><strong>3x</strong></td><td>Error Bars paper</td><td>Clustered SEs can be 3x larger than naive</td></tr><tr><td><strong>6pp</strong></td><td>Infrastructure Noise</td><td>Config alone shifts benchmark scores 6 percentage points</td></tr><tr><td><strong>3pp</strong></td><td>Infrastructure Noise</td><td>Differences below 3pp deserve skepticism</td></tr><tr><td><strong>20-50</strong></td><td>Demystifying Agents</td><td>Starting point for test suite size</td></tr><tr><td><strong>98% vs 42%</strong></td><td>pass@3 vs pass^3</td><td>Same 75% success rate, wildly different numbers</td></tr><tr><td><strong>25%</strong></td><td>CooperBench</td><td>Multi-agent cooperation success rate (vs 50% single-agent)</td></tr><tr><td><strong>17%</strong></td><td>Learning harm research</td><td>Worse exam performance after unrestricted AI use</td></tr><tr><td><strong>75%</strong></td><td>SWE-bench Verified</td><td>Current coding agent performance (Jan 2026)</td></tr></tbody></table> <hr/> <h2 id="appendix-c-glossary-anthropics-terminology">Appendix C: Glossary (Anthropic’s Terminology)</h2> <p>Use these terms. They’re standard.</p> <table><thead><tr><th>Term</th><th>Definition</th><th>Example</th></tr></thead><tbody><tr><td><strong>Task</strong></td><td>Single test with inputs + success criteria</td><td>“Does security-review trigger for SQL injection?”</td></tr><tr><td><strong>Trial</strong></td><td>One attempt at a task</td><td>10 trials per task for pass@10</td></tr><tr><td><strong>Grader</strong></td><td>Logic scoring agent performance</td><td>Code-based, LLM-based, or human</td></tr><tr><td><strong>Transcript</strong></td><td>Complete record of a trial</td><td>All outputs, tool calls, reasoning</td></tr><tr><td><strong>Outcome</strong></td><td>Final environment state</td><td>Not agent’s claim — actual result</td></tr><tr><td><strong>Harness</strong></td><td>Infrastructure running experiments</td><td>Docker containers, API calls, grading</td></tr><tr><td><strong>pass@k</strong></td><td>Probability of at least 1 success in k trials</td><td>Use for coding (one working solution)</td></tr><tr><td><strong>pass^k</strong></td><td>Probability all k trials succeed</td><td>Use for customer-facing (consistency)</td></tr><tr><td><strong>Clustered SE</strong></td><td>Standard error accounting for correlation</td><td>3x larger than naive SE</td></tr><tr><td><strong>Paired differences</strong></td><td>Compare same tasks across models</td><td>Free variance reduction from correlation</td></tr></tbody></table> <hr/> <p><em>Research conducted February 2026. Three parallel research agents + Anthropic deep dive, ~300K tokens total research. All claims sourced to Tier 1 or Tier 2 evidence.</em></p> <p><em>Next steps: Read the <a href="../../scratch/guild-ix-ontology-deliberation-2026-02-08.md">ix ontology deliberation</a> and <a href="../../scratch/ix-lab-architecture-excavation-2026-02-06.md">ix-lab architecture</a> to see how these concepts map to ix’s implementation.</em></p><!----> <!--[--><footer class="article-footer svelte-1iberk1"><div class="read-action svelte-1iberk1"><button class="mark-read-btn svelte-1iberk1" aria-pressed="false"><!--[!--><span class="circle-icon svelte-1iberk1">○</span> Mark as read<!--]--></button> <!--[--><span class="position-label svelte-1iberk1">14 of 14</span><!--]--></div> <!--[!--><!--]--> <nav class="seq-nav svelte-1iberk1" aria-label="Article navigation"><div class="seq-nav-links svelte-1iberk1"><!--[--><a href="../../library/explanation/hype-questioning" class="nav-prev svelte-1iberk1"><span class="nav-arrow svelte-1iberk1">←</span> <span class="nav-label">Hype &amp; Questioning</span></a><!--]--> <!--[!--><a href="../../library" class="nav-next svelte-1iberk1"><span class="nav-label">Back to Library</span> <span class="nav-arrow svelte-1iberk1">→</span></a><!--]--></div></nav></footer><!--]--></article> <aside class="article-sidebar svelte-1wa4r3o"><!--[!--><!--]--><!----> <!--[!--><!--]--></aside></div> <!--[!--><!--]--><!----><!----><!----></main><!----><!--]--><!----></div> <div class="experimental-tag svelte-12qhfyh" aria-hidden="true">experimental</div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_16atlio = {
						base: new URL("../..", location).pathname.slice(0, -1),
						assets: "/cix/pr-preview/pr-22"
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../../_app/immutable/entry/start.CErVG62D.js"),
						import("../../_app/immutable/entry/app.pJCGw2HY.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 2, 8],
							data: [null,null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
