import{a,f as s}from"./BPxar_6W.js";import"./KFEA_jlE.js";import{I as n}from"./DXXdKfFK.js";var o=s('<h1>The Evidence</h1> <p>Research accumulating across disciplines points toward the same pattern: AI improves output while degrading capability.</p> <hr/> <h2>Sources</h2> <p><strong>Skill Formation & Learning</strong></p> <ul><li><a href="https://arxiv.org/pdf/2601.20245" rel="nofollow">Shen & Tamkin (2026). How AI Impacts Skill Formation. Anthropic.</a></li> <li><a href="https://www.pnas.org/doi/10.1073/pnas.2413913122" rel="nofollow">Bastani et al. (2025). Generative AI Can Harm Learning. PNAS.</a></li></ul> <p><strong>Cognitive Effects</strong></p> <ul><li><a href="https://dl.acm.org/doi/10.1145/3613904.3641913" rel="nofollow">Lee et al. (2025). Impact of Generative AI on Critical Thinking. CHI.</a></li> <li><a href="https://www.media.mit.edu/" rel="nofollow">Kosmyna et al. (2025). AI-Assisted Writing and Memory. MIT Media Lab.</a></li> <li><a href="https://www.mdpi.com/2075-4698/15/1/6" rel="nofollow">Gerlich (2025). AI Tools and Cognitive Offloading. MDPI Societies.</a></li> <li><a href="https://www.thelancet.com/journals/langas/article/PIIS2468-1253(24)00301-2/fulltext" rel="nofollow">Budzyń et al. (2025). AI-Assisted Colonoscopy. Lancet.</a></li></ul> <p><strong>Productivity & Perception</strong></p> <ul><li><a href="https://arxiv.org/abs/2507.09089" rel="nofollow">METR (2025). AI Impact on Developer Productivity. RCT.</a></li> <li><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4671691" rel="nofollow">Cui/Demirer et al. (2024). Effects on High Skilled Work. RCTs.</a></li> <li><a href="https://survey.stackoverflow.co/2024/" rel="nofollow">Stack Overflow Developer Survey (2024-2025).</a></li></ul> <p><strong>Collaboration Design</strong></p> <ul><li><a href="https://journals.sagepub.com/doi/10.1177/10946705241253322" rel="nofollow">Blaurock et al. (2024). AI-Based Service Contingencies. Journal of Service Research.</a></li> <li><a href="https://dl.acm.org/doi/10.1145/3411764.3445717" rel="nofollow">Bansal et al. (2021). AI Explanations and Trust. CHI.</a></li></ul> <p><strong>Homogenization & Diversity</strong></p> <ul><li><a href="https://arxiv.org/abs/2510.22954" rel="nofollow">Jiang et al. (2025). Artificial Hivemind. NeurIPS Best Paper.</a></li> <li><a href="https://www.science.org/doi/10.1126/sciadv.adn5290" rel="nofollow">Doshi & Hauser (2024). Individual Creativity vs Collective Diversity. Science Advances.</a></li> <li><a href="https://arxiv.org/abs/2505.17241" rel="nofollow">Meta-analysis (2025). Generative AI and Creativity. arXiv.</a></li> <li><a href="https://www.cell.com/patterns/fulltext/S2666-3899(25)00299-5" rel="nofollow">Hintze et al. (2026). Visual Elevator Music. Patterns/Cell.</a></li> <li><a href="https://www.pnas.org/doi/10.1073/pnas.2504966122" rel="nofollow">Xu et al. (2025). Echoes in AI: LLM Homogenization. PNAS.</a></li> <li><a href="https://journals.sagepub.com/doi/10.1177/00491241251327130" rel="nofollow">Zhang et al. (2025). AI and Survey Homogenization. Sociological Methods & Research.</a></li></ul> <p><strong>Disempowerment</strong></p> <ul><li><a href="https://arxiv.org/abs/2601.19062" rel="nofollow">Sharma et al. (2026). Who’s in Charge? Disempowerment Patterns. Anthropic.</a></li></ul> <p><strong>Longitudinal</strong></p> <ul><li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0160791X25002775" rel="nofollow">Zhou et al. (2025). Creative Scar. Technology in Society.</a></li></ul> <hr/> <h2>Abstract</h2> <p>The evidence clusters into five hypotheses, each supported by multiple independent studies:</p> <table><thead><tr><th>Hypothesis</th><th>Key Effect</th><th>Sources</th></tr></thead><tbody><tr><td><strong>Engagement model determines outcome</strong></td><td>86% vs 24% mastery from same AI</td><td>Shen & Tamkin, Bastani</td></tr><tr><td><strong>Cognitive degradation is measurable</strong></td><td>β = -0.69 (AI confidence → less thinking)</td><td>Lee, Kosmyna, Gerlich, Budzyń</td></tr><tr><td><strong>Perception gap hides harm</strong></td><td>43-point gap (predicted vs actual)</td><td>METR, Stack Overflow</td></tr><tr><td><strong>Control and transparency work</strong></td><td>β = 0.507 (control), β = 0.415 (transparency)</td><td>Blaurock</td></tr><tr><td><strong>Homogenization threatens diversity</strong></td><td>g = -0.863 diversity reduction</td><td>Meta-analysis, Jiang (NeurIPS Best Paper), Doshi</td></tr><tr><td><strong>Users misjudge what helps them</strong></td><td>Harmful interactions rated favorably</td><td>Sharma (Anthropic)</td></tr></tbody></table> <hr/> <h2>Explanation</h2> <h3>Engagement model determines outcome</h3> <p>Shen & Tamkin (Anthropic, 2026) ran an RCT with 52 engineers learning a new library. <span class="ev ev-moderate" title="RCT, n=52, single study">◐</span> Six interaction patterns emerged from identical AI access:</p> <table><thead><tr><th>Pattern</th><th>Mastery</th><th>Behavior</th></tr></thead><tbody><tr><td>Generation-Then-Comprehension</td><td>86%</td><td>Generated code, asked follow-up questions</td></tr><tr><td>Hybrid Code-Explanation</td><td>68%</td><td>Requested explanations with code</td></tr><tr><td>Conceptual Inquiry</td><td>65%</td><td>Conceptual questions only, wrote code themselves</td></tr><tr><td>AI Delegation</td><td>39%</td><td>Paste and move on</td></tr><tr><td>Progressive Reliance</td><td>35%</td><td>Started manual, ended delegating</td></tr><tr><td>Iterative Debugging</td><td>24%</td><td>AI fixes errors repeatedly</td></tr></tbody></table> <p>Same tool. Mastery ranging from 24% to 86%. The interaction pattern, not the technology, determined learning.</p> <p>Bastani’s PNAS study (n=1,000 students) confirmed this in education. <span class="ev ev-strong" title="RCT, n=1,000, PNAS peer-reviewed">●</span> Unrestricted ChatGPT caused 17% worse exam performance. Scaffolded ChatGPT with guardrails showed no significant difference from control. Design shapes outcome.</p> <h3>Cognitive degradation is measurable</h3> <p>Lee et al. (CHI 2025, n=319 knowledge workers) found confidence in AI negatively correlated with critical thinking enacted (β = -0.69). <span class="ev ev-strong" title="CHI peer-reviewed, n=319, structural equation modeling">●</span> Workers shift from execution to oversight without maintaining verification rigor.</p> <p>Kosmyna (MIT Media Lab) measured this neurologically. <span class="ev ev-moderate" title="EEG study, MIT Media Lab, single study">◐</span> EEG during AI-assisted writing showed reduced memory encoding. 83% couldn’t recall content from their own work — they never learned it.</p> <p>Budzyń (Lancet, crossover RCT) showed skill atrophy directly. <span class="ev ev-moderate" title="Lancet crossover RCT, single study, medical domain">◐</span> After 3 months with AI-assisted polyp detection, endoscopists’ unaided detection rate dropped from 28.4% to 22.4%. A 20% decline in 12 weeks.</p> <p>Gerlich (n=666) found AI use negatively predicts critical thinking (β = -1.76) with younger users most affected. <span class="ev ev-moderate" title="Survey, n=666, single study">◐</span></p> <h3>Perception gap hides harm</h3> <p>METR’s RCT (16 experienced developers, 246 real tasks) showed a 43-point perception gap: predicted 24% speedup, actual 19% slowdown. <span class="ev ev-moderate" title="RCT, n=16, small sample but rigorous design">◐</span></p> <p>Stack Overflow surveys show the paradox at scale: trust in AI accuracy dropped 10 points (43% → 33%) while adoption rose 8 points (76% → 84%). People use tools they don’t trust, perceive benefits they don’t get.</p> <p>The gap prevents correction. You can’t fix what you don’t notice.</p> <h3>Control and transparency work</h3> <p>Blaurock’s meta-analysis (106 studies, 654 professionals) identified what predicts good outcomes: <span class="ev ev-strong" title="Meta-analysis, 106 studies, 654 professionals">●</span></p> <table><thead><tr><th>Factor</th><th>Effect Size</th></tr></thead><tbody><tr><td>Process control</td><td>β = 0.507</td></tr><tr><td>Transparency</td><td>β = 0.415</td></tr><tr><td>Outcome control</td><td>Significant positive</td></tr><tr><td>Engagement features</td><td>b = -0.555 (negative)</td></tr></tbody></table> <p>Control is the strongest lever. Transparency is second. Engagement features (AI asking questions) actually hurt frequent users.</p> <p>Bansal (CHI 2021) explained why explanations alone fail: they increased acceptance regardless of correctness. <span class="ev ev-strong" title="CHI peer-reviewed, controlled experiment">●</span> When AI was right, small improvement. When wrong, performance degraded. Explanations build trust without calibrating it.</p> <h3>Homogenization threatens diversity</h3> <p><strong>The Artificial Hivemind (NeurIPS 2025 Best Paper):</strong> Jiang et al. tested 70+ LLMs on 26,000 open-ended queries. <span class="ev ev-strong" title="NeurIPS Best Paper, 70+ models, 26,000 queries">●</span> When 25 different models wrote “a metaphor about time,” only 2 dominant clusters emerged. Temperature and ensembling don’t help — RLHF over-fits to consensus, penalizing valid but idiosyncratic responses.</p> <p><strong>Meta-analysis (28 studies, n=8,214):</strong> <span class="ev ev-strong" title="Meta-analysis, 28 studies, n=8,214, p&lt;0.001">●</span> Pooled effect size for diversity reduction: <strong>g = -0.863</strong> (CI: -1.328 to -0.398, p&lt;0.001). Large negative effect. Individual creative performance goes up (+0.27), but collective diversity goes down hard.</p> <p><strong>Visual convergence:</strong> Hintze et al. (Patterns/Cell, Jan 2026) ran 700 iterative AI image generation loops. <span class="ev ev-moderate" title="Single study, 700 trials, Patterns/Cell">◐</span> ALL converged to just 12 motifs (lighthouses, Gothic cathedrals, rustic buildings…) regardless of starting prompt. “What they generated is bland, pop culture, generic.”</p> <p>Doshi & Hauser (Science Advances) quantified the social dilemma: individual novelty +8.1%, story similarity +10.7%. <span class="ev ev-strong" title="Science Advances peer-reviewed, controlled experiment">●</span> Individually better off, collectively homogenized.</p> <p><strong>Cultural homogenization:</strong> Agarwal et al. (CHI 2025) found AI pushes writing toward Western norms. <span class="ev ev-strong" title="CHI peer-reviewed, classification study">●</span> Cultural classification accuracy dropped from 90.6% to 83.5% with AI assistance.</p> <p>When everyone uses the same AI, outputs converge. The diversity that enables collective intelligence — Hong & Page showed diverse groups outperform best-ability groups — disappears.</p> <p><strong>Mitigation evidence:</strong> Wan & Kalman (2025) showed that using 10 diverse AI “personas” eliminated the homogenization effect. <span class="ev ev-moderate" title="Single study, promising but needs replication">◐</span> Within-persona similarity: 0.92, across-persona: 0.20. Diversity can be preserved through design.</p> <h3>Users misjudge what helps them</h3> <p>Sharma et al. (Anthropic, Jan 2026) analyzed ~1.5 million Claude.ai conversations. <span class="ev ev-moderate" title="Large-scale observational study, ~1.5M conversations, single platform">◐</span> The finding: <strong>users rate disempowering interactions MORE favorably</strong> in the moment. Interactions that distorted reality, value judgments, or actions felt good.</p> <p>But when users actually <strong>acted on AI outputs</strong>, satisfaction dropped <strong>below baseline</strong>. Users expressed regret: “I should have listened to my own intuition.”</p> <p>This explains why the perception gap (METR: predicted 24% speedup, actual 19% slowdown) persists. Short-term satisfaction ≠ long-term benefit. Users can’t self-correct because the feedback loop is broken — the harm feels helpful.</p> <p><strong>Implication:</strong> Design must compensate for miscalibrated user preferences. Transparency and control aren’t just nice-to-have — they’re necessary because users can’t reliably judge what’s good for them in the moment.</p> <h3>The creative scar</h3> <p>Zhou et al. (Technology in Society, 2025) ran a 7-day lab experiment with 2-month follow-up. 61 participants, 3,593 ideas. <span class="ev ev-moderate" title="Longitudinal experiment, n=61, 2-month follow-up">◐</span></p> <p>Key finding: <strong>creativity drops remarkably when AI is withdrawn, and homogeneity keeps climbing even months later.</strong> The “creative scar” persists.</p> <blockquote><p>“Users do not truly acquire the ability to create but easily lose it once generative AI is no longer available.”</p></blockquote> <p>This is the longitudinal evidence that was missing. Not just correlation — capability degradation over time, persisting after AI removal.</p> <h3>The pattern across hypotheses</h3> <p>Each hypothesis has independent support from multiple studies, multiple methods, multiple domains. The convergence isn’t coincidental. These aren’t separate problems. They’re facets of the same dynamic: AI that substitutes for human cognition improves immediate output while degrading the foundation.</p> <p>The research doesn’t say AI is harmful. It says <em>substitutive</em> use is harmful. The same tools, used complementarily — with transparency, control, and active engagement — show neutral or positive effects on capability.</p> <p>Design determines outcome. The question is whether the collaboration is structured for complementarity or substitution.</p>',1);function d(e){var t=o();n(128),a(e,t)}export{d as default};
