<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../../_app/immutable/assets/0.BJicj5Zp.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/CrossLinks.C4T6atsV.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/2.mFuDagjh.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/TableOfContents.DIs43wTQ.css" rel="stylesheet">
		<link rel="modulepreload" href="../../_app/immutable/entry/start.Cjcq6Y6k.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/B-qQsNse.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DXXdKfFK.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DQIWNrPk.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/D0iwhpLH.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/CVdyUIvg.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/CzKX1IyC.js">
		<link rel="modulepreload" href="../../_app/immutable/entry/app.DpBALtll.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/PPVm8Dsz.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/i-IdDckD.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/BPxar_6W.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Ey95jBeI.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DcJ2xzpH.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/CpnKfs42.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DYueXv1s.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/EUq6mvGu.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/0.PCvfuuid.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Dl8jxmRm.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/C1v7wgkm.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Dvbz5Exl.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/CV11oyJS.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/CiTy23f9.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/2.8wx36KRJ.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/8.BixC_fJ9.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/DKRIfpFw.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/Dv3oOIco.js"><!--12qhfyh--><meta name="description" content="Extensions that enhance human capability, not replace it."/><!----><!--1wa4r3o--><!----><title>Article — cix Library</title>
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><a href="#main" class="skip-link">Skip to content</a> <!--[--><nav class="site-nav svelte-qgym72" aria-label="Site navigation"><a href="../../" class="nav-wordmark svelte-qgym72">cix</a> <div class="nav-links svelte-qgym72"><!--[--><a href="../../ethos" class="nav-link svelte-qgym72">ethos</a><a href="../../catalog" class="nav-link svelte-qgym72">catalog</a><a href="../../library" class="nav-link svelte-qgym72">library</a><!--]--></div></nav><!--]--> <div class="page svelte-12qhfyh has-nav"><!--[--><!----><main id="main" class="library-layout svelte-12dzc7l"><!----><div class="article-wrapper svelte-1wa4r3o"><nav class="article-breadcrumb svelte-1wa4r3o"><a href="../../library" class="svelte-1wa4r3o">library</a> <span class="breadcrumb-sep svelte-1wa4r3o">/</span> <a href="../../library#explanation" class="svelte-1wa4r3o">explanation</a></nav> <article class="prose svelte-1wa4r3o"><!----><h1>The Path Forward</h1> <p>Evidence-based practices that preserve human capability while using AI effectively.</p> <hr/> <h2>Sources</h2> <ul><li><a href="https://arxiv.org/pdf/2601.20245" rel="nofollow">Shen &amp; Tamkin (2026). How AI Impacts Skill Formation. Anthropic.</a></li> <li><a href="https://hdl.handle.net/10125/107542" rel="nofollow">Freise et al. (2025). Job Crafting with AI. HICSS.</a></li> <li><a href="https://dl.acm.org/doi/full/10.1145/3706598.3713778" rel="nofollow">Lee et al. (2025). The Impact of Generative AI on Critical Thinking. CHI.</a></li> <li><a href="https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2025.1510919/full" rel="nofollow">Tomisu et al. (2025). Cognitive Mirror. Frontiers in Education.</a></li> <li><a href="https://www.pnas.org/doi/10.1073/pnas.2422633122" rel="nofollow">Bastani et al. (2025). Generative AI Can Harm Learning. PNAS.</a></li> <li><a href="https://www.thelancet.com/journals/langas/article/PIIS2468-1253(24)00301-2/fulltext" rel="nofollow">Budzyń et al. (2025). Effect of AI-Assisted Colonoscopy. Lancet.</a></li> <li><a href="https://doi.org/10.3316/informit.106046894747026" rel="nofollow">Pallant et al. (2025). Mastery Orientation. ACU Research Bank.</a></li></ul> <hr/> <h2>Abstract</h2> <p>The problem is real. The solutions are also real.</p> <p>AI generates code, then the human asks follow-up questions — 86% mastery. AI generates, human accepts — 39% mastery. <span class="ev ev-moderate" title="RCT, n=52, Anthropic 2026">◐</span> The technology is identical. The variable is engagement, not who generates the code.</p> <p>Developers who assign AI to mundane work while reserving hard problems for themselves upskill. Developers who delegate hard problems to AI atrophy. <span class="ev ev-moderate" title="HICSS 2025, qualitative study">◐</span> The differentiator is task allocation.</p> <p>Mastery-oriented users maintain critical thinking at 35.7x the odds of performance-oriented users. <span class="ev ev-moderate" title="Single study, odds ratio">◐</span> The largest effect size in the literature. Frame interactions as learning, not just task completion, and capability compounds instead of eroding.</p> <p>Three months of AI-assisted work produces measurable skill degradation. <span class="ev ev-moderate" title="Lancet crossover RCT, medical domain">◐</span> Relearning takes less than 50% of original training time. <span class="ev ev-moderate" title="Cognitive science, multiple studies">◐</span> Periodic unassisted work prevents atrophy. Weekly practice maintains skills; quarterly deep work rebuilds them.</p> <hr/> <h2>Explanation</h2> <p>The research doesn’t just document what fails. It shows what works.</p> <h3>Generation-Then-Comprehension</h3> <p><strong>Source:</strong> Shen &amp; Tamkin (Anthropic 2026)</p> <p>The highest-performing interaction pattern wasn’t writing code without AI. It was AI generates, then human comprehends through questioning.</p> <table><thead><tr><th>Interaction Pattern</th><th>Mastery Score</th><th>What Happened</th></tr></thead><tbody><tr><td>Generation-Then-Comprehension</td><td>86%</td><td>AI generated code → human asked follow-up questions</td></tr><tr><td>Hybrid Code-Explanation</td><td>68%</td><td>Human requested explanations alongside code</td></tr><tr><td>Conceptual Inquiry</td><td>65%</td><td>Human asked conceptual questions, wrote code themselves</td></tr><tr><td>AI Delegation</td><td>39%</td><td>Human accepted AI code without engagement</td></tr><tr><td>Progressive Reliance</td><td>35%</td><td>Human started writing, gradually delegated more</td></tr><tr><td>Iterative Debugging</td><td>24%</td><td>AI fixed errors repeatedly without explanation</td></tr></tbody></table> <p><strong>The insight:</strong> AI generation is not the problem. Disengagement is the problem.</p> <p>The 86% mastery group generated code faster than the no-AI control group while learning more than those who wrote code themselves. They leveraged speed AND built understanding. The failure mode isn’t accepting AI output — it’s accepting without comprehending.</p> <p><strong>Pattern in practice:</strong></p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>✅ Human: "Explain how this async pattern handles cancellation."</span></span>
<span class="line"><span>✅ Human: "Walk me through why you chose this data structure."</span></span>
<span class="line"><span>✅ Human: "What edge cases does this miss?"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>❌ Human: "Looks good." [paste, commit, move on]</span></span></code></pre> <p>For learning contexts where foundational schema must be built, the interaction flips — human generates, AI critiques. <span class="ev ev-moderate" title="Same RCT, task variation">◐</span> This builds generative capability directly. But for established practitioners, AI generation followed by active comprehension is both faster and more educational.</p> <h3>Job Crafting</h3> <p><strong>Source:</strong> Freise et al. (HICSS 2025)</p> <p>Two task allocation patterns produce opposite outcomes.</p> <p><strong>Approach Crafting</strong> — assign AI to mundane work, reserve hard problems for yourself:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>AI: Boilerplate, CRUD, formatting, test scaffolding</span></span>
<span class="line"><span>Human: Architecture, domain models, edge cases, design decisions</span></span></code></pre> <p>Result: Human practices hard skills more, not less. AI removes friction that consumed cognitive budget. Skills compound.</p> <p><strong>Avoidance Crafting</strong> — use AI to avoid cognitively demanding tasks:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>AI: Complex algorithms, architecture, debugging hard problems</span></span>
<span class="line"><span>Human: Review, simple implementations, routine changes</span></span></code></pre> <p>Result: Human stops practicing the skills that matter most. Capability erodes. AI becomes essential because the human is no longer capable without it.</p> <p><strong>The differentiator:</strong> Not usage frequency. Not AI capability. What the human reserves for themselves.</p> <table><thead><tr><th>Reserved for Human</th><th>Trajectory</th></tr></thead><tbody><tr><td>Hard cognitive work (architecture, design, analysis)</td><td>Upskilling</td></tr><tr><td>Review and approval only</td><td>Atrophy</td></tr></tbody></table> <p><strong>Implementation:</strong></p> <p>When an AI offers to handle something, ask: “Is this routine or is this where I learn?”</p> <ul><li>Routine → delegate</li> <li>Cognitively challenging → keep</li></ul> <p>The goal is more hard problems per day, not fewer. AI should free bandwidth to tackle harder challenges, not eliminate challenge entirely.</p> <h3>Mastery Orientation</h3> <p><strong>Source:</strong> Pallant et al. (2025)</p> <p>Frame interactions as learning opportunities, not just task completion.</p> <table><thead><tr><th>Orientation</th><th>Critical Thinking Maintenance</th></tr></thead><tbody><tr><td>Mastery (focused on learning)</td><td>OR = 35.7</td></tr><tr><td>Performance (focused on output)</td><td>Baseline</td></tr></tbody></table> <p>Odds ratio of 35.7 is the largest effect size in the collaborative AI literature. <span class="ev ev-moderate" title="Single study, odds ratio">◐</span> It dominates control (β = 0.507) and transparency (β = 0.415) — the second and third strongest levers.</p> <p><strong>What mastery orientation looks like:</strong></p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>Performance: "Get this done fast."</span></span>
<span class="line"><span>Mastery: "What can I learn from this?"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Performance: "Did it work?"</span></span>
<span class="line"><span>Mastery: "Why did it work?"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Performance: "Ship and move on."</span></span>
<span class="line"><span>Mastery: "What's the transferable principle?"</span></span></code></pre> <p><strong>Implementation for extensions:</strong></p> <p>Frame every interaction as a learning moment:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>✅ "Let's understand why this approach works..."</span></span>
<span class="line"><span>✅ "What's the transferable principle here?"</span></span>
<span class="line"><span>✅ "Where might this pattern break down?"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>❌ "Here's the solution."</span></span>
<span class="line"><span>❌ "This will work."</span></span></code></pre> <p>The subtle shift from “getting it done” to “learning while getting it done” changes everything.</p> <h3>Recovery Protocols</h3> <p><strong>Source:</strong> Budzyń et al. (Lancet 2025), cognitive science literature</p> <p>Measurable skill degradation occurs in 3 months of AI-assisted work. <span class="ev ev-moderate" title="Lancet crossover RCT, medical domain">◐</span> But skills aren’t permanently lost — they’re dormant. Relearning takes less than 50% of original training time (the Savings Effect). <span class="ev ev-moderate" title="Cognitive science, multiple studies">◐</span></p> <p>Three evidence-based approaches:</p> <p><strong>1. Switch-Off Protocol</strong></p> <p>Periodic work without AI assistance.</p> <table><thead><tr><th>Frequency</th><th>Duration</th><th>Purpose</th></tr></thead><tbody><tr><td>Weekly</td><td>2-4 hours</td><td>Maintain baseline capability</td></tr><tr><td>Monthly</td><td>Full day</td><td>Test independent function</td></tr><tr><td>Quarterly</td><td>Complex task end-to-end</td><td>Deep capability assessment</td></tr></tbody></table> <p>The work must be cognitively challenging. Doing easy tasks without AI doesn’t exercise the skills at risk. Architecture decisions, debugging complex issues, designing from scratch — these maintain the capabilities most vulnerable to atrophy.</p> <p><strong>2. Simulator Protocol</strong></p> <p>Attempt-first before AI consultation. <span class="ev ev-speculative" title="Aviation training analogy + Bastani PNAS findings">◌</span></p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>Before AI assistance: 15-30 minutes of independent effort</span></span>
<span class="line"><span>"What's your diagnosis?"</span></span>
<span class="line"><span>"Walk me through your approach."</span></span></code></pre> <p>The effort itself is valuable, even if the conclusion is wrong. This preserves problem-solving practice while still leveraging AI for efficiency. The human works harder per problem, but solves more problems per day.</p> <p><strong>3. Hybrid Protocol</strong></p> <p>Generation-Then-Comprehension as daily practice (see above). AI generates, human comprehends through questioning. Speed is preserved, understanding compounds.</p> <h3>Metacognitive Scaffolding</h3> <p><strong>Source:</strong> Lee et al. (CHI 2025), Tomisu et al. (2025)</p> <p>Three techniques that restore critical engagement.</p> <p><strong>1. PME Friction</strong> — Planning, Monitoring, Evaluation checkpoints</p> <table><thead><tr><th>Phase</th><th>Prompt</th><th>Effect</th></tr></thead><tbody><tr><td>Planning</td><td>“What’s your approach before I assist?”</td><td>Preserves generative step</td></tr><tr><td>Monitoring</td><td>“Does this match what you expected?”</td><td>Maintains engagement</td></tr><tr><td>Evaluation</td><td>“What would you change next time?”</td><td>Crystallizes learning</td></tr></tbody></table> <p>Lee et al. found three-component friction significantly restored metacognitive engagement that was otherwise suppressed by AI confidence. <span class="ev ev-strong" title="CHI peer-reviewed, n=319, SEM">●</span> Single-point friction was insufficient — all three phases needed.</p> <p><strong>2. Cognitive Mirror</strong> — reflect reasoning back with questions</p> <p>Instead of answering directly, mirror the human’s thinking:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>Human: "Should I use Redis or PostgreSQL for this cache?"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Substitutive: "Use Redis — it's faster for caching."</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Cognitive Mirror:</span></span>
<span class="line"><span>"You're thinking about caching. Let me reflect what I see:</span></span>
<span class="line"><span>- You have a read-heavy workload</span></span>
<span class="line"><span>- You mentioned durability matters for this data</span></span>
<span class="line"><span>- You're already running PostgreSQL</span></span>
<span class="line"><span></span></span>
<span class="line"><span>What's driving the instinct toward Redis specifically?</span></span>
<span class="line"><span>What happens if the cache goes down?"</span></span></code></pre> <p>This forces articulation of reasoning, evaluation of logic, and discovery of gaps. The human builds transferable frameworks instead of receiving one-time answers. <span class="ev ev-moderate" title="Frontiers in Education, qualitative study">◐</span></p> <p><strong>3. Confidence-Competence Calibration</strong></p> <p>Two confidence signals with opposite effects:</p> <table><thead><tr><th>Confidence Type</th><th>Effect on Critical Thinking</th><th>Mechanism</th></tr></thead><tbody><tr><td>AI-confidence (trust in AI)</td><td>β = -0.69</td><td>“AI is reliable” → cognitive offloading</td></tr><tr><td>Self-confidence (trust in self)</td><td>β = +0.35</td><td>“I can evaluate” → active engagement</td></tr></tbody></table> <p><strong>Design response:</strong> Reduce AI-confidence signals, boost self-confidence signals.</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>❌ "The answer is X." (projects authority)</span></span>
<span class="line"><span>✅ "Based on what I see, X seems right — but you have</span></span>
<span class="line"><span>    context I lack. What does your experience suggest?"</span></span></code></pre> <p>High AI confidence → low human engagement. Moderate AI confidence + high self-confidence → sustained thinking quality. <span class="ev ev-strong" title="CHI peer-reviewed, n=319, SEM">●</span></p> <h3>Novice Protection</h3> <p><strong>Source:</strong> Bastani et al. (PNAS 2025)</p> <p>Direct AI answers harm learning for novices. Hint-only AI shows no harm. <span class="ev ev-strong" title="RCT, n=1,000, PNAS">●</span></p> <table><thead><tr><th>Experience Level</th><th>Recommended Interaction</th><th>Why</th></tr></thead><tbody><tr><td>Novice (0-2 years)</td><td>Hint-only, Socratic, explain reasoning</td><td>Schema not yet formed</td></tr><tr><td>Intermediate (2-5 years)</td><td>Collaborative with verification</td><td>Schema needs reinforcement</td></tr><tr><td>Expert (5+ years)</td><td>Full collaboration</td><td>Schema robust</td></tr></tbody></table> <p><strong>What this looks like:</strong></p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>Novice request: "How do I handle null?"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>❌ "Add if (value != null) on line 12."</span></span>
<span class="line"><span>✅ "What happens when this function receives null?</span></span>
<span class="line"><span>    Walk me through the execution."</span></span></code></pre> <p>The novice must build mental models of program execution, debugging intuition, design reasoning. These form through struggle. Providing solutions prevents formation of the schema that makes someone capable.</p> <p>For intermediates and experts, full collaboration preserves capability because the schema already exists. The judgment to evaluate AI output is present. For novices, it isn’t yet.</p> <h3>Protective Factors</h3> <p>Characteristics that predict maintained capability during AI use:</p> <table><thead><tr><th>Factor</th><th>Effect</th><th>Mechanism</th></tr></thead><tbody><tr><td>Skepticism</td><td>Maintains verification</td><td>Senior devs trust AI least (2.5%) but use effectively</td></tr><tr><td>Second-reader approach</td><td>Treats AI output as draft</td><td>Preserves evaluative practice</td></tr><tr><td>Deep domain expertise</td><td>Can evaluate meaningfully</td><td>Judgment precedes collaboration</td></tr><tr><td>Active direction</td><td>Tells AI what to do</td><td>Preserves agency</td></tr><tr><td>High self-confidence</td><td>β = +0.35 for critical thinking</td><td>Willing to override AI</td></tr></tbody></table> <p>The pattern: users who maintain capability treat AI as a tool they control, not an authority they obey.</p> <p><strong>Implication:</strong> Extensions should support active direction, encourage verification, surface uncertainty, and affirm the human’s capability to evaluate.</p> <h3>The Scaffolding Metaphor</h3> <p><strong>Source:</strong> Vygotsky’s Zone of Proximal Development</p> <p>Scaffolding in construction is temporary support designed to be removed. The goal is a building that stands alone.</p> <p>AI collaboration should work the same way. The goal isn’t permanent dependency. The goal is temporary support that enables capability the human couldn’t achieve alone — then becomes unnecessary as that capability internalizes.</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>Day 1: AI explains async patterns, human doesn't yet understand</span></span>
<span class="line"><span>Week 2: AI reminds of edge cases, human catches most</span></span>
<span class="line"><span>Month 3: Human designs async patterns fluently, AI rarely needed</span></span></code></pre> <p>If the pattern goes the other direction — more dependency over time — the design has failed.</p> <h3>Success Metrics</h3> <p>How to know if collaboration is working:</p> <p><strong>Positive signals:</strong></p> <ul><li>“I would approach this differently now than a month ago”</li> <li>“I caught an error in AI output I wouldn’t have noticed before”</li> <li>“I understand why this works, not just that it works”</li> <li>Human spends more time on harder problems than before</li></ul> <p><strong>Warning signals:</strong></p> <ul><li>“I don’t know how I’d do this without AI”</li> <li>“I trust the output without checking”</li> <li>“I feel less confident in my judgment than before”</li> <li>Difficulty working unassisted for short periods</li></ul> <p>The goal is compounding capability, not compounding dependency.</p> <hr/> <h2>Key Principles</h2> <p>Synthesizing the evidence into actionable guidance:</p> <ol><li><p><strong>Engagement over generation</strong> — Who generates code matters less than whether the human comprehends it. AI can generate; the human must understand.</p></li> <li><p><strong>Reserve hard problems</strong> — Delegate routine work. Keep cognitively challenging work. The hard work is where learning happens.</p></li> <li><p><strong>Frame as learning</strong> — Mastery orientation (OR = 35.7) dominates all other factors. “What can I learn?” beats “how fast can I ship?” for long-term capability.</p></li> <li><p><strong>Practice unassisted</strong> — Weekly 2-4 hours maintains skills. Monthly full days test capability. Quarterly complex tasks rebuild atrophied skills.</p></li> <li><p><strong>Scaffold, don’t substitute</strong> — Temporary support that builds capability, not permanent crutches that create dependency.</p></li> <li><p><strong>Protect novices</strong> — Hints and explanations build schema. Direct answers prevent formation of foundational understanding.</p></li> <li><p><strong>Surface uncertainty</strong> — High AI confidence reduces human thinking. Show reasoning, acknowledge limits, invite verification.</p></li> <li><p><strong>Multiple checkpoints</strong> — Planning, monitoring, evaluation. Single-point friction is insufficient.</p></li></ol> <p>These aren’t expensive. They require intentionality in design.</p> <hr/> <h2>The Hope</h2> <p>The problem is real. Cognitive offloading is real. Skill atrophy is real. Perception gaps are real.</p> <p>The solutions are also real.</p> <p>AI that explains its reasoning maintains critical thinking. AI that invites questions builds understanding. AI that surfaces uncertainty preserves verification. AI that frames interactions as learning produces 35.7x better outcomes than AI that optimizes for task completion.</p> <p>The same technology, designed differently, produces opposite trajectories. Not “use AI less” — use AI differently. Generation-Then-Comprehension users learned more than those who worked without AI while completing tasks faster. Both learning and productivity increased.</p> <p>The path forward exists. The evidence shows what works. The question is whether the defaults we establish now create positive or negative compounding.</p> <p>Design determines the trajectory.</p><!----></article> <!--[--><nav class="article-nav svelte-1l1vath" aria-label="Article navigation"><span class="article-position svelte-1l1vath">03 of 13</span> <div class="article-nav-links svelte-1l1vath"><!--[--><a href="../../library/explanation/the-risks" class="nav-prev svelte-1l1vath"><span class="nav-arrow svelte-1l1vath">←</span> <span class="nav-label">The Risks</span></a><!--]--> <!--[--><a href="../../library/explanation/the-problem" class="nav-next svelte-1l1vath"><span class="nav-label">The Problem</span> <span class="nav-arrow svelte-1l1vath">→</span></a><!--]--></div></nav><!--]--></div><!----><!----></main><!----><!--]--><!----></div> <div class="experimental-tag svelte-12qhfyh" aria-hidden="true">experimental</div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_12bsqvx = {
						base: new URL("../..", location).pathname.slice(0, -1),
						assets: "/cix/pr-preview/pr-20"
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../../_app/immutable/entry/start.Cjcq6Y6k.js"),
						import("../../_app/immutable/entry/app.DpBALtll.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 2, 8],
							data: [null,null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
