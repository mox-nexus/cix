import{a as o,f as n}from"./BKQXIwCx.js";import"./BLJ2k1vR.js";import{y as r}from"./Cfark-2s.js";var a=n(`<h1 id="skill-formation">Skill Formation</h1> <p>The interaction pattern, not the technology, determines whether you learn.</p> <hr/> <p>You’re learning a new Python library. You’ve never used Trio before—async programming, concurrency, error handling patterns all new to you. You have two hours to build a working prototype.</p> <p>You could grind through the docs, hit errors, debug stack traces, slowly piece together how it works. Or you could ask Claude to generate the code, paste it in, ship it.</p> <p>Both get you to working code. Only one leaves you more capable.</p> <h2 id="the-six-patterns">The Six Patterns</h2> <p>Anthropic ran an experiment with 52 software engineers learning Trio. <span class="ev ev-moderate" title="RCT, n=52, Anthropic 2026">◐</span> Same AI assistant. Same task. Six different ways people chose to interact.</p> <p>Three patterns preserved learning. Three destroyed it.</p> <p><strong>Generation-Then-Comprehension</strong> (86% mastery): Generate code with AI, then ask follow-up questions to understand what it did. “Why did you use <code>trio.open_nursery()</code> here? What happens if I remove this line?”</p> <p><strong>Hybrid Code-Explanation</strong> (68% mastery): Request explanations alongside code. “Show me how to handle timeouts in Trio and explain when each pattern applies.”</p> <p><strong>Conceptual Inquiry</strong> (65% mastery): Only ask conceptual questions. Write all code yourself. “What’s the difference between structured and unstructured concurrency?”</p> <p>These three patterns have something in common: errors. Users hit TypeError exceptions, RuntimeWarnings, async context issues. They debugged them. That debugging is where learning happened.</p> <p>Now the three patterns that failed:</p> <p><strong>AI Delegation</strong> (39% mastery): Ask AI to generate code, paste it, move on. Fastest completion time (19.5 minutes). Worst learning outcome.</p> <p><strong>Progressive Reliance</strong> (35% mastery): Start writing code yourself, gradually hand more to the AI as you realize it’s faster. Watch your understanding erode in real-time.</p> <p><strong>Iterative Debugging</strong> (24% mastery): Have AI fix your errors without understanding what broke or why the fix works. The error message becomes a handoff, not a learning signal.</p> <p>The gap between best and worst: <strong>86% vs 24%</strong>. Same AI. Same task. Different interaction choices.</p> <h2 id="errors-are-the-curriculum">Errors Are the Curriculum</h2> <p>The control group—no AI access—encountered a median of 3 errors during the task. <span class="ev ev-moderate" title="Same RCT, n=52">◐</span> The AI group encountered 1.</p> <p>Those two extra errors weren’t obstacles. They were the lesson.</p> <p>When you hit <code>TypeError: 'async for' requires an object with __aiter__ method</code>, you learn something about Python’s async protocol. When you see <code>RuntimeWarning: Trio cannot be run from a running event loop</code>, you learn about execution contexts. When your timeout doesn’t work, you learn about cancellation scopes.</p> <p>AI users who delegated never encountered these. They copied working code and moved on. The code ran. They learned nothing about why.</p> <p>The errors weren’t bugs in the process. The errors <em>were</em> the process.</p> <h2 id="what-people-noticed">What People Noticed</h2> <p>The participants who delegated to AI knew something was wrong:</p> <ul><li>“I feel like I got lazy”</li> <li>“There are still a lot of gaps in my understanding”</li> <li>“I wish I’d taken the time to understand the explanations more”</li></ul> <p>They completed the task. They passed the immediate test—ship working code. But they failed the actual test: can you do this again without the AI?</p> <p>The mastery assessment came later, after the task, without AI access. That’s where the 86% vs 24% gap appeared. The people who delegated couldn’t reconstruct what they’d built. They’d outsourced not just the typing, but the thinking.</p> <h2 id="productivity-vs-capability">Productivity vs Capability</h2> <p>The AI Delegation pattern produced the fastest task completion. It also produced the worst learning.</p> <p>The Generation-Then-Comprehension pattern took 23% longer. It produced 2.2x better learning outcomes.</p> <p>This is the tradeoff: you can optimize for speed today or capability tomorrow.</p> <p>If you’re learning something new—a framework, a library, a domain—the interaction pattern determines which path you’re on:</p> <p><strong>Path 1: Capability compounds</strong> You generate code with AI, then interrogate it. Why this pattern? What does this function do? What happens if I change this parameter? You hit errors. You debug them. You emerge able to work in this domain with or without AI.</p> <p><strong>Path 2: Dependency compounds</strong> You ask AI to solve the problem. You paste the solution. You move to the next problem. You complete tasks faster. Your ability to work independently atrophies. Six months later, you can’t remember how any of it works without the AI.</p> <p>The gap between these paths is measurable. It’s not subtle. It’s 86% mastery vs 24%.</p> <h2 id="the-mechanism">The Mechanism</h2> <p>Why does Generation-Then-Comprehension work when AI Delegation fails?</p> <p>The difference is <strong>error exposure</strong>.</p> <p>When you generate code and then ask questions, you’re still in the loop. You see what the AI produced. You ask why. Those questions create cognitive load. You’re forced to reconcile the code with your mental model. The mismatch triggers learning.</p> <p>When you debug errors—even AI-generated code—you’re building pattern recognition. This error message maps to this mistake. This warning means I misunderstood this concept. Over time, you internalize the library’s execution model.</p> <p>When you delegate fully, none of this happens. The code works. Your mental model stays shallow. You never reconcile anything because you never look closely enough to notice the gaps.</p> <p>The AI becomes a black box. Put problem in, get solution out. Learning requires opening the box.</p> <h2 id="what-this-means">What This Means</h2> <p>If you’re building something you already know how to build, AI acceleration makes sense. Generate, review, ship.</p> <p>If you’re learning something new, the pattern shifts:</p> <ol><li><strong>Ask the AI to generate</strong>, but don’t stop there</li> <li><strong>Ask follow-up questions</strong> about what it produced</li> <li><strong>Let errors happen</strong>—they’re teaching you</li> <li><strong>Debug with understanding</strong>, not just iteration</li></ol> <p>The goal isn’t to avoid AI. The goal is to use it in ways that make you more capable, not more dependent.</p> <p>The difference between 86% and 24% mastery is entirely within your control. It’s not about the AI. It’s about how you choose to interact with it.</p> <hr/> <p><a href="../reference/skill-formation-evidence">Full skill formation evidence →</a></p>`,1);function p(e){var t=a();r(102),o(e,t)}export{p as default};
