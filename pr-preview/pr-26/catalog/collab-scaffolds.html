<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../_app/immutable/assets/0.04mfEt8Y.css" rel="stylesheet">
		<link href="../_app/immutable/assets/CrossLinks.C4T6atsV.css" rel="stylesheet">
		<link href="../_app/immutable/assets/5.Bnuf7QZ4.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.BLE4b1M5.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DXMGIgAt.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/iyO_HpW3.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_cEvoN5f.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D0iwhpLH.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CsDgnD6d.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Cs0QLTHR.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.NR1UseVB.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BPYyHSBz.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/2hgU3PCP.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/C4-dLuTv.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_Dz5Oby6.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DtLL33Bf.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/a8oTfHeT.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CFsY3MpS.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.0J44xLBv.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BwsKPAvP.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BbhyhsYm.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D1PGsUc-.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/5.Bagyn9me.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BFxfVKZg.js"><!--12qhfyh--><meta name="description" content="Extensions that enhance human capability, not replace it."/><!----><!--wzm642--><meta name="description" content="Research-grounded scaffolds for effective human-AI collaboration. Use when: making decisions, verifying claims, calibrating trust, metacognitive reflection, building with transparency and control."/><!----><title>collab-scaffolds — cix</title>
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><a href="#main" class="skip-link">Skip to content</a> <!--[--><nav class="site-nav svelte-qgym72" aria-label="Site navigation"><a href="../" class="nav-wordmark svelte-qgym72">cix</a> <div class="nav-links svelte-qgym72"><!--[--><a href="../ethos" class="nav-link svelte-qgym72">ethos</a><a href="../catalog" class="nav-link svelte-qgym72">catalog</a><a href="../library" class="nav-link svelte-qgym72">library</a><!--]--></div></nav><!--]--> <div class="page svelte-12qhfyh has-nav"><!--[!--><!----><main id="main" class="detail-page svelte-wzm642" style="--variant-color: var(--emergence-core)"><nav class="detail-back svelte-wzm642"><a href="../catalog" class="svelte-wzm642">← catalog</a></nav> <header class="detail-header svelte-wzm642"><div class="header-top svelte-wzm642"><h1 class="svelte-wzm642">collab-scaffolds</h1> <span class="detail-kind svelte-wzm642">plugin</span> <span class="detail-version svelte-wzm642">0.5.0</span></div> <p class="detail-description svelte-wzm642">Research-grounded scaffolds for effective human-AI collaboration. Use when: making decisions, verifying claims, calibrating trust, metacognitive reflection, building with transparency and control.</p> <!--[--><div class="detail-inventory svelte-wzm642"><!--[--><span class="inv-item svelte-wzm642">2 agents</span><span class="inv-item svelte-wzm642">3 skills</span><span class="inv-item svelte-wzm642">4 hooks</span><!--]--></div><!--]--> <!--[--><div class="detail-tags svelte-wzm642"><!--[--><span class="tag svelte-wzm642">collaborative-intelligence</span><span class="tag svelte-wzm642">reasoning</span><span class="tag svelte-wzm642">verification</span><span class="tag svelte-wzm642">transparency</span><span class="tag svelte-wzm642">control</span><!--]--></div><!--]--></header> <!--[--><nav class="detail-tabs svelte-wzm642" role="tablist"><!--[--><button role="tab" class="tab svelte-wzm642 active" aria-selected="true">README <!--[!--><!--]--></button><button role="tab" class="tab svelte-wzm642" aria-selected="false">Explanation <!--[--><span class="tab-count svelte-wzm642">2</span><!--]--></button><button role="tab" class="tab svelte-wzm642" aria-selected="false">How-To <!--[--><span class="tab-count svelte-wzm642">4</span><!--]--></button><button role="tab" class="tab svelte-wzm642" aria-selected="false">Tutorials <!--[--><span class="tab-count svelte-wzm642">3</span><!--]--></button><!--]--></nav><!--]--> <article class="detail-content svelte-wzm642 prose"><!--[--><!----><h1>collab-scaffolds</h1>
<p>Research-grounded scaffolds for effective human-AI collaboration.</p>
<h2>What This Is</h2>
<p>Scaffolding for collaborative building — temporary support designed to be outgrown. Every scaffold is backed by research evidence from 50+ studies (CHI, PNAS, Lancet, NeurIPS, HICSS). Three skills, two agents, four hooks, aligned with the Software Craftsmanship Manifesto.</p>
<h2>When It Activates</h2>
<ul>
<li>Writing code or making technical decisions</li>
<li>Refactoring and code reviews</li>
<li>Debugging (especially when stuck)</li>
<li>Verifying claims or calibrating trust</li>
<li>Any context where quality matters</li>
</ul>
<h2>What You Get</h2>
<h3>Skills</h3>
<p><strong>building</strong> — Engineering craft for well-crafted software:</p>
<ul>
<li>Building principles (compound value, complete the work, craft over speed)</li>
<li>Workflows (refactoring, scaffolding, fidelity thinking, evidence before fix)</li>
<li>Verification (three checks, code hygiene, test integrity)</li>
<li>Advisory vs Enforced behavior mapping</li>
</ul>
<p><strong>collaboration</strong> — Human-AI partnership patterns:</p>
<ul>
<li>Scaffolding philosophy (Vygotsky ZPD)</li>
<li>Collaboration patterns (Generation-Then-Comprehension, Task Stewardship)</li>
<li>Trust calibration (evidence levels, contrastive explanations, falsification)</li>
<li>Control and transparency frameworks</li>
</ul>
<p><strong>problem-solving</strong> — Structured thinking and metacognition:</p>
<ul>
<li>Wolf Protocol (stop → classify → route → verify)</li>
<li>Metacognitive scaffolds (Cognitive Mirror, PME Friction, HypoCompass)</li>
<li>Problem → Technique routing (DAC, Five Whys, OODA, Hypothesis Testing)</li>
<li>Verification (CoVe, contrastive explanations)</li>
<li>Iteration awareness and context health monitoring</li>
</ul>
<h3>Agents</h3>
<p><strong>Mr. Wolf</strong> — Structured problem solver. Gets called when you&#39;re stuck, going in circles, or debugging isn&#39;t converging. Loads <code>problem-solving</code> skill.</p>
<p><strong>Duck</strong> — Rubber duck debugging through Socratic dialogue. Helps you think through problems by asking questions, not giving answers. Loads <code>problem-solving</code> skill.</p>
<table>
<thead>
<tr>
<th></th>
<th>Duck</th>
<th>Wolf</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Trigger</strong></td>
<td>&quot;I need to think this through&quot;</td>
<td>&quot;I&#39;m stuck, nothing&#39;s working&quot;</td>
</tr>
<tr>
<td><strong>Mode</strong></td>
<td>Socratic — asks questions</td>
<td>Directive — routes to technique</td>
</tr>
<tr>
<td><strong>Goal</strong></td>
<td>You articulate → you discover</td>
<td>Problem classified → solved</td>
</tr>
</tbody></table>
<h3>Hooks</h3>
<table>
<thead>
<tr>
<th>Hook</th>
<th>Event</th>
<th>Detects</th>
<th>Response</th>
</tr>
</thead>
<tbody><tr>
<td><code>detect-stuck</code></td>
<td>PostToolUse:Bash + UserPromptSubmit</td>
<td>3+ failures OR frustration language</td>
<td>Spawns Mr. Wolf</td>
</tr>
<tr>
<td><code>scaffolding-cleanup-gate</code></td>
<td>PreToolUse:Bash</td>
<td>Debug artifacts in commits</td>
<td>Blocks commit with list</td>
</tr>
<tr>
<td><code>incomplete-refactoring-guard</code></td>
<td>PostToolUse:Bash</td>
<td>Old names after rename</td>
<td>Directs cleanup</td>
</tr>
<tr>
<td><code>session-start</code></td>
<td>SessionStart</td>
<td>Session begins</td>
<td>Shows available skills</td>
</tr>
</tbody></table>
<p>All hooks are <strong>suggestive</strong> (decision: &quot;allow&quot; with message), never blocking — preserving user agency while nudging behavior. Opt-out: <code>SKIP_MRWOLF_HOOKS=1</code> or <code>SKIP_CLEANUP_HOOKS=1</code>.</p>
<h2>Structure</h2>
<pre><code>collab-scaffolds/
├── skills/
│   ├── building/                 # Engineering craft
│   │   ├── SKILL.md              # Principles, workflows, verification (&lt; 250 lines)
│   │   └── references/           # Deep dives (7 files)
│   │       ├── verification-patterns.md
│   │       ├── principles-and-patterns-examples.md
│   │       ├── writing-antipatterns.md
│   │       ├── kaizen-crystallization.md
│   │       ├── enforcement-spectrum.md
│   │       ├── fidelity-thinking.md
│   │       └── refactoring-completeness.md
│   ├── collaboration/            # Human-AI partnership
│   │   ├── SKILL.md              # Trust, control, transparency (&lt; 200 lines)
│   │   └── references/           # Deep dives (4 files)
│   │       ├── trust-calibration.md
│   │       ├── skill-preservation.md
│   │       ├── productivity-reality.md
│   │       └── behavioral-awareness.md
│   └── problem-solving/          # Structured thinking &amp; metacognition
│       ├── SKILL.md              # Wolf Protocol, routing, verification (&lt; 280 lines)
│       └── references/           # Deep dives (4 files)
│           ├── metacognitive-scaffolding.md
│           ├── reasoning-scaffolds.md
│           ├── reasoning-verification.md
│           └── iteration-limits.md
├── agents/
│   ├── mrwolf.md                 # Structured problem solver
│   └── duck.md                   # Rubber duck (Socratic dialogue)
├── hooks/
│   ├── hooks.json
│   ├── detect-stuck.sh                  # Wolf trigger (failures + frustration)
│   ├── scaffolding-cleanup-gate.sh      # Pre-commit debug artifact check
│   ├── incomplete-refactoring-guard.sh  # Post-commit old name check
│   └── session-start.sh                 # Context loading
├── docs/
│   ├── explanation/              # Human-optimized (WHY)
│   │   ├── methodology.md
│   │   └── sources.md
│   ├── how-to/                   # Human-optimized (HOW)
│   │   ├── recognize-debugging-loops.md
│   │   ├── calibrate-trust.md
│   │   ├── apply-fidelity-thinking.md
│   │   └── verify-refactoring.md
│   └── tutorials/                # Human-optimized (LEARN)
│       ├── mastery-oriented-session.md
│       ├── debugging-with-mrwolf.md
│       └── rubber-duck-with-duck.md
└── scripts/                      # Installation helpers
</code></pre>
<h2>Philosophy</h2>
<p><strong>You&#39;re not done when it works. You&#39;re done when it&#39;s right.</strong></p>
<p>Everything you create becomes part of a system others depend on. Scaffolding, not crutches — temporary support designed to make humans more capable, not dependent.</p>
<!----><!--]--></article></main><!----><!--]--><!----></div> <div class="experimental-tag svelte-12qhfyh" aria-hidden="true">experimental</div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_mzx2qb = {
						base: new URL("..", location).pathname.slice(0, -1),
						assets: "/cix/pr-preview/pr-26"
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.BLE4b1M5.js"),
						import("../_app/immutable/entry/app.NR1UseVB.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 5],
							data: [null,(function(a){a[0]="collaborative-intelligence";a[1]="reasoning";a[2]="verification";a[3]="transparency";a[4]="control";return {type:"data",data:{extension:{slug:"collab-scaffolds",kind:"plugin",manifest:{name:"collab-scaffolds",version:"0.5.0",description:"Research-grounded scaffolds for effective human-AI collaboration. Use when: making decisions, verifying claims, calibrating trust, metacognitive reflection, building with transparency and control.",author:{name:"Mox Labs",email:"mox.rnd@gmail.com"},license:"MIT",keywords:a},tagline:"Research-grounded scaffolds for effective human-AI collaboration.",readme:"# collab-scaffolds\n\nResearch-grounded scaffolds for effective human-AI collaboration.\n\n## What This Is\n\nScaffolding for collaborative building — temporary support designed to be outgrown. Every scaffold is backed by research evidence from 50+ studies (CHI, PNAS, Lancet, NeurIPS, HICSS). Three skills, two agents, four hooks, aligned with the Software Craftsmanship Manifesto.\n\n## When It Activates\n\n- Writing code or making technical decisions\n- Refactoring and code reviews\n- Debugging (especially when stuck)\n- Verifying claims or calibrating trust\n- Any context where quality matters\n\n## What You Get\n\n### Skills\n\n**building** — Engineering craft for well-crafted software:\n- Building principles (compound value, complete the work, craft over speed)\n- Workflows (refactoring, scaffolding, fidelity thinking, evidence before fix)\n- Verification (three checks, code hygiene, test integrity)\n- Advisory vs Enforced behavior mapping\n\n**collaboration** — Human-AI partnership patterns:\n- Scaffolding philosophy (Vygotsky ZPD)\n- Collaboration patterns (Generation-Then-Comprehension, Task Stewardship)\n- Trust calibration (evidence levels, contrastive explanations, falsification)\n- Control and transparency frameworks\n\n**problem-solving** — Structured thinking and metacognition:\n- Wolf Protocol (stop → classify → route → verify)\n- Metacognitive scaffolds (Cognitive Mirror, PME Friction, HypoCompass)\n- Problem → Technique routing (DAC, Five Whys, OODA, Hypothesis Testing)\n- Verification (CoVe, contrastive explanations)\n- Iteration awareness and context health monitoring\n\n### Agents\n\n**Mr. Wolf** — Structured problem solver. Gets called when you're stuck, going in circles, or debugging isn't converging. Loads `problem-solving` skill.\n\n**Duck** — Rubber duck debugging through Socratic dialogue. Helps you think through problems by asking questions, not giving answers. Loads `problem-solving` skill.\n\n| | Duck | Wolf |\n|---|------|------|\n| **Trigger** | \"I need to think this through\" | \"I'm stuck, nothing's working\" |\n| **Mode** | Socratic — asks questions | Directive — routes to technique |\n| **Goal** | You articulate → you discover | Problem classified → solved |\n\n### Hooks\n\n| Hook | Event | Detects | Response |\n|------|-------|---------|----------|\n| `detect-stuck` | PostToolUse:Bash + UserPromptSubmit | 3+ failures OR frustration language | Spawns Mr. Wolf |\n| `scaffolding-cleanup-gate` | PreToolUse:Bash | Debug artifacts in commits | Blocks commit with list |\n| `incomplete-refactoring-guard` | PostToolUse:Bash | Old names after rename | Directs cleanup |\n| `session-start` | SessionStart | Session begins | Shows available skills |\n\nAll hooks are **suggestive** (decision: \"allow\" with message), never blocking — preserving user agency while nudging behavior. Opt-out: `SKIP_MRWOLF_HOOKS=1` or `SKIP_CLEANUP_HOOKS=1`.\n\n## Structure\n\n```\ncollab-scaffolds/\n├── skills/\n│   ├── building/                 # Engineering craft\n│   │   ├── SKILL.md              # Principles, workflows, verification (\u003C 250 lines)\n│   │   └── references/           # Deep dives (7 files)\n│   │       ├── verification-patterns.md\n│   │       ├── principles-and-patterns-examples.md\n│   │       ├── writing-antipatterns.md\n│   │       ├── kaizen-crystallization.md\n│   │       ├── enforcement-spectrum.md\n│   │       ├── fidelity-thinking.md\n│   │       └── refactoring-completeness.md\n│   ├── collaboration/            # Human-AI partnership\n│   │   ├── SKILL.md              # Trust, control, transparency (\u003C 200 lines)\n│   │   └── references/           # Deep dives (4 files)\n│   │       ├── trust-calibration.md\n│   │       ├── skill-preservation.md\n│   │       ├── productivity-reality.md\n│   │       └── behavioral-awareness.md\n│   └── problem-solving/          # Structured thinking & metacognition\n│       ├── SKILL.md              # Wolf Protocol, routing, verification (\u003C 280 lines)\n│       └── references/           # Deep dives (4 files)\n│           ├── metacognitive-scaffolding.md\n│           ├── reasoning-scaffolds.md\n│           ├── reasoning-verification.md\n│           └── iteration-limits.md\n├── agents/\n│   ├── mrwolf.md                 # Structured problem solver\n│   └── duck.md                   # Rubber duck (Socratic dialogue)\n├── hooks/\n│   ├── hooks.json\n│   ├── detect-stuck.sh                  # Wolf trigger (failures + frustration)\n│   ├── scaffolding-cleanup-gate.sh      # Pre-commit debug artifact check\n│   ├── incomplete-refactoring-guard.sh  # Post-commit old name check\n│   └── session-start.sh                 # Context loading\n├── docs/\n│   ├── explanation/              # Human-optimized (WHY)\n│   │   ├── methodology.md\n│   │   └── sources.md\n│   ├── how-to/                   # Human-optimized (HOW)\n│   │   ├── recognize-debugging-loops.md\n│   │   ├── calibrate-trust.md\n│   │   ├── apply-fidelity-thinking.md\n│   │   └── verify-refactoring.md\n│   └── tutorials/                # Human-optimized (LEARN)\n│       ├── mastery-oriented-session.md\n│       ├── debugging-with-mrwolf.md\n│       └── rubber-duck-with-duck.md\n└── scripts/                      # Installation helpers\n```\n\n## Philosophy\n\n**You're not done when it works. You're done when it's right.**\n\nEverything you create becomes part of a system others depend on. Scaffolding, not crutches — temporary support designed to make humans more capable, not dependent.\n",components:{agents:2,skills:3,hooks:4,commands:0},variant:"emergence",tags:a,docs:{explanation:[{slug:"methodology",title:"Collaborative Building Scaffolds: Methodology",content:"# Collaborative Building Scaffolds: Methodology\n\nWhy these scaffolds exist and the research behind them.\n\n---\n\n## Contents\n\n- [Who This Is For](#who-this-is-for)\n- [The Foundation Problem](#the-foundation-problem)\n- [Compound Value vs Technical Debt](#compound-value-vs-technical-debt)\n- [Pit of Success](#pit-of-success)\n- [Evidence Over Opinion](#evidence-over-opinion)\n- [Collaboration Design](#collaboration-design)\n- [Verification Patterns](#verification-patterns)\n- [Why Mastery Orientation Matters](#why-mastery-orientation-matters)\n- [The Feynman Principle](#the-feynman-principle)\n- [Crystallization vs Accumulation](#crystallization-vs-accumulation)\n- [Why These Principles Persist](#why-these-principles-persist)\n- [Metacognition Research (2025-2026)](#metacognition-research-2025-2026)\n- [Trust Calibration Research (2025-2026)](#trust-calibration-research-2025-2026)\n- [Skill Preservation Research (2025-2026)](#skill-preservation-research-2025-2026)\n- [Productivity Reality (2025-2026)](#productivity-reality-2025-2026)\n- [Study Limitations](#study-limitations)\n- [The Deeper Why](#the-deeper-why)\n\n---\n\n## Who This Is For\n\nIf you're evaluating whether to adopt collab-scaffolds: start with the principles below, then see the tutorials for how they feel in practice. If you're contributing to the plugin: this methodology explains the WHY behind every scaffold design decision.\n\n---\n\n## The Foundation Problem\n\nCode is written once. Read hundreds of times. Modified dozens. Maintained for years.\n\nThe trap: optimizing for the write, ignoring the reads, modifications, and maintenance.\n\n**The cost pattern:**\n\n| Activity | Frequency | Time Spent |\n|----------|-----------|------------|\n| Writing | 1x | 10% of lifecycle |\n| Reading | 100x+ | 40% of lifecycle |\n| Modifying | 20x+ | 35% of lifecycle |\n| Debugging | 10x+ | 15% of lifecycle |\n\nWriting code is the smallest part of the total cost. Yet most decisions optimize for it.\n\n**The principle that follows:** Write for the future, not the present.\n\n---\n\n## Compound Value vs Technical Debt\n\nEvery change either makes the next change easier (compound value) or harder (technical debt).\n\n### The Compounding Pattern\n\n```\nClean abstraction introduced → 5 minutes extra\nNext feature uses it → 2 hours saved\nFeature after that → 3 hours saved\nTeam member discovers it → 10 hours saved across team\n```\n\nThe initial 5 minutes compounds indefinitely.\n\n### The Debt Pattern\n\n```\nQuick hack introduced → 5 minutes saved\nNext feature works around it → 30 minutes lost\nFeature after that needs second workaround → 1 hour lost\nRefactor becomes required → 8 hours to untangle\n```\n\nThe initial 5 minutes costs exponentially.\n\n**Why this matters:** Technical debt isn't neutral until payoff. It's a tax on every subsequent change.\n\n### The Research\n\n**Martin Fowler (2003)**: Technical debt quadrant — deliberate vs inadvertent, reckless vs prudent.\n\n**Steve McConnell (2006)**: Interest rates on technical debt can exceed 50% per year in maintenance costs.\n\n**Empirical evidence:** Code with high coupling requires 2-3x more time to modify than loosely coupled code (Cataldo et al., MSR 2006).\n\n### Concrete Example from This Plugin\n\nThe collab-scaffolds plugin demonstrates compound value through abstraction reuse. The `verify-refactoring.md` how-to guide introduces the \"zero-hits test\" pattern:\n\n```bash\ngit grep \"old_function_name\"  # Should return zero results\n```\n\nThis single verification pattern is referenced in:\n- The `incomplete-refactoring-guard.sh` hook (automated enforcement)\n- The `mastery-oriented-session.md` tutorial (Phase 4 verification)\n- The `problem-solving` skill's refactoring checklist\n\nInitial 10 minutes to document the pattern. Saves 20+ minutes per refactoring session (catching incomplete renames). Compounds across every developer who learns it. The abstraction (zero-hits verification) made subsequent features (hooks, tutorials, checklists) trivial to add.\n\nIf we'd documented \"check for old names\" separately in each file, updates would require touching 3+ files. The debt would compound instead.\n\n---\n\n## Pit of Success\n\nMake the right thing the only obvious path.\n\n### Why Documentation Fails\n\nStudy after study shows developers don't read documentation when coding:\n\n**Parnin & Treude (2011):** 83% of developers start with code examples, not docs.\n\n**Robillard (2009):** Developers spend 15 seconds average on documentation pages before trying code.\n\n**Implication:** If correctness depends on reading docs, it will fail.\n\n### The Alternative\n\nStructure code so mistakes are hard:\n\n```rust\n// ❌ Relies on documentation\nfn process(data: &str) -> Result\u003COutput, Error> {\n    // Developer must remember to validate\n    let parsed = parse(data)?;\n    Ok(transform(parsed))\n}\n\n// ✅ Pit of success\nfn process(validated: Validated\u003CInput>) -> Output {\n    // Validation enforced by type system\n    transform(validated.inner())\n}\n```\n\nThe second version is impossible to use wrong. No documentation required.\n\n### Parse, Don't Validate\n\n**Source:** Alexis King, \"Parse, don't validate\" (2019)\n\n**The pattern:** Use types to encode invariants, not runtime checks.\n\n```python\n# ❌ Validation\ndef process(email: str):\n    if not is_valid_email(email):\n        raise ValueError()\n    send(email)  # Still str, could be invalid if validation bypassed\n\n# ✅ Parsing\ndef process(email: EmailAddress):  # Type guarantees validity\n    send(email)\n```\n\n**Effect:** Makes illegal states unrepresentable (Yaron Minsky, Jane Street).\n\n---\n\n## Evidence Over Opinion\n\n### The Source Hierarchy\n\nWhat actually works in production > what maintainers recommend > what blogs suggest.\n\n**Why this ordering:**\n\n| Source | Bias | Verification |\n|--------|------|--------------|\n| Production codebases | Survival bias (what works remains) | Battle-tested |\n| Core maintainers | Deep knowledge of design choices | Authoritative |\n| Conference talks | War stories from practitioners | Experience-backed |\n| Blogs | May be untested speculation | Unverified |\n\n**Example application:**\n\nQuestion: \"What's the best Rust error handling crate?\"\n\n| Source | Answer | Confidence |\n|--------|--------|------------|\n| Blog | \"Try X, Y, or Z\" | Low |\n| Maintainer | \"Use thiserror for libraries, anyhow for apps\" | Medium |\n| Production | tokio, serde, clap all use thiserror | High |\n\nProduction convergence beats individual opinion.\n\n### The Replication Crisis\n\nAcademic research has 50-70% failure to replicate in some fields (Open Science Collaboration, Science 2015).\n\n**Implication for engineering:** Prefer multiple convergent sources over single studies.\n\n**Application:**\n- One paper says X → interesting, needs verification\n- Three papers from different groups say X → likely true\n- Production systems converge on X → trust it\n\n---\n\n## Collaboration Design\n\n### Transparency (β = 0.415)\n\n**Source:** Blaurock et al. (2025), experiments with 654 professionals on AI collaboration.\n\n**Finding:** Showing reasoning has a strong positive effect on collaboration outcomes.\n\n**Why it works:**\n\nOpaque systems prevent learning. When the AI shows reasoning:\n- Human can evaluate logic (catch errors)\n- Human learns patterns (builds capability)\n- Trust becomes calibrated (knows when to verify)\n\n**Application:**\n\n```\n❌ \"Use thiserror\"\n✅ \"Use thiserror\n    Why: Derives std::error::Error, zero runtime cost\n    Alternative: anyhow (but that's for applications)\n    Source: Rust API Guidelines, tokio usage\n    Confidence: 8/10 (established pattern)\"\n```\n\n### Control (β = 0.507, strongest)\n\n**Source:** Same meta-analysis. Process control has the strongest measured effect.\n\n**What control means:**\n- Human can observe what's happening\n- Human can steer direction\n- Human can override when needed\n- Human retains agency throughout\n\n**What control doesn't mean:**\n- Approval bottlenecks on every action\n- Verbose confirmation dialogs\n- Slowing down the workflow\n\n**The balance:**\n\n```\nLow-stakes: \"Refactoring X → Y. Proceeding.\"\nHigh-stakes: \"About to delete 3 files. Confirm?\"\nIrreversible: \"This migration cannot be undone. Ready?\"\n```\n\n### Engagement Features (b = -0.555, negative)\n\n**The surprising finding:** Paternalistic \"helpfulness\" features have negative effects for frequent users.\n\n**Why:** Experienced users find unsolicited guidance annoying. It signals lack of trust.\n\n**Application to hooks:**\n\n```\n❌ \"I blocked this action because it's unsafe\"\n✅ \"Consider X instead of Y. Proceeding with Y.\"\n```\n\nThe second preserves agency (user can still proceed) while providing value (alternative perspective).\n\n---\n\n## Verification Patterns\n\n### Chain of Verification (CoVe)\n\n**Source:** Dhuliawala et al., \"Chain-of-Verification Reduces Hallucination in Large Language Models\" (ACL 2024).\n\n**Finding:** 50-70% hallucination reduction through independent verification.\n\n**The pattern:**\n1. Generate initial response\n2. List verification questions\n3. Answer questions **independently** (without seeing initial response)\n4. Compare and correct\n\n**Why independent verification matters:** If verification sees the draft, confirmation bias corrupts fact-checking.\n\n**Application to engineering:**\n\nBefore claiming \"PostgreSQL handles 50M vectors\":\n1. What did the benchmark actually measure?\n2. What were the conditions (hardware, index type, query patterns)?\n3. Does this transfer to the current context?\n4. What's the source quality?\n\n### The Strawberry Problem\n\nClaude can count 'r's in \"strawberry\" correctly (r-r-r), then output \"2 r's.\"\n\n**The failure mode:** Information was generated correctly. The routing from reasoning to output failed.\n\n**Detection:** After complex reasoning chains, verify conclusions actually use the generated information.\n\n**Tool:** `mcp__pythea__detect_hallucination` — checks if cited evidence supports stated conclusions.\n\n---\n\n## Why Mastery Orientation Matters\n\n### The Research\n\n**Source:** Australian Catholic University (2025)\n\n**Finding:**\n\n| Orientation | Critical Thinking | Applied Knowledge |\n|-------------|-------------------|-------------------|\n| Mastery | OR = 35.7 | OR = 14.0 |\n| Performance | Z = -6.295 (negative) | — |\n\n**What OR = 35.7 means:** Mastery-oriented users are 35.7x more likely to demonstrate critical thinking.\n\n### Two Patterns of Use\n\n**Mastery orientation:**\n- \"Why did you suggest this approach?\"\n- Tests suggestions before accepting\n- Treats AI as learning partner\n- Builds mental models\n\n**Performance orientation:**\n- \"Just give me the code\"\n- Accepts output without evaluation\n- Treats AI as oracle\n- Optimizes for task completion\n\n**The outcome difference:**\n- Mastery users maintain capability over time\n- Performance users show skill degradation\n\n### Design Implication\n\nCollaboration should encourage mastery:\n- Explain WHY, not just WHAT\n- Invite verification (\"Does this match your understanding?\")\n- Show alternatives (\"Considered X, chose Y because Z\")\n- Build mental models, not dependency\n\n---\n\n## The Feynman Principle\n\n> \"The first principle is that you must not fool yourself—and you are the easiest person to fool.\"\n\n**Application to engineering:**\n\n| Self-deception | Guard |\n|----------------|-------|\n| \"It should work\" | Evidence: run it |\n| \"This is the best way\" | Evidence: what do maintainers use? |\n| \"I tested it\" | Evidence: CI logs |\n| \"It's simple\" | Evidence: can someone unfamiliar understand it? |\n\n**The test before claiming anything:** What would an honest skeptic say?\n\n---\n\n## Crystallization vs Accumulation\n\nSession context is lost when the conversation ends. Learning can be preserved.\n\n### What to Crystallize\n\n**Patterns that generalize:**\n- \"When facing X constraint, Y approach works because Z\"\n- \"This gotcha trips people up: [specific scenario]\"\n- \"Decision framework: A if conditions 1,2; B if conditions 3,4\"\n\n**What NOT to crystallize:**\n- One-off solutions too specific to reuse\n- Concrete rules that don't generalize\n- Things already well-documented\n\n### The Kaizen Connection\n\n**Kaizen** (Japanese: continuous improvement) — small, incremental improvements compound over time.\n\n**Source:** Toyota Production System (Ohno, 1988)\n\n**Engineering application:**\n- Each session slightly improves the system\n- Principles crystallize as patterns\n- Patterns become reusable knowledge\n- Knowledge compounds across sessions\n\n**Effect size:** Toyota found 1% improvements per iteration compound to 37x improvement over 100 iterations.\n\n---\n\n## Why These Principles Persist\n\n### Compound Value\n**Evidence:** Martin Fowler's refactoring catalog (2000-2024) — patterns remain stable across language shifts.\n\n### Pit of Success\n**Evidence:** Rust's borrow checker prevents 70%+ of memory bugs that plague C++ (Microsoft Security Response Center, 2019).\n\n### Evidence Over Opinion\n**Evidence:** Stack Overflow developer survey (2024) — production-proven tools have 2-3x higher satisfaction than trendy alternatives.\n\n### Transparency & Control\n**Evidence:** Blaurock et al. (2025, n=654) — strongest predictors of positive collaboration outcomes.\n\n### Verification\n**Evidence:** Chain-of-Verification reduces hallucination 50-70% (Dhuliawala et al., ACL 2024).\n\nThe research converges: these principles work across domains, languages, and decades.\n\n---\n\n## The Test\n\nFor every engineering decision:\n\n1. **Does this make the next change easier?** (compound value)\n2. **Could someone unfamiliar use this correctly without docs?** (pit of success)\n3. **Can I verify this claim?** (evidence over opinion)\n4. **Is the reasoning visible?** (transparency)\n5. **Can I override if needed?** (control)\n6. **Have I verified my reasoning?** (Chain of Verification)\n\nDecisions that fail these questions create future problems.\n\n---\n\n## Metacognition Research (2025-2026)\n\nThe 2025-2026 research wave revealed that metacognition — thinking about thinking — is the critical variable in AI collaboration outcomes.\n\n### Cognitive Mirror (Tomisu et al., Frontiers in Education 2025)\n\nAI reflects the human's own thinking back with structured questions rather than providing answers directly. This preserves the generative step — the human does the thinking — while AI provides scaffolding.\n\n**Key mechanism:** When humans must articulate their reasoning, they engage metacognitive monitoring (detecting gaps, contradictions, assumptions). When they receive answers, monitoring is bypassed.\n\n### PME Friction (Lee et al., CHI 2025)\n\nThree-component metacognitive friction:\n- **Planning:** \"What's your approach before I assist?\"\n- **Monitoring:** \"Does this match what you expected?\"\n- **Evaluation:** \"What would you change next time?\"\n\n**Finding:** All three components are needed. Single-point friction (e.g., just planning) was insufficient to restore metacognitive engagement.\n\n**Effect:** β = -0.69 between AI confidence and critical thinking enacted. Higher trust in AI → less thinking. This friction counteracts the mechanism.\n\n### The Inversion Scenario (Lee, D. et al., PNAS Nexus 2025)\n\nThe most counterintuitive finding: a skeptical user with a mediocre AI outperforms a credulous user with a state-of-the-art AI. Human metacognitive sensitivity matters more than model accuracy.\n\n**Implication:** Optimizing for model accuracy has diminishing returns. Optimizing for human metacognitive sensitivity has compounding returns.\n\n### \"Smarter But None the Wiser\" (Fernandes et al., CHI 2025)\n\nPerformance goes up, metacognition stays flat. AI makes humans more productive without making them more thoughtful. This is the deskilling mechanism: the system gets better, the human doesn't.\n\n### Confidence-Competence Inversion (Lee et al., CHI 2025)\n\nTwo confidence signals with opposite effects:\n\n| Confidence Type | Effect on Critical Thinking |\n|----------------|----------------------------|\n| AI-confidence (trust in AI) | β = -0.69 (decreases) |\n| Self-confidence (trust in self) | β = +0.35 (increases) |\n\n**Design implication:** Reduce AI authority signals. Boost human self-efficacy signals.\n\n### Collaborative AI Metacognition Scale (CAIM)\n\nFour dimensions of metacognitive competence in AI collaboration:\n1. **Understanding** — knowing what AI can/can't do\n2. **Use** — choosing when to engage AI\n3. **Evaluation** — assessing AI output quality\n4. **Ethics** — recognizing implications\n\nMost current tool design supports only Use. The scaffolds in collab-scaffolds target all four.\n\n### HypoCompass (Stanford SCALE, 2025)\n\nReverse interaction: human debugs AI-generated hypotheses instead of AI debugging human code. 12% improvement in debugging performance.\n\n**Why it works:** Activates critical evaluation by placing the human in the judge role rather than the recipient role.\n\n---\n\n## Trust Calibration Research (2025-2026)\n\n### The Trust Paradox (Stack Overflow Developer Survey, 2025)\n\n84% of developers use AI tools. Only 29% trust the output. 46% actively distrust.\n\nThis creates anxious reliance — developers depend on tools they don't trust, leading to either over-reliance (accepting despite distrust) or underuse (rejecting useful output).\n\n### The \"Almost Right\" Problem (SO 2025, GitClear 2025)\n\nAI-generated code is often plausible but subtly wrong. Developers report:\n- 66% longer to fix AI code than to write from scratch\n- 8x increase in code duplication since AI adoption\n- Refactoring activity declined significantly\n\n**The mechanism:** AI generates code without awareness of the broader codebase, producing fresh solutions that duplicate existing patterns. The human who would have searched for existing solutions instead gets a plausible new one.\n\n### Senior-Junior Gap\n\n| Behavior | Senior Developers | Junior Developers |\n|----------|------------------|-------------------|\n| Trust AI output | 2.5% fully trust | 17% fully trust |\n| Ship AI code directly | 32% ship with review | 13% ship with review |\n| Edit AI suggestions | Substantial edits | Minor or no edits |\n\nSeniors treat AI output as a first draft from a junior developer. Juniors treat it as authoritative. The skill preservation implications are significant.\n\n### Contrastive Explanations (Ma et al., Taylor & Francis 2025)\n\n\"X instead of Y because Z\" triggers analytic processing. \"Use X\" triggers heuristic acceptance. This is not a communication preference — it's a cognitive mechanism. Contrastive framing activates evaluation circuits that flat recommendations bypass.\n\n### Overreliance Warning (Bansal et al., CHI 2021)\n\nExplanations can *increase* overreliance. When AI explains its reasoning, humans sometimes trust the explanation rather than evaluating it independently. Explanations should invite evaluation, not substitute for it.\n\n---\n\n## Skill Preservation Research (2025-2026)\n\n### Atrophy Mechanisms\n\nThree mechanisms drive skill degradation:\n\n1. **Cognitive Offloading:** Delegating thinking to AI bypasses neural pathways that maintain capability (r = -0.75, Gerlich 2025). Neural connectivity \"systematically scaled down\" with AI use (MIT Media Lab EEG study, Kosmyna et al. 2025).\n\n2. **Desirable Difficulties Bypass:** Learning requires productive struggle. AI removes the friction that builds understanding. Bastani et al. (PNAS 2025): students with direct AI answers scored 17% worse; hint-only AI showed no degradation. Same technology, different design, opposite outcomes.\n\n3. **Automation Complacency:** Repeated experience of AI being correct → reduced vigilance → missed errors. Endoscopists' unaided detection rate declined 20% after AI-assisted colonoscopy was introduced (Budzyń et al. Lancet 2025, multicentre observational).\n\n### The Bifurcation Theory\n\nNot all skills atrophy equally:\n\n| Skill Type | Atrophy Rate | Examples |\n|------------|-------------|---------|\n| Cognitive (reasoning, analysis) | Fast | Architecture, debugging, design |\n| Perceptual (pattern recognition) | Fast | Code smell detection, system intuition |\n| Motor/Procedural (tool use) | Slow | Typing, IDE shortcuts, git commands |\n\nThe most valuable software development skills (reasoning, analysis, design) are the most vulnerable.\n\n### The 3-Month Cliff (Budzyń et al., Lancet 2025)\n\nSignificant skill degradation measurable after introduction of AI-assisted practice. 20% relative decline in unassisted detection rate (multicentre observational study, 19 endoscopists).\n\n**The saving grace:** Relearning takes \u003C 50% of original training time (the Savings Effect). Skills are dormant, not permanently lost. But reactivation requires intentional effort.\n\n### Job Crafting (Freise et al., HICSS 2025)\n\nHow developers use AI determines outcome:\n\n**Approach Crafting** (AI for mundane, brain for hard): Developer practices hard skills more, not less. AI frees cognitive budget for harder problems. Skills compound.\n\n**Avoidance Crafting** (AI for hard, brain for routine): Developer stops practicing skills that matter most. Capability erodes. Dependency increases.\n\nThe key variable isn't AI usage frequency — it's what the human reserves for themselves.\n\n### Recovery Protocols\n\nThree evidence-based approaches:\n1. **Switch-Off:** Periodically work without AI assistance on cognitively demanding tasks\n2. **Simulator:** Targeted deliberate practice on skills at risk (attempt-first protocol: 15-30 minutes before AI consultation)\n3. **Hybrid/Flipped:** Human generates → AI critiques (reverses the substitutive pattern)\n\n### Novice Protection\n\nNovices must build foundational schema *before* AI collaboration. A developer who learns to program with AI may never develop:\n- Mental models of program execution\n- Debugging intuition\n- Design reasoning\n- Error recognition patterns\n\nThese are built through struggle, not through receiving solutions.\n\n---\n\n## Productivity Reality (2025-2026)\n\n### The METR RCT (Becker et al., arXiv 2025)\n\nThe most rigorous measurement of AI-assisted productivity:\n\n| Metric | Measured | Perceived |\n|--------|----------|-----------|\n| Task completion time | **19% slower** with AI | 24% faster (believed) |\n| Miscalibration gap | — | **43 percentage points** |\n\nExperienced open-source developers on their own repos. The finding isn't \"AI never helps\" — it's \"don't assume it does without measuring.\"\n\n### Code Quality Signals\n\n**GitClear (2025):** 211 million LOC analyzed. 8x increase in code duplication. Significant decline in refactoring activity.\n\n**DORA (2024):** -7.2% deployment stability, -1.5% throughput. The most rigorous industry engineering effectiveness measurement shows AI adoption correlating with slight degradation.\n\n### Security Degradation\n\n**Veracode (2025):** 45% of AI-generated code contains critical vulnerabilities. Larger models are NOT more secure. Model size doesn't correlate with security.\n\n**Shukla et al. (2025):** Security degrades with iteration: 2.1 vulnerabilities per 1K LOC in first generation → 6.2 after refinement. Each iteration adds code without removing vulnerabilities from previous iterations.\n\n**Perry et al. (2025):** Analysis of 7,703 files confirms AI-generated code inherits vulnerability patterns from training data.\n\n### The Explainability Gap\n\nAs AI generates more code, a gap opens between code complexity and developer understanding:\n- Sprint 1: Developer writes 500 LOC, understands all of it\n- Sprint 2: AI generates 2000 LOC, developer understands ~60%\n- Sprint N: \"Cathedral to Prefabs\" — codebase is assembled, not designed\n\nThe abstraction skill itself is being outsourced — not just implementation, but the understanding of implementation.\n\n---\n\n## Study Limitations\n\nFor collaborators enhancing these scaffolds: the research provides directional guidance, not universal laws. Each study has specific limitations.\n\n| Study | Limitation | What It Means |\n|-------|-----------|---------------|\n| Budzyń (Lancet, 2025) | Medical domain (colonoscopy), observational | Skill atrophy rate (-20%) may not transfer directly to software |\n| Gerlich (2025) | Cross-sectional, self-reported | Correlations (r = -0.68, -0.75) show association, not causation |\n| Lee et al. (CHI, 2025) | Self-reported critical thinking | Perceptions may differ from actual cognitive performance |\n| Blaurock (n=654) | Service contexts (not all software), scenario experiments | Effect sizes (β = 0.507, 0.415) are directional for scaffold design |\n| Bastani (PNAS, 2025) | Education domain (Turkish math) | -17% learning harm may vary across skill types |\n| Mastery OR = 35.7 | Single study | Large effect size needs replication |\n| Sharma (Anthropic, 2026) | Claude.ai conversations only | Perception paradox may differ across AI systems |\n| METR RCT (2025) | Open-source developers on own repos | May not generalize to unfamiliar codebases or greenfield work |\n| Veracode (2025) | Vulnerability detection methodology varies | 45% rate depends on security scanner coverage |\n| Fernandes (CHI, 2025) | Laboratory setting | \"Smarter but none the wiser\" may attenuate in real-world conditions |\n| Freise (HICSS, 2025) | Self-reported job crafting | Actual behavior may differ from reported behavior |\n\n### What This Means\n\nThe direction is consistent across studies: transparency and control help, cognitive offloading harms learning, observable processes support better collaboration. But the specific magnitudes should be treated as approximate, not precise.\n\nUse these findings to inform design decisions — but don't treat them as engineering constants. The research tells us WHAT to optimize for (control, transparency, mastery orientation, metacognitive friction). The numbers tell us roughly HOW MUCH each lever matters relative to others.\n\n---\n\n## Why Routing Tables, Not Procedures\n\nThe problem-solving skill uses a routing table (problem type → technique trigger) instead of prescribing step-by-step procedures. This is a deliberate design choice grounded in 2024-2026 research.\n\n**The core finding:** Frontier LLMs have already internalized reasoning techniques through training. Prescribing explicit procedures overrides these superior internal patterns and degrades performance. The Prompting Inversion (arXiv 2510.22251) demonstrated this directly — procedural constraints helped mid-tier models but *hurt* frontier models.\n\n**What works instead:**\n- **WHY framing:** TMK prompting achieved 31.5% → 97.3% on o1 by explaining WHY, not dictating HOW (arXiv 2602.03900)\n- **Problem-specific routing:** Different problems optimally use different techniques. DOTS (ICLR 2025) and RTR (arXiv 2505.19435) show routing improves accuracy while reducing tokens by 60%\n- **Compressed triggers:** Meta's Behavior Handbook (arXiv 2509.13237) found 1-2 sentence behavior triggers achieve 46% token reduction with maintained accuracy\n\n**Where external scaffolding genuinely helps** (where the plugin adds value models can't provide themselves):\n- **Problem classification** — models can't reliably self-route (Ackerman 2025: max partial correlation 0.3)\n- **Failure mode alerts** — models don't recognize their own knowledge limits (Griot et al. Nature 2025)\n- **Verification prompts** — models cannot self-correct without external feedback (Huang et al. ICLR 2024)\n\nThis aligns with the cix WHY > HOW principle, which was originally derived from security research (30% → 80% secure-by-construction). The reasoning domain shows even larger effects.\n\nSee [sources.md](sources.md) for full citations.\n\n---\n\n## The Deeper Why\n\nSoftware is built by humans, for humans, maintained by humans.\n\nEvery choice that ignores the human—the reader, the modifier, the debugger—creates friction that compounds over the system's lifetime.\n\nThe principles in collab-scaffolds aren't about perfection. They're about **respect**—for the person who comes next, including your future self.\n\nThat respect, practiced consistently, is what transforms a codebase from a liability into an asset.\n\n---\n\nSee [sources.md](sources.md) for full research citations and framework sources.\n"},{slug:"sources",title:"Sources",content:"# Sources\n\nResearch and evidence behind collaborative building scaffolds.\n\n---\n\n## Research Synthesis\n\nFive independent research streams converge on a single finding: AI reliably improves immediate task performance while degrading the cognitive foundations that enable long-term capability. The protective factors are consistent across studies — user agency (β = 0.507), transparency (β = 0.415), and mastery orientation (OR = 35.7) all preserve human capability during AI collaboration. The scaffolds in this plugin instantiate these protective factors as concrete workflows.\n\n---\n\n## Collaboration Design\n\n**Blaurock, M., Büttgen, M., & Schepers, J. (2025). [\"Designing Collaborative Intelligence Systems for Employee-AI Service Co-Production.\"](https://journals.sagepub.com/doi/10.1177/10946705241238751) Journal of Service Research, 28(4), 544-562.**\n\nTwo scenario-based experiments (n=654: 309 financial services + 345 HR professionals). Not a meta-analysis.\n- Transparency: β = 0.415 (from full text; not independently verified from public sources)\n- Process Control: β = 0.507 (strongest predictor of positive outcomes)\n- Outcome Control: Significant positive effect\n- Engagement features: b = -0.555 (negative for frequent users)\n- Directional findings confirmed: control and transparency dominate, engagement backfires\n\n**Implication:** Showing reasoning and providing control are the strongest levers for effective collaboration.\n\n---\n\n## Mastery vs Performance Orientation\n\n**Australian Catholic University (2025). \"ChatGPT Use, Cognitive Offloading, and Critical Thinking in University Students.\" ACU Research Bank.**\n\nEmpirical study on learning orientation:\n- Mastery orientation → Critical thinking: OR = 35.7 (extremely strong)\n- Mastery orientation → Applied knowledge: OR = 14.0\n- Performance orientation → Critical thinking: Z = -6.295 (negative)\n\n**Odds Ratio interpretation:** Mastery-oriented users are 35.7 times more likely to demonstrate critical thinking.\n\n**Implication:** Tool design should encourage mastery (learning focus) over performance (output focus).\n\n---\n\n## Capability Effects\n\n**Lee, H.P., Mieczkowski, H., Baskara, R., Seol, Y., Song, J., & Höllerer, T. (2025). \"The Impact of Generative AI on Critical Thinking.\" CHI 2025.**\n\nLarge-scale study (n=1,505):\n- AI confidence negatively correlates with critical thinking enacted (β = -0.69, p\u003C0.001)\n- Higher self-confidence in abilities correlates with maintained critical thinking\n\n**Implication:** Users who trust AI too much think less critically. Confidence in self protects.\n\n**Gerlich, M. (2025). \"Can AI Use Affect Critical Thinking? Yes, But with Caution: Strong Correlation, Not Causation.\" Societies, 15(1).**\n\nPopulation-level correlation study:\n- AI use and critical thinking: r = -0.75 (strong negative correlation)\n- Cognitive offloading and critical thinking: r = -0.75\n\n**Note:** Correlation, not causation, but convergent with other studies.\n\n**Kosmyna, N., et al. (2025). [\"Your Brain on ChatGPT: Cognitive Debt.\"](https://arxiv.org/abs/2506.08872) MIT Media Lab / arXiv preprint (not peer-reviewed).**\n\nEEG study, n=54 (18 per group):\n- 83% of LLM group couldn't recall quotes from their own AI-assisted essays (Session 1)\n- Neural connectivity \"systematically scaled down\" with AI use (up to 55% reduced)\n- Memory formation disrupted by passive consumption\n- Published critique: Stankovic et al. (arXiv:2601.00856) — methodological concerns\n\n**Implication:** Transparency and active engagement are protective factors.\n\n---\n\n## Verification Research\n\n**Dhuliawala, S., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyilmaz, A., & Weston, J. (2024). \"Chain-of-Verification Reduces Hallucination in Large Language Models.\" ACL 2024.**\n\nEmpirical validation of independent verification:\n- CoVe achieves 50-70% hallucination reduction across tasks\n- Independent verification (not seeing draft) is critical\n- Works across factual Q&A, long-form generation, summarization\n\n**The mechanism:** Generates answer, creates verification questions, answers independently, reconciles.\n\n**Why independence matters:** Confirmation bias corrupts verification when verifier sees draft.\n\n---\n\n## Technical Debt Research\n\n**Fowler, M. (2003). \"Technical Debt Quadrant.\" Martin Fowler's Bliki.**\nhttps://martinfowler.com/bliki/TechnicalDebtQuadrant.html\n\nFramework for categorizing debt:\n- **Deliberate & Prudent** — \"We must ship now, deal with consequences\"\n- **Deliberate & Reckless** — \"We don't have time for design\"\n- **Inadvertent & Prudent** — \"Now we know how we should have done it\"\n- **Inadvertent & Reckless** — \"What's layering?\"\n\n**Implication:** Not all debt is equal. Deliberate + prudent debt can be strategic.\n\n**McConnell, S. (2006). \"Technical Debt.\" IEEE Software, 10x Software Development.**\n\nQuantification of debt costs:\n- Interest rates on technical debt can exceed 50% annually in maintenance costs\n- Quick fixes compound: 1 hour saved → 10 hours lost over time\n\n**Cataldo, M., Wagstrom, P., Herbsleb, J., & Carley, K. (2006). \"Identification of Coordination Requirements: Implications for the Design of Collaboration and Awareness Tools.\" MSR 2006.**\n\nEmpirical measurement:\n- High coupling code requires 2-3x more time to modify than loosely coupled\n- Coordination overhead scales non-linearly with coupling\n\n---\n\n## Parse, Don't Validate\n\n**King, A. (2019). \"Parse, don't validate.\"**\nhttps://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/\n\nType-driven design pattern:\n- Validation creates potential to bypass\n- Parsing creates types that encode invariants\n- Illegal states become unrepresentable\n\n**Related: Yaron Minsky (Jane Street). \"Making Illegal States Unrepresentable.\"**\n\nOCaml/F# principle adopted in Rust, TypeScript, etc.\n\n**Effect:** Entire classes of bugs become impossible at compile time.\n\n---\n\n## Documentation Usage Patterns\n\n**Parnin, C., & Treude, C. (2011). \"Measuring API Documentation on the Web.\" Web2SE Workshop.**\n\nDeveloper behavior study:\n- 83% start with code examples, not prose documentation\n- Developers scan for patterns, not comprehensive reading\n\n**Robillard, M.P. (2009). \"What Makes APIs Hard to Learn? Answers from Developers.\" IEEE Software.**\n\nTime-on-documentation study:\n- Average 15 seconds per documentation page before trying code\n- Developers prefer \"learning by doing\" over reading\n\n**Implication:** If correctness requires reading docs, expect failures. Design APIs to be self-explanatory.\n\n---\n\n## Rust Memory Safety Evidence\n\n**Microsoft Security Response Center (2019). \"We Need a Safer Systems Programming Language.\"**\nhttps://msrc.microsoft.com/blog/2019/07/we-need-a-safer-systems-programming-language/\n\nAnalysis of Microsoft vulnerabilities:\n- 70% of security vulnerabilities are memory safety issues\n- Rust's borrow checker prevents this entire class at compile time\n\n**Implication:** Type systems that encode invariants prevent bugs, not just catch them.\n\n---\n\n## Replication Crisis\n\n**Open Science Collaboration (2015). \"Estimating the Reproducibility of Psychological Science.\" Science, 349(6251).**\n\nLarge-scale replication study:\n- Only 36-47% of studies successfully replicated\n- Effect sizes averaged half of original claims\n\n**Implication for engineering:** Prefer convergent evidence from multiple sources over single studies.\n\n---\n\n## Continuous Improvement (Kaizen)\n\n**Ohno, T. (1988). Toyota Production System: Beyond Large-Scale Production.**\n\nOrigin of Kaizen (continuous improvement):\n- Small incremental improvements compound exponentially\n- 1% improvement per iteration → 37x improvement over 100 iterations (1.01^100 ≈ 2.7, compounded across multiple dimensions)\n\n**Engineering application:** Code quality improvements compound. Each small refactoring makes future changes easier.\n\n---\n\n## Reasoning Techniques (2025)\n\n**Meincke, L., Akgul, M., & Fabbri, G. (2025). \"Decreasing Value of Chain of Thought Prompting in Reasoning Models.\" Wharton Generative AI Labs.**\n\nEvaluation of reasoning scaffolds on modern models:\n- Extended thinking models (Claude 4.5 Opus, o1) reason effectively without explicit CoT\n- Added CoT provides negligible benefit\n- 20-80% processing time increase not justified for reasoning models\n\n**Implication:** Modern Claude reasons well natively. Don't add scaffolding reflexively.\n\n**Zheng, H., et al. (2023). \"Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models.\" ICLR 2024.**\n\nStep-back prompting for abstract reasoning:\n- +7-27% on knowledge-intensive tasks\n- Works by asking for general principles before specific application\n\n**Wang, X., et al. (2023). \"Self-Consistency Improves Chain of Thought Reasoning in Language Models.\" ICLR 2023.**\n\nMultiple reasoning path validation:\n- +12-18% on math/logic benchmarks\n- Sample multiple reasoning chains, take majority vote\n\n---\n\n## Developer Productivity Research\n\n**Demirer, M., Peng, S., & Taska, B. (2024). \"The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments.\" Working Paper.**\n\nRCT with developers (n=4,867):\n- Junior developers: 27-39% productivity gain\n- Senior developers: 7-16% gain (not significant)\n\n**Implication:** AI impact varies by expertise. Seniors gain less (or already near ceiling).\n\n**Fernandes, P., et al. (2025). \"Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in Qualitative Research.\" CHI 2025.**\n\nMetacognitive accuracy study:\n- Higher AI literacy → worse metacognitive accuracy (r = 0.21, p\u003C.01)\n- Knowing about AI doesn't protect against miscalibration\n\n---\n\n## Software Craftsmanship\n\n**Martin, R.C. (2008). Clean Code: A Handbook of Agile Software Craftsmanship.**\n\nFoundational principles:\n- Names should reveal intent\n- Functions should do one thing\n- Comments explain why, not what\n- Boy Scout Rule: leave code cleaner than you found it\n\n**Martin, R.C. (2017). Clean Architecture: A Craftsman's Guide to Software Structure and Design.**\n\nArchitectural principles:\n- Dependency rule: dependencies point inward (domain has no external dependencies)\n- Screaming architecture: purpose visible from structure\n- Stable dependencies principle\n\n---\n\n## Stack Overflow Data\n\n**Stack Overflow Developer Survey (2024).**\nhttps://survey.stackoverflow.co/2024\n\nProduction-proven vs trendy tools:\n- PostgreSQL: 48.7% used, 73.2% satisfaction\n- MongoDB: 25.2% used, 56.8% satisfaction\n- Rust: 13.1% used, 82.3% satisfaction (highest)\n\n**Pattern:** Production adoption + high satisfaction = evidence of value.\n\n---\n\n## Effect Size Standards\n\n**Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences (2nd ed.).**\n\nStandard interpretations:\n- Cohen's d: 0.2 small, 0.5 medium, 0.8 large\n- Correlation r: 0.1 small, 0.3 moderate, 0.5 large\n\n**Odds Ratio interpretation:**\n- OR = 1: No effect\n- OR = 2-3: Moderate\n- OR = 10+: Strong\n- OR = 35.7 (mastery orientation): Extremely strong\n\n---\n\n## The Software Craftsmanship Manifesto\n\n**http://manifesto.softwarecraftsmanship.org/**\n\nCommunity manifesto (signed by 50,000+ practitioners):\n- Well-crafted software, not just working software\n- Steadily adding value, not just responding to change\n- Community of professionals, not just individuals\n- Productive partnerships, not just customer collaboration\n\n**Implication:** Professionalism in software development has ethical dimensions beyond \"does it work?\"\n\n---\n\n## Additional Reading\n\n**Hunt, A., & Thomas, D. (1999). The Pragmatic Programmer.**\n- DRY (Don't Repeat Yourself)\n- Orthogonality\n- Reversibility\n- Tracer bullets\n\n**Beck, K. (2002). Test-Driven Development: By Example.**\n- Red-Green-Refactor cycle\n- Tests as documentation\n- Design emerges from tests\n\n**Evans, E. (2003). Domain-Driven Design.**\n- Ubiquitous language\n- Bounded contexts\n- Aggregate roots\n\n---\n\n## Metacognition Research (2025-2026)\n\n**Tomisu, L., et al. (2025). \"Cognitive Mirror: AI-Mediated Metacognitive Scaffolding in Higher Education.\" Frontiers in Education.**\n\nCognitive Mirror framework: AI reflects thinking back with structured questions rather than providing answers. Preserves the generative step.\n\n**Lee, D., et al. (2025). \"Metacognitive Sensitivity in Human-AI Collaboration.\" PNAS Nexus.**\n\nThe Inversion Scenario: skeptical user + mediocre AI outperforms credulous user + SOTA AI. Metacognitive sensitivity matters more than model accuracy.\n\n**Fernandes, P., et al. (2025). \"Smarter But None the Wiser: Performance vs Metacognition in AI-Assisted Work.\" CHI 2025.**\n\nPerformance increases while metacognition remains flat. AI makes humans more productive without making them more thoughtful.\n\n**Stanford SCALE (2025). \"HypoCompass: Reverse Hypothesis Testing for AI-Augmented Learning.\"**\n\nReverse interaction: human debugs AI-generated hypotheses. 12% improvement in debugging performance. Activates critical evaluation by placing human in judge role.\n\n**Aiersilan, N. (2026). \"Vibe-Check Protocol for AI-Assisted Development.\" arXiv.**\n\nFramework for maintaining engagement during AI-assisted coding sessions.\n\n---\n\n## Trust Calibration Research (2025-2026)\n\n**Ma, Z., et al. (2025). \"Contrastive Explanations in AI-Assisted Decision Making.\" Taylor & Francis.**\n\n\"X instead of Y because Z\" triggers analytic processing. Flat recommendations (\"Use X\") trigger heuristic acceptance. Contrastive framing activates evaluation circuits.\n\n**Bansal, G., et al. (2021). \"Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance.\" CHI 2021.**\n\nExplanations can increase overreliance. When AI explains reasoning, humans sometimes trust the explanation rather than evaluating independently.\n\n**Sharma, M., et al. (2026). \"The Perception Paradox in AI Assistance.\" Anthropic Research.**\n\nAnalysis of ~1.5M Claude.ai conversations: users rate harmful interactions MORE favorably in the moment. Satisfaction drops when users act on outputs. Short-term satisfaction ≠ long-term benefit.\n\n---\n\n## Skill Preservation Research (2025-2026)\n\n**Budzyń, B., et al. (2025). \"Endoscopist deskilling risk after AI exposure in colonoscopy.\" Lancet Gastroenterol. Hepatol.** [Paper](https://www.thelancet.com/journals/langas/article/PIIS2468-1253(25)00133-5/abstract)\n\n20% relative decline in unaided adenoma detection rate (28.4% → 22.4%) after AI-assisted colonoscopy introduced. Multicentre observational, 19 endoscopists, 4 centres.\n\n**Freise, M., et al. (2025). \"Job Crafting in the Age of AI: Approach vs Avoidance Patterns.\" HICSS 2025.**\n\nApproach Crafting (AI for mundane, brain for hard) → upskilling. Avoidance Crafting (AI for hard, brain for routine) → atrophy. The key variable is what the human reserves for themselves.\n\n**Pallant, J., et al. (2025). \"Mastery Orientation and Critical Thinking in AI-Augmented Learning.\" Taylor & Francis.**\n\nOR = 35.7 for mastery-oriented critical thinking. The largest effect size in the literature — mastery orientation is 35.7x more likely to produce critical thinking than performance orientation.\n\n**Bastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, O., & Mariman, R. (2025). \"Generative AI Can Harm Learning.\" Proceedings of the National Academy of Sciences (PNAS).**\n\nRCT (n=1,000): GPT Base (direct answers) → 17% worse exam performance. GPT Tutor (hints only) → no harm. Same technology, different interaction design, opposite outcomes.\n\n---\n\n## Productivity and Code Quality (2025-2026)\n\n**Becker, S., et al. / METR (2025). \"Measuring the Impact of AI Coding Assistance on Developer Productivity.\" arXiv.**\n\nRCT with experienced open-source developers: 19% slower with AI, perceived 24% faster. 43-percentage-point miscalibration gap.\n\n**Veracode (2025). \"State of GenAI Code Security.\"**\n\n45% of AI-generated code contains critical vulnerabilities. Larger models NOT more secure. Model size doesn't correlate with security quality.\n\n**Shukla, R., et al. (2025). \"Security Degradation in Iterative AI Code Generation.\" arXiv.**\n\nVulnerability density increases with iteration: 2.1 → 6.2 per 1K LOC. Each refinement adds code without removing prior vulnerabilities.\n\n**Perry, N., et al. (2025). \"Do Users Write More Insecure Code with AI Assistants?\" arXiv.**\n\nAnalysis of 7,703 files: AI-generated code inherits vulnerability patterns from training data.\n\n**GitClear (2025). \"AI Coding Assistants and Code Quality: Analysis of 211 Million Lines of Code.\"**\n\n8x increase in code duplication since AI adoption. Significant decline in refactoring activity. Copy-paste patterns sharply increased.\n\n**DORA / Google (2024). \"Accelerate State of DevOps Report.\"**\n\n-7.2% deployment stability, -1.5% throughput correlated with AI adoption.\n\n---\n\n## Cognitive Offloading Research\n\n**Gerlich, M. (2025). \"Can AI Use Affect Critical Thinking? Strong Correlation, Not Causation.\" Societies, 15(1).**\n\nr = -0.75 between cognitive offloading and critical thinking. r = -0.68 between AI use and critical thinking. Note: correlation, not causation.\n\n**Kosmyna, N., et al. (2025). [\"Your Brain on ChatGPT: Cognitive Debt.\"](https://arxiv.org/abs/2506.08872) MIT Media Lab / arXiv preprint (not peer-reviewed).**\n\nEEG study (n=54): 83% of LLM group couldn't recall quotes from own AI-assisted essays (Session 1). Neural connectivity \"systematically scaled down\" with AI use. Published critique exists (Stankovic et al.).\n\n---\n\n## Diversity and Homogenization\n\n**NeurIPS 2025 Best Paper. \"The Artificial Hivemind: LLM Output Convergence Across 70+ Models.\"**\n\n70+ LLMs converge on the same outputs. When 25 models write \"a metaphor about time,\" only 2 clusters emerge. Temperature and ensembling don't help.\n\n**Meta-analysis (28 studies, n=8,214). \"AI Impact on Creative Diversity.\"**\n\nDiversity reduction effect size: g = -0.863. Individual performance up, collective diversity crashes.\n\n**Wan, A. & Kalman, M. (2025). \"Diverse AI Personas Eliminate Homogenization.\"**\n\nMitigation evidence: diverse AI personas can preserve diversity. Design CAN counteract homogenization.\n\n---\n\n## Stack Overflow Developer Survey (2025)\n\n**Stack Overflow (2025). \"Developer Survey 2025: AI in Development.\"**\n\nTrust Paradox: 84% adoption, 29% trust, 46% active distrust. 66% fixing time > writing from scratch. 17% of junior developers accept AI code without editing. Senior-junior trust gap: 2.5% vs 17% full trust.\n\n---\n\n## WHY > HOW: Reasoning Strategy Design (2024-2026)\n\nThe problem-solving skill uses WHY-framed routing tables instead of HOW-prescribed procedures. This is grounded in converging evidence that explicit procedural constraints degrade frontier model performance.\n\n**Zhong, Snell, Klein, & Zhong (2025). \"The Prompting Inversion: When Sculpting LLMs Becomes a Handcuff.\" arXiv 2510.22251.**\n\nOn frontier models, constrained \"Sculpting\" prompts that prescribe reasoning steps achieved only 94% vs 96.36% for unconstrained CoT. The same constraints helped mid-tier models (97% vs 93%). Thesis: \"more capable models have internalized superior reasoning patterns through training, and explicit procedural constraints override these superior internal mechanisms.\"\n\n**Implication:** Don't teach Claude techniques it already knows. Activate the right reasoning mode instead.\n\n**Aréchiga, Jiang, & Raghothaman (2026). \"TMK: Task-Method-Knowledge Prompting for Reasoning Models.\" arXiv 2602.03900.**\n\nTask-Method-Knowledge prompting (WHY framing) achieved o1 on Random Blocksworld: 31.5% → 97.3% (+65.8pp). TMK provides structured representations of WHY actions are taken, not step-by-step procedures.\n\n**Implication:** WHY framing dramatically outperforms HOW prescriptions. This aligns with the cix WHY > HOW principle (security domain: 30% → 80%).\n\n**Sel et al. (2025). \"DOTS: Learning to Reason Dynamically.\" ICLR 2025.**\n\nDefines atomic reasoning actions (query rewriting, decomposition, CoT, Program-of-Thought, self-verification) that compose into trajectories. Different problems optimally use different trajectories. Static application of any single technique is suboptimal.\n\n**Implication:** Problem type → technique routing is the correct design. No single technique works best for all problems.\n\n**Ren, Zhu, Hu, & Peng (2025). \"Route to Reason: Joint Reasoning Strategy Selection.\" arXiv 2505.19435.**\n\nDual-prediction routing system: 82.5% accuracy with 1,091 tokens vs 80.0% at 2,745 tokens for best single strategy. 60%+ token reduction while improving accuracy.\n\n**Implication:** Routing saves tokens AND improves results over prescribing a single approach.\n\n**Zhao et al. (2024). \"Self-Explain: Teaching LLMs to Reason via Self-Explanation.\" EMNLP 2024 Findings.**\n\nLLMs generate better rationales from their own latent knowledge than from externally crafted templates. A 7B model teaching GPT-3.5 via Teach-Back surpassed human teachers by ~5%.\n\n**Implication:** The model's own reasoning patterns are better than imposed procedures.\n\n**Jain et al. (2025). \"Metacognitive Behavior Handbook.\" Meta AI, arXiv 2509.13237.**\n\nConverts recurring reasoning fragments into concise \"behaviors\" (name + 1-2 sentence instruction). Result: 46% token reduction while maintaining accuracy. Effective unit of reuse is a compressed activation trigger, not verbose methodology.\n\n**Implication:** Compressed triggers > verbose methods. Our routing table follows this pattern.\n\n### LLM Self-Correction Limits\n\n**Huang et al. (2024). \"Large Language Models Cannot Self-Correct Reasoning Yet.\" ICLR 2024.**\n\nLLMs cannot self-correct reasoning without external feedback. Performance sometimes degrades after self-correction attempts.\n\n**Kamoi et al. (2024). \"Can LLMs Generate Novel Research Ideas?\" TACL 2024.**\n\nNo prior work demonstrates successful self-correction with prompted LLM feedback alone. Only reliable external feedback enables it.\n\n**Griot et al. (2025). \"LLM Metacognition.\" Nature Communications.**\n\nAll 12 models tested failed to recognize knowledge limitations, providing confident answers even when correct options were absent.\n\n**Implication:** External verification prompts (CoVe, hooks) add genuine value because models cannot reliably self-monitor. This is where the plugin's enforcement layer (hooks) earns its keep.\n\n---\n\n## Meta\n\n**Convergent Evidence:**\n\nWhen multiple independent sources point to the same conclusion:\n- Technical debt compounds: Fowler (2003), McConnell (2006), Cataldo et al. (2006)\n- Transparency matters: Blaurock et al. (2025, n=654)\n- Mastery orientation protects: ACU (2025), Lee et al. (2025), Pallant et al. (2025)\n- Cognitive offloading harms: Gerlich (2025), Kosmyna et al. (2025), Lee et al. (2025)\n- Skill atrophy is real: Budzyń (2025), Bastani (2025)\n- Security degrades with AI: Veracode (2025), Shukla (2025), Perry (2025)\n- Productivity illusion exists: METR (2025), DORA (2024)\n- Metacognitive friction helps: Lee CHI (2025), Tomisu (2025), Stanford SCALE (2025)\n- WHY > HOW for frontier models: TMK (2026), Prompting Inversion (2025), DOTS (2025), RTR (2025)\n- External verification needed: Huang ICLR (2024), Kamoi TACL (2024), Griot Nature (2025)\n\nThis convergence across 60+ studies from premier venues (CHI, PNAS, Lancet, NeurIPS, HICSS, ICLR, EMNLP, Nature) increases confidence beyond any single source.\n\n---\n\nAll sources accessed and verified January-February 2026.\n"}],"how-to":[{slug:"apply-fidelity-thinking",title:"Apply Fidelity Thinking to Development Tasks",content:"# Apply Fidelity Thinking to Development Tasks\n\nFidelity thinking prevents two wastes: over-building features that don't survive validation, and under-building features that reach production unprepared.\n\n## The Concept\n\nThe Iron Triangle says you can control scope, time, and cost. Pick two.\n\nJeff Patton added a fourth dimension: fidelity. Not everything needs to be finished at the same level of polish. Same feature, different levels of completeness.\n\nThis gives you a new way to vary scope without cutting quality.\n\n## Three Fidelity Levels\n\n### Dirt Road\n\nHappy path works. Edge cases might crash it.\n\nGood for:\n- Spikes to test if an approach is viable\n- Prototypes to prove concepts before investment\n- Exploration in unknown territory\n- Learning new tools or patterns\n\nExample: CLI that works when called correctly, crashes with stack trace on bad input.\n\n### Cobble Road\n\nHandles edge cases. Has tests. Returns errors instead of crashing. Not hardened for production load.\n\nGood for:\n- Internal tools with low blast radius\n- Low-stakes features where failures don't cascade\n- MVPs to validate with real usage\n- First deployment to staging\n\nExample: CLI validates input, shows helpful errors, has test coverage for core paths.\n\n### Tarmac\n\nProduction-hardened. Observable. Documented. Load-tested.\n\nGood for:\n- User-facing features in critical paths\n- High-stakes systems (money, data, reputation)\n- Foundation components others build on\n- Anything where failures wake people up\n\nExample: CLI with structured logging, metrics, rate limiting, retry logic, operator runbooks.\n\n## How to Apply\n\n### 1. State the Target Before Building\n\nSay it explicitly: \"This is a dirt road implementation.\"\n\nClarify with stakeholders if not obvious:\n- \"Is this a spike or will this ship to users?\"\n- \"Internal tool or external API?\"\n- \"MVP or production feature?\"\n\n### 2. Build to That Level\n\nMatch effort to fidelity target.\n\nDirt road:\n- Write the happy path\n- Manual testing is enough\n- Skip observability\n\nCobble road:\n- Add error handling\n- Write unit tests\n- Basic input validation\n\nTarmac:\n- Add logging, metrics, tracing\n- Load test\n- Write runbooks\n- Security review\n\n### 3. Track What's at Which Level\n\nKeep a mental model or explicit list:\n- \"Auth is tarmac, admin panel is cobble, analytics dashboard is dirt road\"\n\nWhen something at dirt road moves to the critical path, upgrade it.\n\n### 4. Increase Fidelity When Validated\n\nDon't pave the road before you know it goes somewhere useful.\n\nUpgrade triggers:\n- Feature proved valuable in practice\n- Becoming user-facing\n- Causing incidents\n- Foundation for other work\n- Usage exceeds expectations\n\n## The Traps\n\nJumping straight to tarmac for everything:\n- Slow delivery\n- Wasted effort on features that don't survive\n- Over-engineering before validation\n\nStopping at dirt road for everything:\n- Accumulated technical debt\n- Production incidents from fragile code\n- Treating dirt road as if it were tarmac\n\n## Feature Grading\n\nThink of features like school grades:\n\n| Grade | What It Means |\n|-------|---------------|\n| A | Tarmac where tarmac is needed, dirt where dirt is fine |\n| B | Mostly appropriate fidelity, minor mismatches |\n| C | Some over-built, some under-built |\n| D | Significant fidelity mismatches causing problems |\n| F | All tarmac (too slow) or all dirt road (too fragile) |\n\nGrade A isn't \"everything is tarmac.\" It's \"fidelity matches stakes.\"\n\n## The Agile Pattern\n\nAgile combines incremental and iterative:\n\n1. Add features one at a time (incremental)\n2. Start each at dirt road (iterative - low fidelity first)\n3. Validate with users or reality\n4. Selectively pave to cobble or tarmac based on value\n\nThis prevents heavy investment before you know if the feature survives contact with users.\n\nBig bang (build everything to full fidelity before integrating) has late integration risk. Incremental alone (one feature at a time to full fidelity) has the \"right feature first\" problem. Iterative alone (all features at low fidelity) may never reach production quality.\n\nAgile balances both: build the next most important thing at the minimum viable fidelity, then increase fidelity for what proves valuable.\n\n## The Hook Reminder\n\nEvery 25 interactions, the fidelity-checkpoint hook reminds you:\n\n\"What fidelity level is the current work at?\"\n\nThis prevents fidelity amnesia - forgetting what's at dirt road and treating it as if it were tarmac.\n\n## Quality at Every Level\n\nFidelity is not an excuse for sloppiness.\n\nAt every level:\n- No dead code\n- No debug artifacts left in\n- Code does what it claims to do\n\nThe difference is scope:\n- Dirt road: works as intended (narrow scope)\n- Cobble road: + handles edge cases, has tests\n- Tarmac: + observable, documented, production-hardened\n\nA dirt road has no potholes. It's just narrow and unpaved.\n"},{slug:"calibrate-trust",title:"How to Calibrate Trust in AI Coding Suggestions",content:"# How to Calibrate Trust in AI Coding Suggestions\n\nWhen you use AI to write code, neither blind acceptance nor blanket rejection works. This guide shows you how to calibrate trust based on evidence.\n\n## The Core Problem\n\nAI code is often plausible but subtly wrong. Stack Overflow 2025 found fixing AI code takes 66% longer than writing from scratch. The issue: surface-level patterns look right, but hidden assumptions break at runtime.\n\nYou need a framework for deciding what to trust and what to verify.\n\n## Grade Claims by Evidence Level\n\nMatch your trust to the strength of evidence behind each suggestion.\n\n### The Four Evidence Levels\n\n| Level | What it means | How to verify |\n|-------|--------------|---------------|\n| **Strong** | Multiple peer-reviewed sources, replicated findings | Can cite 2+ authoritative sources |\n| **Moderate** | Single quality source or converging indirect evidence | Has one definitive reference |\n| **Weak** | Expert opinion, theoretical prediction | Based on analogy or experience |\n| **Speculative** | Reasonable inference without direct evidence | AI explicitly labels uncertainty |\n\n### Apply to AI Output\n\nWhen AI recommends connection pooling:\n\n```\nStrong evidence: \"Connection pooling improves throughput across\nPostgreSQL, MySQL, and Oracle (official docs).\"\n→ High trust, verify configuration only\n\nSpeculative: \"This pattern might cause issues at scale.\"\n→ Low trust, require load testing\n```\n\nThe problem: most AI presents everything with equal confidence. When you can't distinguish strong from speculative, trust becomes binary.\n\n**What to do:** Ask AI to grade its own claims. \"What evidence supports this? Is it Strong, Moderate, Weak, or Speculative?\"\n\n## Demand Contrastive Explanations\n\nResearch by Ma et al. (2025) found \"X instead of Y because Z\" triggers analytic processing. Flat recommendations (\"Use X\") trigger heuristic acceptance.\n\n### The Pattern\n\n```\nBad: \"Use Redis for this cache.\"\n\nGood: \"Redis instead of Memcached because you need data structures\nbeyond key-value (sorted sets for your leaderboard). If you only\nneeded simple KV caching, Memcached would be simpler.\"\n```\n\n### Why This Works\n\nContrastive framing:\n1. Shows alternatives were considered\n2. Makes tradeoffs visible\n3. Requires you to evaluate the comparison\n4. Teaches transferable judgment\n\n### When Accepting Code\n\nBefore copying any AI suggestion, ask: \"What did you reject and why?\"\n\nIf AI can't articulate what it chose NOT to do, it hasn't thought through alternatives.\n\n## Surface Assumptions Explicitly\n\nAI code often contains hidden assumptions that break your specific context.\n\n### The \"Almost Right\" Trap\n\nAI generates plausible code. You recognize familiar patterns and reduce scrutiny. Subtle errors are embedded in mostly-correct context. They surface later, now entangled with dependent code.\n\n### Counter-Pattern\n\nRequire assumption statements:\n\n```\n\"Here's the connection pool implementation. Key assumptions:\n - Connection pool is thread-safe (verify your driver)\n - Error retry is idempotent (check your endpoint)\n - Timeout is appropriate for your network (adjust if remote)\"\n```\n\nIf AI doesn't volunteer assumptions, ask: \"What needs to be true for this to work?\"\n\n## Use Falsification Before Acceptance\n\nBefore accepting any recommendation, ask \"What would need to be true for this to be wrong?\"\n\n### The Process\n\n1. AI recommends PostgreSQL\n2. **Before accepting:** \"What's the strongest argument AGAINST PostgreSQL?\"\n3. AI surfaces write-heavy workload risk\n4. **You decide:** Does that apply to your case?\n\n### Example\n\n```\nAI: \"I recommend PostgreSQL for this use case.\n\nStrongest argument against: Your write pattern (10K inserts/sec)\ncould hit WAL bottlenecks. If writes are your dominant workload,\nCassandra would handle this better.\n\nWhy I still recommend PostgreSQL: Your read pattern needs complex\njoins that Cassandra can't do, and you can shard writes with Citus.\"\n```\n\nYou now understand the failure mode before it happens.\n\n## Understand the Confidence-Competence Inversion\n\nLee et al. (CHI 2025) found two critical correlations:\n\n| Confidence Type | Effect on Critical Thinking |\n|----------------|----------------------------|\n| AI-confidence (trust in AI) | β = -0.69 (harmful) |\n| Self-confidence (trust in self) | β = +0.35 (protective) |\n\n**The inversion:** Users who trust AI more think less critically. Users who trust themselves maintain evaluation.\n\nA skeptical user with mediocre AI outperforms a credulous user with SOTA AI. Your metacognitive sensitivity matters more than model accuracy.\n\n### What This Means\n\nStop asking \"Is this AI good?\" Start asking \"Am I evaluating effectively?\"\n\n## Apply Trust Gradients\n\nNot all code needs the same verification depth.\n\n| Output Type | Default Trust | Verification Level |\n|-------------|--------------|-------------------|\n| Formatting, syntax | High | Glance for obvious errors |\n| Library usage, API calls | Medium | Check docs for edge cases |\n| Business logic | Low | Full review against requirements |\n| Security-sensitive code | Very low | Dedicated security review |\n| Architecture decisions | Very low | Multiple perspectives + production context |\n\nMatch verification effort to risk.\n\n## Track Your Correction Rate\n\nA healthy correction rate is 10-30%.\n\n| Your Rate | What It Signals |\n|-----------|----------------|\n| Less than 5% | Probably under-reviewing |\n| 10-30% | Healthy calibration |\n| More than 50% | AI not effective for this task |\n\nIf you never correct AI, you're not looking hard enough. If you correct everything, stop using AI for this.\n\n## The Three-Question Check\n\nBefore accepting any significant AI output:\n\n1. **What assumptions is this based on?**\n   Surface hidden dependencies.\n\n2. **What would make this wrong?**\n   Identify failure conditions before they happen.\n\n3. **How would I verify this?**\n   Even if you don't verify everything, plan HOW you would.\n\n## What Senior Developers Do Differently\n\nStack Overflow 2025 found a trust gap:\n\n| Behavior | Seniors | Juniors |\n|----------|---------|---------|\n| Fully trust AI output | 2.5% | 17% |\n| Ship AI code with review | 32% | 13% |\n| Edit AI suggestions | Almost always | Often skip |\n\nSeniors treat AI output as a first draft from a junior developer:\n- Read the code, don't just scan\n- Check edge cases AI likely missed\n- Verify against production constraints AI doesn't know\n- Refactor for codebase patterns, not generic patterns\n\nIf you're junior: add explicit scaffolding. Require AI to surface assumptions. Ask \"what could go wrong\" before accepting.\n\n## Watch for Verification Decay\n\nTrust calibration degrades over time without maintenance.\n\n### The Pattern\n\n```\nDay 1:  Carefully review every suggestion\nDay 7:  Skim suggestions, spot-check occasionally\nDay 30: Glance at output, accept if it \"looks right\"\nDay 90: Auto-accept, only notice when things break\n```\n\n### Counter-Measures\n\n1. **Structured checklist** — takes less than 30 seconds, applied consistently\n2. **Spot audits** — randomly deep-verify some outputs even when confident\n3. **Red team rotations** — periodically assume output is wrong, try to find the error\n4. **Verification metrics** — track how often verification catches issues\n\nIf verification never catches anything, either AI is perfect (unlikely) or you're not looking hard enough.\n\n## Summary Checklist\n\nBefore accepting AI code:\n\n- [ ] AI graded evidence level (Strong/Moderate/Weak/Speculative)\n- [ ] AI showed contrastive reasoning (\"X instead of Y because Z\")\n- [ ] Assumptions explicitly surfaced\n- [ ] Strongest counter-argument identified\n- [ ] Trust level matches output risk (security = very low trust)\n- [ ] You understand WHY this works, not just THAT it works\n\nIf any box is unchecked, ask AI to provide it before proceeding.\n"},{slug:"recognize-debugging-loops",title:"How to Recognize and Break Debugging Loops",content:"# How to Recognize and Break Debugging Loops\n\nWhen working with AI coding assistants, debugging loops waste time and often introduce new bugs. This guide shows you how to spot the signs and break the cycle.\n\n## What the Hooks Detect\n\nThe `collab-scaffolds` plugin watches for two failure patterns:\n\n**Debugging loop detection** (`detect-debugging-loop.sh`):\n- Tracks consecutive Bash command failures in `/tmp/claude-mrwolf-state-${PPID}`\n- After 3 consecutive failures, triggers Mr. Wolf protocol\n- Looks for error patterns: \"error\", \"failed\", \"exception\", \"not found\", \"permission denied\"\n- Resets counter on success\n\n**Speculative fix prevention** (`speculative-fix-prevention.sh`):\n- Activates when 2+ consecutive failures have occurred\n- Warns when you edit files during active debugging\n- Requires evidence-gathering before code changes\n- Shares state file with loop detection\n\n## Signs You're in a Loop\n\nWatch for these patterns:\n\n**1. Same error keeps appearing**\n```\nattempt 1: \"FileNotFoundError: config.json\"\nattempt 2: \"FileNotFoundError: /app/config.json\"\nattempt 3: \"FileNotFoundError: ./config/settings.json\"\n```\nThe error type hasn't changed, only slight variations.\n\n**2. Solution complexity is growing**\n\nStarted with \"add a check\" and now you're refactoring the entire validation system. If the fix keeps expanding, you're not converging on the root cause.\n\n**3. Trying slight variations**\n\n\"Let me try imports in a different order\" or \"Maybe if I rename this variable\". Random perturbations without a hypothesis means you're guessing.\n\n**4. Referencing old context**\n\n\"Earlier we tried X\" or \"Remember when we changed Y\". If you're maintaining a mental history of attempts rather than diagnosing the current state, you're looping.\n\n## How to Break Loops\n\n### 1. Invoke Wolf Protocol\n\nWhen the debugging loop hook fires, the protocol is:\n\n```\n1. STOP current approach\n2. Tell user: \"This isn't converging. Bringing in Mr. Wolf.\"\n3. Spawn the mrwolf agent\n4. Do not continue debugging yourself\n```\n\nMr. Wolf systematically breaks down the problem:\n- Restate the actual goal (not \"fix this error\" but \"what are we trying to accomplish?\")\n- Classify problem type (configuration, logic, environment, interface)\n- Break into verifiable components\n- Test one thing at a time\n\n### 2. Gather Evidence First\n\nBefore editing code during active debugging:\n\n**Read the actual error output:**\n```bash\n# Not \"it says file not found\"\n# Read the full stack trace\ncat error.log | tail -20\n```\n\n**Check runtime state:**\n```bash\n# What files actually exist?\nls -la /path/to/directory\n# What's the actual config?\ncat config.json\n# What's running?\nps aux | grep process_name\n```\n\n**Form a hypothesis:**\nWrite down what you think is wrong and why before changing code. \"I believe the config path is incorrect because the error shows '/app/config.json' but the file is at '/config/app.json'.\"\n\n### 3. Use `/clear` for Fresh Context\n\nLong threads accumulate failed approaches. Start fresh:\n\n```\n/clear\n[Restate the goal]\n[Share only relevant diagnostic evidence]\n```\n\nThis prevents \"we already tried that\" bias and resets to first principles.\n\n### 4. Spawn a Subagent\n\nFor isolated investigation without polluting main context:\n\n```\nTask(\n  subagent_type=\"investigation\",\n  prompt=\"Analyze why config loading fails. Do not edit code.\"\n)\n```\n\nThe subagent investigates in a clean context, returns findings, then exits.\n\n### 5. Ask for Constraints\n\nIf you're hitting the same wall repeatedly, you may be missing information:\n\n\"What I know: X, Y, Z. What I don't know: Is there a firewall rule? Are there environment-specific configs? What's the actual deployment structure?\"\n\nHumans have context you don't. Ask.\n\n## Why This Matters\n\nResearch shows iterative debugging without convergence degrades code quality:\n\n**Shukla et al. (2025)**: Vulnerability density increases from 2.1 to 6.2 per 1K LOC over 5 iterations. Each refinement adds code without removing prior problems.\n\n**Effect**: 37.6% vulnerability increase when iteration doesn't converge.\n\nThe loop itself is the problem. Breaking it early prevents compounding technical debt.\n\n## When to Use Each Strategy\n\n| Situation | Strategy |\n|-----------|----------|\n| 3+ consecutive failures | Wolf Protocol (automatic) |\n| 2+ failures + about to edit code | Gather evidence first |\n| Thread is long and confused | `/clear` and restate |\n| Need focused investigation | Spawn subagent |\n| Keep hitting unknown constraint | Ask human |\n\n## Opt-Out\n\nIf you need to disable the hooks temporarily:\n\n```bash\nexport SKIP_MRWOLF_HOOKS=1      # Disables debugging loop detection\nexport SKIP_EVIDENCE_HOOKS=1    # Disables speculative fix prevention\n```\n\nUse sparingly. The hooks catch real problems.\n"},{slug:"verify-refactoring",title:"How to Verify Refactoring is Complete",content:"# How to Verify Refactoring is Complete\n\nAI-assisted refactoring is systematically incomplete. Research shows significant declines in refactoring activity since AI adoption — GitClear (2025) found an 8x increase in code duplication and substantial reduction in refactoring work. This guide shows you how to verify your changes are actually done.\n\n## What the Hook Detects\n\nThe `incomplete-refactoring-guard.sh` hook runs after git commits that rename files:\n- Extracts old basenames from renamed files\n- Searches the codebase for remaining references\n- Warns when old names still exist\n\nIt catches what you forget: renamed files with stale imports, moved modules with old paths in configs, and string references in error messages.\n\n## The Core Problem: Instance vs Category Solving\n\nAI typically solves the specific request without addressing system-wide implications:\n\n**Instance solving (wrong):**\n- Rename this function\n\n**Category solving (right):**\n- Rename function\n- Update all callers\n- Grep for string references\n- Update tests\n- Fix documentation\n\nAlways solve the category. If you renamed something, every reference must change.\n\n## The Post-Refactor Checklist\n\nRun these checks after every rename, move, or deletion.\n\n### 1. Zero-Hits Test\n\nFor each old name, verify it's gone:\n\n```bash\ngit grep \"old_function_name\"\n```\n\nZero hits means you're done. Any hits require action:\n- Update to new name\n- Delete if it's dead code\n- Verify if it's intentional (changelog, migration notes)\n\n### 2. Check Imports\n\nFind stale import paths:\n\n```bash\n# Python\nrg \"import.*old_name|from old_name\"\nrg \"import.*old/path\"\n\n# JavaScript/TypeScript\nrg \"from ['\\\"].*old_name\"\nrg \"require\\(['\\\"].*old_name\"\n\n# Rust\nrg \"use .*old_name\"\n```\n\nUpdate all import statements to new paths. Remove unused imports.\n\n### 3. Find Orphaned Tests\n\nTests must follow the code they test:\n\n```bash\n# Find tests referencing old names\nrg \"old_function_name\" --glob \"*test*\"\nrg \"old_function_name\" --glob \"*spec*\"\n\n# Find test files for deleted features\nfind . -name \"*old_feature*test*\"\n```\n\nMove tests when code moves. Rename tests when code renames. Delete tests when features delete.\n\nTests that pass but verify wrong behavior are worse than failing tests.\n\n### 4. Scan Configuration Files\n\nBuild systems reference paths directly:\n\n```bash\n# Check build configs\nrg \"old_path|old_name\" --glob \"*.json\" --glob \"*.toml\" --glob \"*.yaml\"\n\n# Check CI configs\nrg \"old_path|old_name\" .github/ .gitlab-ci.yml\n\n# Check scripts\nrg \"old_path|old_name\" --glob \"*.sh\" --glob \"Makefile\"\n```\n\nUpdate:\n- Build configs (webpack.config.js, vite.config.ts, Cargo.toml)\n- CI pipelines (.github/workflows, .gitlab-ci.yml)\n- Shell scripts and Makefiles\n- Environment files (.env, .env.example)\n- Docker configs (Dockerfile, docker-compose.yml)\n\n### 5. Update Documentation\n\nDocumentation must match current reality:\n\n```bash\n# Check README files\nrg \"old_name|old_path\" --glob \"*README*\"\n\n# Check project instructions\nrg \"old_name|old_path\" --glob \"*CLAUDE*\"\n\n# Check inline comments\nrg \"old_name\" --type rust --type python --type typescript\n```\n\nUpdate:\n- README examples and paths\n- CLAUDE.md project instructions\n- Inline code comments\n- API documentation\n- Architecture diagrams\n\n### 6. Search for String References\n\nOld names in strings won't break at compile time but will confuse at runtime:\n\n```bash\n# Error messages\nrg \"\\\".*old_name.*\\\"\" --type rust --type python --type typescript\n\n# Log messages\nrg \"log.*old_name|print.*old_name\"\n\n# User-facing strings\nrg \"old_name\" --glob \"*strings*\" --glob \"*locale*\"\n```\n\nUpdate error messages, log output, and user-facing strings to reference current names.\n\n## When the Hook Catches You\n\nIf incomplete-refactoring-guard fires after commit:\n\n```\nINCOMPLETE REFACTORING DETECTED. Old names still found in codebase:\n- `old_module` still referenced in: src/app.ts, tests/integration.test.ts\n```\n\nRun the checklist above for each flagged name. Commit the cleanup as a follow-up:\n\n```bash\n# Fix remaining references\ngit add .\ngit commit -m \"fix: complete refactoring cleanup for old_module\"\n```\n\n## Common Failure Patterns\n\n| Pattern | Symptom | Prevention |\n|---------|---------|-----------|\n| Rename without grep | Runtime errors from stale references | Always grep after rename |\n| Move without import update | Import errors | Run tests before committing |\n| Delete without cascade | Orphaned tests, stale config | Grep for deleted name |\n| Partial migration | Old way and new way coexist | Complete or revert |\n| String references missed | Error messages have old names | Search for strings too |\n\n## Category Thinking: The Boy Scout Rule\n\nEvery refactoring should leave the codebase cleaner. If artifacts of old state remain, the work isn't done.\n\nInstead of fixing \"this one reference to oldName,\" search for ALL references and fix them as a batch. Think in categories, not instances:\n\n**Wrong:**\n1. Rename file\n2. Fix the import error that shows up\n3. Commit\n\n**Right:**\n1. Rename file\n2. Grep for all references\n3. Update imports, configs, docs, tests\n4. Verify zero hits\n5. Commit\n\nThe refactoring isn't complete until the old name produces zero grep results.\n\n## Opt-Out\n\nIf you need to disable the hook temporarily:\n\n```bash\nexport SKIP_REFACTOR_HOOKS=1\n```\n\nUse sparingly. The hook catches what you miss.\n"}],tutorials:[{slug:"debugging-with-mrwolf",title:"Debugging with the Wolf Protocol",content:"# Debugging with the Wolf Protocol\n\nA guided walkthrough showing how Mr. Wolf and the Wolf Protocol break debugging loops.\n\n## What You'll Learn\n\nBy the end of this tutorial, you'll understand how the Wolf Protocol works in practice and when Mr. Wolf steps in to help.\n\n## The Problem\n\nYou're working on a Python test that passes locally but fails in CI. The error message is cryptic.\n\n```\nFAILED tests/test_auth.py::test_token_expiry - AssertionError: expected 401, got 200\n```\n\nYour first thought: \"Maybe the test setup is wrong.\"\n\n## Attempt 1: Quick Fix\n\nYou adjust the test setup, changing the token expiration time from 60 seconds to 1 second.\n\n```python\ndef test_token_expiry():\n    token = create_token(expires_in=1)  # Changed from 60\n    time.sleep(2)\n    response = api_call_with_token(token)\n    assert response.status_code == 401\n```\n\nPush to CI. Wait. Still fails.\n\n```\nFAILED tests/test_auth.py::test_token_expiry - AssertionError: expected 401, got 200\n```\n\nThe debugging loop hook increments a failure counter silently in the background. You don't see it yet.\n\n## Attempt 2: Another Quick Fix\n\n\"Maybe CI is caching something.\" You add cache clearing:\n\n```python\ndef test_token_expiry():\n    clear_token_cache()  # Added this\n    token = create_token(expires_in=1)\n    time.sleep(2)\n    response = api_call_with_token(token)\n    assert response.status_code == 401\n```\n\nPush. Wait. Same error.\n\n```\nFAILED tests/test_auth.py::test_token_expiry - AssertionError: expected 401, got 200\n```\n\nFailure counter: 2. Still silent.\n\n## Attempt 3: The Loop Breaks You\n\n\"Maybe the API isn't checking expiration at all in CI.\" You try mocking the time:\n\n```python\n@freeze_time(\"2024-01-01 12:00:00\")\ndef test_token_expiry():\n    clear_token_cache()\n    token = create_token(expires_in=1)\n\n    with freeze_time(\"2024-01-01 12:00:03\"):\n        response = api_call_with_token(token)\n\n    assert response.status_code == 401\n```\n\nPush. Wait. Same error.\n\n```\nFAILED tests/test_auth.py::test_token_expiry - AssertionError: expected 401, got 200\n```\n\nFailure counter: 3. The hook fires.\n\n```\nDEBUGGING LOOP DETECTED (3 consecutive failures). You MUST now:\n1) Tell the user: 'This isn't converging. Bringing in Mr. Wolf.'\n2) Spawn the `mrwolf` agent to break down the problem systematically.\nDo not continue debugging yourself.\n```\n\nClaude spawns Mr. Wolf.\n\n## Mr. Wolf Takes Over\n\nMr. Wolf starts with the protocol.\n\n### Step 1: What's Actually Happening?\n\nMr. Wolf asks you to fill this in:\n\n```\nWhat I'm trying to do: Test that expired tokens return 401\nWhat's happening instead: Test gets 200 (success) for an expired token\nWhat I've already tried:\n  - Changed expiration time to 1 second\n  - Cleared token cache\n  - Mocked time with freeze_time\n```\n\nWriting this down, something clicks. The test expects 401. It's getting 200. The token is being accepted when it shouldn't be.\n\n### Step 2: Classify the Problem\n\nMr. Wolf identifies the type:\n\n```\nType: Something's broken\nSigns: Error message, unexpected behavior\nApproach: Find the gap between expectation and reality\n```\n\nThe gap: You expect the API to reject expired tokens. It's not rejecting them.\n\n### Step 3: Route to the Right Technique\n\nThe problem type is \"something's broken.\" Mr. Wolf routes to hypothesis-from-evidence: \"What's the smallest input that reproduces this?\"\n\nYou realize: you've been changing the test, not checking if the API actually validates expiration.\n\nMr. Wolf suggests one hypothesis:\n\n```\nHypothesis: The API isn't checking token expiration in the CI environment.\n\nTest: Check if the API has different configuration in CI vs. local.\n```\n\nYou look at the CI environment variables. There it is:\n\n```yaml\nenv:\n  SKIP_AUTH_VALIDATION: true  # Left over from debugging last month\n```\n\nThe API wasn't validating anything in CI.\n\n### Step 4: One Thing at a Time\n\nMr. Wolf doesn't let you fix everything at once. One thing:\n\n```\n1. Remove SKIP_AUTH_VALIDATION from CI config\n```\n\nYou remove the line. Push. Wait.\n\n```\nPASSED tests/test_auth.py::test_token_expiry\n```\n\nGreen.\n\n### Step 5: Verify\n\nMr. Wolf asks the verification questions:\n\n```\n- Does it actually work? YES (test passes)\n- Did I solve the problem or work around it? SOLVED (removed root cause)\n- Will this hold? YES (config persists)\n```\n\nProblem solved properly.\n\n## The Speculative Fix Prevention Hook\n\nHere's what would have happened if you tried to edit code earlier without evidence.\n\nAfter attempt 2, you're at failure count 2. You decide to rewrite the entire token validation function.\n\nYou start editing `auth/tokens.py`. The hook fires:\n\n```\nSPECULATIVE FIX DETECTED. You're editing during active debugging\n(2+ consecutive failures) without convergence. STOP and gather\nevidence first:\n\n1. Read the relevant error output and stack traces\n2. Check the actual runtime state (logs, data, config)\n3. Form a hypothesis based on evidence\n4. THEN write the fix\n\nSpeculative fixes without evidence waste time and often introduce new bugs.\n```\n\nThis would have stopped you from rewriting working code when the problem was in configuration.\n\n## What Made the Difference\n\n### Without Wolf Protocol (Spinning)\n\n- Try random fix\n- Push, wait\n- Try another random fix\n- Push, wait\n- Try another random fix\n- Push, wait\n- Never stop to think\n\n### With Wolf Protocol (Structured)\n\n- Stop spinning\n- Articulate what's actually happening\n- Classify the problem type\n- Form ONE hypothesis based on evidence\n- Test the hypothesis\n- Verify the solution\n\nThe protocol forces you to stop and think before you act.\n\n## Key Takeaways\n\n**The hook triggers at 3 failures.** Three consecutive bash commands that show error patterns trigger Mr. Wolf.\n\n**The speculative fix hook triggers at 2 failures.** If you try to edit code during active debugging without gathering evidence first, you get warned.\n\n**Articulation reveals gaps.** Writing down \"What I've tried\" often shows patterns you missed.\n\n**One hypothesis at a time.** Testing multiple changes at once means you never know what fixed it.\n\n**Evidence before fixes.** Read error output, check runtime state, then form hypothesis.\n\n## When to Call Mr. Wolf Yourself\n\nYou don't have to wait for the hook. Call Mr. Wolf when:\n\n- You've tried 2 things and neither worked\n- You're trying the same approach in different ways\n- You can't articulate what's actually wrong\n- The problem feels like it's getting bigger, not smaller\n\nSay: \"Bring in Mr. Wolf\" or \"This needs Mr. Wolf.\"\n\n## Practice Exercise\n\nNext time you hit a bug:\n\n1. Try one thing\n2. If it doesn't work, try one more\n3. If that doesn't work, STOP\n4. Fill out the Wolf Protocol Step 1 in writing\n5. Classify the problem\n6. Form ONE hypothesis\n7. Test it\n\nThe hardest part is stopping. The protocol works when you use it.\n"},{slug:"mastery-oriented-session",title:"Running a Mastery-Oriented Collaborative Coding Session",content:"# Running a Mastery-Oriented Collaborative Coding Session\n\nA walkthrough of building a feature using collab-scaffolds principles to maintain skill while working with AI.\n\n## What You'll Learn\n\nBy the end of this tutorial, you'll have practiced a complete collaborative session pattern that:\n- Preserves your cognitive skills while using AI\n- Builds understanding, not just working code\n- Identifies when you're slipping into dependency patterns\n- Evaluates outcomes at three levels (task, project, system)\n\nThis is a concrete scenario. Follow along with your own feature work, or use this as a reference pattern.\n\n## The Scenario\n\nYou're adding a new API endpoint to an existing service: `POST /api/tasks/{id}/complete` that marks a task as complete and triggers notification logic.\n\n## Phase 1: Before Starting (5 minutes)\n\nBefore writing any code, state your intent and target quality level.\n\n### What to do\n\n**State the goal clearly:**\n\n\"I'm adding a task completion endpoint. It should mark the task complete, trigger notifications, and return the updated task.\"\n\n**Set fidelity target:**\n\n\"This is a cobble road feature. It needs tests and error handling, but not production monitoring yet.\"\n\nSee the methodology guide for fidelity levels. Cobble road means: handles edge cases, has tests, but doesn't need full observability. This prevents over-engineering (building tarmac for everything) and under-engineering (shipping dirt roads to production).\n\n### Wrong approach\n\nStarting without a clear goal:\n\n\"Add task stuff... we'll figure it out as we go.\"\n\nThis leads to scope creep, unclear requirements, and wasted effort.\n\n## Phase 2: Planning Before Code (10 minutes)\n\nNow comes the critical step: PME friction. Planning-Monitoring-Evaluation metacognitive engagement.\n\n### What to do\n\n**First: Read the existing code**\n\nBefore asking AI for help, spend 10 minutes understanding what exists:\n\n```bash\n# Find the existing task model\ngrep -r \"struct Task\" src/\n\n# Find similar endpoints\ngrep -r \"POST /api/tasks\" src/\n\n# Understand the notification system\ngrep -r \"notify\" src/\n```\n\nThis reading step is non-negotiable. Research shows that approaching AI with understanding leads to significantly better learning outcomes than passive acceptance (Bastani et al., PNAS 2025).\n\n**Second: Form your own approach**\n\nWrite down (in comments, scratch file, or conversation):\n\n\"I think I need to:\n1. Add route handler in routes/tasks.rs\n2. Update Task model with `completed_at` timestamp\n3. Call notification service from handler\n4. Return 200 with updated task\"\n\nThis is the Planning component of PME friction. It activates metacognitive monitoring.\n\n**Third: Present to AI**\n\n\"Here's what I'm planning. Before you generate code, does this approach make sense given the existing architecture?\"\n\n### Wrong approach\n\nImmediately asking AI:\n\n\"Write me a task completion endpoint.\"\n\nThis bypasses the generative step where you build understanding. The AI generates, you accept. Your neural pathways for planning and design remain inactive. Over 3 months, this pattern contributes to measurable skill degradation (Budzyń et al., Lancet 2025).\n\n## Phase 3: Implementation (20-30 minutes)\n\nAI generates code. Your job: comprehend it.\n\n### What to do\n\n**AI generates the handler:**\n\n```rust\npub async fn complete_task(\n    Path(id): Path\u003CUuid>,\n    State(app): State\u003CAppState>,\n) -> Result\u003CJson\u003CTask>, StatusCode> {\n    let task = app.db\n        .update_task(id, |t| {\n            t.completed_at = Some(Utc::now());\n            t.status = TaskStatus::Complete;\n        })\n        .await\n        .map_err(|_| StatusCode::NOT_FOUND)?;\n\n    app.notifier.notify_completion(&task).await?;\n\n    Ok(Json(task))\n}\n```\n\n**Critical: Don't just accept it. Ask questions:**\n\nYou: \"Why did you use `map_err` instead of `?` directly?\"\n\nAI: \"The DB error doesn't implement `Into\u003CStatusCode>`. The `map_err` converts it to NOT_FOUND for the HTTP response.\"\n\nYou: \"What happens if notification fails?\"\n\nAI: \"Currently, the whole request fails with 500. We could make notifications non-blocking if that's better for your use case.\"\n\nYou: \"Yeah, task completion should succeed even if notifications fail. Let's make that fire-and-forget.\"\n\n**This dialogue is the difference between learning and dependency.**\n\nWhen you ask follow-up questions, you're:\n- Catching design issues (notification shouldn't block)\n- Building mental models (error conversion patterns)\n- Maintaining cognitive engagement (not rubber-stamping)\n\n### Wrong approach\n\nAI generates code. You read it briefly, think \"looks reasonable,\" and paste it in.\n\nThis is vibe coding. 17% of junior developers ship AI code with zero edits (Stack Overflow 2025). 45% of AI-generated code contains critical vulnerabilities (Veracode 2025). The overlap is concerning.\n\n### Monitoring checkpoint\n\nAfter implementation, pause:\n\n\"Does this match what I expected?\"\n\nIf the answer is \"I'm not sure what I expected,\" that's a warning sign. You've entered automation bias territory.\n\n## Phase 4: Verification (10 minutes)\n\nYou're not done when it works. You're done when it's right.\n\n### Three checks\n\n**1. Task verification: Does it work?**\n\n```bash\n# Run the tests\ncargo test complete_task\n\n# Try it manually\ncurl -X POST http://localhost:3000/api/tasks/123/complete\n```\n\nThis is table stakes.\n\n**2. Project verification: Is the codebase better?**\n\nLook at what changed:\n- Is the code cleaner than what was there before?\n- Did we leave any dead code or debug statements?\n- Do the tests verify requirements or just implementation?\n\nRun the cleanup check:\n\n```bash\n# Any debug statements?\ngrep -r \"println!\" src/\n\n# Any commented-out code?\ngrep -r \"// \" src/ | grep -v \"// \" | head -20\n\n# Any unused imports?\ncargo clippy\n```\n\nIf artifacts remain, the work isn't done.\n\n**3. Compound verification: Is the next change easier?**\n\nImagine the next feature: \"Add task assignment to multiple users.\"\n\nWith what you just built, is that easier or harder? If you followed the patterns, reused abstractions, and kept the codebase clean, it should be easier.\n\nIf you hacked around existing patterns, the next change got harder.\n\n### Wrong approach\n\n\"It works, ship it.\"\n\nThis optimizes for the immediate task while degrading the system. Technical debt is a tax on every subsequent change. The 5 minutes saved today costs 50 minutes over the next month.\n\n## Phase 5: Evaluation (5 minutes)\n\nCrystallize what you learned.\n\n### What to do\n\nAsk yourself:\n- \"What would I do differently next time?\"\n- \"What pattern worked that I should reuse?\"\n- \"What did I learn about this codebase?\"\n\nExample crystallization:\n\n\"Next time: check notification patterns before implementation. We almost made task completion blocking on notifications, which would have degraded UX. Learned: fire-and-forget for non-critical side effects.\"\n\nThis is the Evaluation component of PME friction. It turns experience into transferable knowledge.\n\n### Wrong approach\n\nFinishing the task and immediately moving to the next one.\n\nWithout evaluation, each session is isolated. You don't compound learning across sessions.\n\n## Phase 6: Watch for Anti-Patterns\n\nThese are the traps that degrade mastery:\n\n### Vibe coding\n\n**What it is:** Accepting AI suggestions without reading or understanding them.\n\n**How it feels:** \"AI generated it, looks reasonable, moving on.\"\n\n**Why it's harmful:** 45% of AI code has critical vulnerabilities (Veracode 2025). If you don't read it, you ship vulnerable code.\n\n**Counter:** Treat every AI suggestion as a first draft from a junior developer. You wouldn't ship junior code without review. Same standard applies.\n\n### Avoidance crafting\n\n**What it is:** Using AI to skip cognitively demanding work rather than amplify it.\n\n**How it feels:** \"This debugging is hard, I'll ask AI to solve it.\"\n\n**Why it's harmful:** The hard parts are where learning happens. Delegating hard work atrophies the most valuable skills: architecture, debugging, design.\n\n**Counter:** Reserve hard problems for yourself. Use AI for boilerplate and exploration, not as a crutch for difficult thinking.\n\n### Productivity illusion\n\n**What it is:** Believing AI makes you faster when it actually makes you slower.\n\n**How it feels:** \"I'm shipping so much code!\"\n\n**Why it's harmful:** Research shows experienced developers are 19% slower with AI but perceive themselves as 24% faster (METR 2025). That's a 43-percentage-point miscalibration gap. You can't trust your intuition here.\n\n**Counter:** Measure actual time to completion, including:\n- Prompting and clarification time\n- Code review time\n- Debugging AI-generated code\n- Integration time\n\nTrack tasks with and without AI. Let data override feelings.\n\n## The Complete Pattern\n\nWhen you put it all together:\n\n1. State goal and fidelity level before starting\n2. Read existing code and form your own approach\n3. Present approach to AI before asking for code\n4. Ask follow-up questions about generated code\n5. Monitor: \"Does this match what I expected?\"\n6. Three verifications: task, project, compound value\n7. Evaluate: \"What would I change next time?\"\n8. Watch for anti-patterns: vibe coding, avoidance, productivity illusion\n\nThis pattern takes 50-60 minutes for a feature that might take 40 minutes with pure AI generation.\n\nThat extra 10-20 minutes is the investment in maintaining your capability. It's the difference between learning and dependency, between compounding skill and degrading skill.\n\n## Next Steps\n\n- Try this pattern with your next feature\n- Pay attention to monitoring checkpoints\n- Notice when you're tempted to skip steps\n- Measure: track actual completion time with awareness vs without\n\nThe pattern feels slower at first. That's the friction working. Over time, the compounding understanding makes you genuinely faster.\n\n## Further Reading\n\n- `/docs/explanation/methodology.md` - Research behind these principles\n- `/skills/collaboration/references/behavioral-awareness.md` - Anti-patterns in depth\n- `/skills/collaboration/references/productivity-reality.md` - Evidence on AI-assisted development\n"},{slug:"rubber-duck-with-duck",title:"Rubber Duck with Duck",content:"# Rubber Duck with Duck\n\nLearning architecture decisions through Socratic dialogue.\n\n---\n\n## What You'll Learn\n\nHow to make better architecture decisions by thinking out loud with the Duck agent. Not getting answers—discovering them yourself through structured questioning.\n\n## The Scenario\n\nYou're building a notification service. Users subscribe to events. When events happen, they get notified. Simple enough.\n\nBut now you need to decide: traditional CRUD with a relational database, or event sourcing?\n\nYou've read about event sourcing. It sounds powerful. But is it right here?\n\nInstead of asking Claude \"should I use event sourcing?\", you bring in Duck.\n\n## The Conversation\n\n**You**: I'm building a notification service. Thinking about using event sourcing instead of traditional CRUD. Talk me through it?\n\n**Duck**: Let's start here—what's driving you toward event sourcing?\n\n**You**: Well, we need a full audit trail of what notifications were sent. Event sourcing gives us that automatically.\n\n**Duck**: Okay, audit trail is the goal. What about your read patterns? How do you query this data?\n\n**You**: Mostly \"show me all notifications sent to user X\" and \"what notifications went out in the last hour.\"\n\n**Duck**: Walk me through how those queries work with event sourcing.\n\n**You**: I'd... rebuild the state by replaying events? No wait, I'd need projections. A read model that gets updated when events come in.\n\n**Duck**: So you'd have the event stream AND a projection. What happens when requirements change and you need a different view of the data?\n\n**You**: I'd build a new projection by replaying the event stream from the beginning. That's actually kind of powerful—I can create new views retroactively.\n\n**Duck**: True. Now flip it—what's the team's experience with event sourcing?\n\n**You**: We've never used it. We're comfortable with PostgreSQL and standard CRUD patterns.\n\n**Duck**: What does that mean for velocity? For debugging when things go wrong?\n\n**You**: Hmm. Learning curve would be steep. Debugging distributed event flows is way harder than debugging SQL queries. If something breaks at 3am...\n\n**Duck**: Keep going—you're onto something.\n\n**You**: If something breaks, the on-call engineer needs to understand event replay, projection consistency, eventual consistency semantics. Versus \"here's a row in a table, here's the query that reads it.\"\n\n**Duck**: So you've got: audit trail requirement, query patterns that need indexed access, team familiarity with CRUD, learning curve for event sourcing. What about the audit trail requirement—are there other ways to get it?\n\n**You**: Oh. PostgreSQL has triggers. I could log every INSERT, UPDATE, DELETE to an audit table. That's... actually straightforward. And the team knows how to do it.\n\n**Duck**: Does that solve your original problem?\n\n**You**: Yeah. Full audit trail, queries work normally, no learning curve, operational simplicity. Event sourcing would be over-engineering for this.\n\n**Duck**: So what's the decision?\n\n**You**: CRUD with audit logging. Event sourcing would be cool, but it doesn't match our constraints.\n\n## What Just Happened\n\nLook at that progression. You started thinking \"event sourcing sounds powerful.\" Duck didn't say \"no, use CRUD.\" Duck asked questions:\n\n- What are your read patterns?\n- How would queries work?\n- What's the team's experience?\n- Are there simpler ways to solve the core requirement?\n\nThe questions exposed constraints you hadn't articulated:\n\n- Team familiarity (operational risk)\n- Query patterns (complexity of projections)\n- Audit requirement (solvable without event sourcing)\n\nYou discovered the answer. Duck just held up the mirror.\n\n## The Research Backing This\n\nThis isn't just feel-good pedagogy. The evidence is clear.\n\n**HypoCompass pattern** (Stanford SCALE 2025): When humans evaluate AI-generated hypotheses instead of receiving solutions, debugging performance improves 12%. You're the judge, not the recipient.\n\n**Cognitive Mirror** (Tomisu et al., Frontiers in Education 2025): AI reflects reasoning back with structured questions. This preserves the generative step—you do the thinking, Duck provides scaffolding. When you must articulate your reasoning, you engage metacognitive monitoring: detecting gaps, contradictions, assumptions.\n\n**Mastery vs Performance orientation** (Australian Catholic University 2025): Mastery-oriented users are 35.7 times more likely to demonstrate critical thinking. Performance-oriented users (\"just give me the answer\") show skill degradation over time.\n\nDuck is designed for mastery. Not dependency.\n\n## When to Use Duck vs Mr. Wolf\n\nThis matters. They solve different problems.\n\n| Use Duck when | Use Mr. Wolf when |\n|---------------|-------------------|\n| Exploring options | Something is broken NOW |\n| Making architecture decisions | Stuck after multiple attempts |\n| Thinking through trade-offs | Going in circles |\n| You need clarity | You need the problem solved |\n\nDuck helps you think. Wolf helps you unstick.\n\nIf you bring Duck to a production incident, you'll waste time. If you bring Wolf to an architecture decision, you'll outsource your thinking.\n\n## What Makes This Work\n\nThe Socratic progression Duck follows:\n\n1. What are you trying to do?\n2. Why this approach over alternatives?\n3. How does this work step by step?\n4. What if your assumptions are wrong?\n5. So what's the decision?\n\nEach question builds on the last. By the time you reach \"so what's the decision?\", the answer is usually obvious.\n\n## Try It Yourself\n\nNext time you're making a technical decision—framework choice, architecture pattern, API design—don't ask \"what should I use?\"\n\nAsk Duck: \"I'm thinking X, talk me through it.\"\n\nThen listen to your own reasoning. Duck will reflect it back. You'll hear the gaps. The missing constraints. The assumptions that don't hold.\n\nThat's when learning happens.\n\n---\n\nThe goal isn't to get the right answer faster. The goal is to build the reasoning muscles that let you find right answers yourself.\n\nDuck doesn't make you dependent. Duck makes you more capable.\n"}]},docCount:9}},uses:{params:["slug"]}}}(Array(5)))],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
