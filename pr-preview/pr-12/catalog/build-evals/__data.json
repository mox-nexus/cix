{"type":"data","nodes":[null,{"type":"data","data":[{"plugin":1},{"slug":2,"manifest":3,"tagline":9,"readme":10,"components":11,"variant":14,"narrativeHook":15,"constraint":16},"build-evals",{"name":2,"description":4,"version":5,"author":6},"Eval methodology for AI systems. Use when: writing evals for agents, skills, MCP servers, prompts, or measuring AI behavior.","0.2.1",{"name":7,"email":8},"yzavyas","yza.vyas@gmail.com","","# build-evals\n\nEval methodology for AI systems. Write evals that measure what matters.\n\n## When to Use\n\n- Building test suites for agents, skills, MCP servers\n- Measuring agent effectiveness\n- Evaluating multi-agent coordination\n- Choosing eval frameworks\n\n## Skills\n\n- **build-eval**: Eval methodology covering grader types, metrics, frameworks\n\n## Key Concepts\n\n- **Three grader types**: Code-based, Model-based, Human\n- **Non-determinism**: pass@k, pass^k for stochastic outputs\n- **Match eval to agent type**: Coding, Conversational, Research, Computer Use, Multi-Agent\n",{"agents":12,"skills":13,"hooks":12,"commands":12},0,1,"emergence","Rigorous evaluation methodology for AI agents, skills, and prompts.","Evidence-Driven Design"],"uses":{"params":["slug"]}}]}
