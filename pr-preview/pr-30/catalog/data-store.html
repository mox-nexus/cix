<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../_app/immutable/assets/0.04mfEt8Y.css" rel="stylesheet">
		<link href="../_app/immutable/assets/CrossLinks.C4T6atsV.css" rel="stylesheet">
		<link href="../_app/immutable/assets/5.Bnuf7QZ4.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.CsUPtz0y.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D-20Pf_E.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B4rBkemD.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BIl_gPaX.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D0iwhpLH.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D0_jRxqB.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BrIrVKyt.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app._hLoc1zF.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CDHW4-dF.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BN47ziLw.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BoBPFXpg.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DoYPt3WR.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/G6a9SXxM.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DOGfeGBa.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/EaAB93f5.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.BouOHi01.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CWj1OF8I.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BvVmrJMT.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B_iuWMbd.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/5.BlQAopRs.js"><!--12qhfyh--><meta name="description" content="Extensions that enhance human capability, not replace it."/><!----><!--wzm642--><meta name="description" content="Data storage and retrieval patterns. Use when: choosing databases, implementing search, designing hybrid retrieval, selecting embedding models, building RAG systems."/><!----><title>data-store — cix</title>
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><a href="#main" class="skip-link">Skip to content</a> <!--[--><nav class="site-nav svelte-qgym72" aria-label="Site navigation"><a href="../" class="nav-wordmark svelte-qgym72">cix</a> <div class="nav-links svelte-qgym72"><!--[--><a href="../catalog" class="nav-link svelte-qgym72">catalog</a><a href="../library" class="nav-link svelte-qgym72">library</a><!--]--></div></nav><!--]--> <div class="page svelte-12qhfyh has-nav"><!--[!--><!----><main id="main" class="detail-page svelte-wzm642" style="--variant-color: var(--spark-core)"><nav class="detail-back svelte-wzm642"><a href="../catalog" class="svelte-wzm642">← catalog</a></nav> <header class="detail-header svelte-wzm642"><div class="header-top svelte-wzm642"><h1 class="svelte-wzm642">data-store</h1> <span class="detail-kind svelte-wzm642">plugin</span> <span class="detail-version svelte-wzm642">0.1.0</span></div> <p class="detail-description svelte-wzm642">Data storage and retrieval patterns. Use when: choosing databases, implementing search, designing hybrid retrieval, selecting embedding models, building RAG systems.</p> <!--[--><div class="detail-inventory svelte-wzm642"><!--[--><span class="inv-item svelte-wzm642">1 skill</span><!--]--></div><!--]--> <!--[--><div class="detail-tags svelte-wzm642"><!--[--><span class="tag svelte-wzm642">storage</span><span class="tag svelte-wzm642">retrieval</span><span class="tag svelte-wzm642">search</span><span class="tag svelte-wzm642">embeddings</span><span class="tag svelte-wzm642">vector-db</span><span class="tag svelte-wzm642">rag</span><span class="tag svelte-wzm642">hybrid-search</span><!--]--></div><!--]--></header> <!--[--><nav class="detail-tabs svelte-wzm642" role="tablist"><!--[--><button role="tab" class="tab svelte-wzm642 active" aria-selected="true">README <!--[!--><!--]--></button><button role="tab" class="tab svelte-wzm642" aria-selected="false">Explanation <!--[--><span class="tab-count svelte-wzm642">2</span><!--]--></button><!--]--></nav><!--]--> <article class="detail-content svelte-wzm642 prose"><!--[--><!----><h1>data-store</h1>
<p>Production patterns for storage and retrieval: backends, search, embeddings, RAG.</p>
<h2>When to Use</h2>
<ul>
<li>Choosing storage backend for vectors/search</li>
<li>Implementing hybrid search (keyword + semantic)</li>
<li>Selecting embedding models</li>
<li>Building RAG retrieval layers</li>
<li>Personal corpus/knowledge base systems</li>
</ul>
<h2>Quick Guidance</h2>
<p><strong>Backend selection:</strong></p>
<ul>
<li>&lt;100K docs → DuckDB, SQLite, LanceDB (embedded)</li>
<li>100K-50M → PostgreSQL + pgvector ← Most apps</li>
<li>50M-1B → Qdrant, Weaviate</li>
<li><blockquote>
<p>1B → Milvus, ClickHouse</p>
</blockquote>
</li>
</ul>
<p><strong>Search mode:</strong></p>
<ul>
<li>Default to <strong>hybrid (RRF)</strong> — recall improves 0.72 → 0.91 over BM25 alone</li>
<li>Use RRF (rank-based), not score blending</li>
</ul>
<p><strong>Embedding models:</strong></p>
<ul>
<li>Personal corpus: Nomic Embed v1.5 (768 dims)</li>
<li>Speed-critical: all-MiniLM-L6-v2 (384 dims)</li>
<li>Best accuracy: Voyage-4-Large API (1024 dims)</li>
</ul>
<h2>Skills</h2>
<table>
<thead>
<tr>
<th>Skill</th>
<th>Use When</th>
</tr>
</thead>
<tbody><tr>
<td><code>data-store</code></td>
<td>Storage selection, search implementation, embedding choices</td>
</tr>
</tbody></table>
<h2>References</h2>
<ul>
<li><code>storage-backends.md</code> — Backend selection, scaling</li>
<li><code>search-algorithms.md</code> — HNSW, IVF, BM25, SPLADE</li>
<li><code>embedding-models.md</code> — Model comparison</li>
<li><code>hybrid-patterns.md</code> — RRF, re-ranking</li>
<li><code>rag-architecture.md</code> — Chunking, CRAG, Self-RAG</li>
<li><code>duckdb-search.md</code> — DuckDB FTS/VSS specifics</li>
</ul>
<!----><!--]--></article></main><!----><!--]--><!----></div> <div class="experimental-tag svelte-12qhfyh" aria-hidden="true">experimental</div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_18a28wn = {
						base: new URL("..", location).pathname.slice(0, -1),
						assets: "/cix/pr-preview/pr-30"
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.CsUPtz0y.js"),
						import("../_app/immutable/entry/app._hLoc1zF.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 5],
							data: [null,(function(a){a[0]="storage";a[1]="retrieval";a[2]="search";a[3]="embeddings";a[4]="vector-db";a[5]="rag";a[6]="hybrid-search";return {type:"data",data:{extension:{slug:"data-store",kind:"plugin",manifest:{name:"data-store",version:"0.1.0",description:"Data storage and retrieval patterns. Use when: choosing databases, implementing search, designing hybrid retrieval, selecting embedding models, building RAG systems.",author:{name:"Mox Labs",email:"mox.rnd@gmail.com"},license:"MIT",keywords:a},tagline:"Production patterns for storage and retrieval: backends, search, embeddings, RAG.",readme:"# data-store\n\nProduction patterns for storage and retrieval: backends, search, embeddings, RAG.\n\n## When to Use\n\n- Choosing storage backend for vectors/search\n- Implementing hybrid search (keyword + semantic)\n- Selecting embedding models\n- Building RAG retrieval layers\n- Personal corpus/knowledge base systems\n\n## Quick Guidance\n\n**Backend selection:**\n- \u003C100K docs → DuckDB, SQLite, LanceDB (embedded)\n- 100K-50M → PostgreSQL + pgvector ← Most apps\n- 50M-1B → Qdrant, Weaviate\n- >1B → Milvus, ClickHouse\n\n**Search mode:**\n- Default to **hybrid (RRF)** — recall improves 0.72 → 0.91 over BM25 alone\n- Use RRF (rank-based), not score blending\n\n**Embedding models:**\n- Personal corpus: Nomic Embed v1.5 (768 dims)\n- Speed-critical: all-MiniLM-L6-v2 (384 dims)\n- Best accuracy: Voyage-4-Large API (1024 dims)\n\n## Skills\n\n| Skill | Use When |\n|-------|----------|\n| `data-store` | Storage selection, search implementation, embedding choices |\n\n## References\n\n- `storage-backends.md` — Backend selection, scaling\n- `search-algorithms.md` — HNSW, IVF, BM25, SPLADE\n- `embedding-models.md` — Model comparison\n- `hybrid-patterns.md` — RRF, re-ranking\n- `rag-architecture.md` — Chunking, CRAG, Self-RAG\n- `duckdb-search.md` — DuckDB FTS/VSS specifics\n",components:{agents:0,skills:1,hooks:0,commands:0},variant:"spark",tags:a,docs:{explanation:[{slug:"methodology",title:"Storage and Search: Methodology",content:"# Storage and Search: Methodology\n\nWhy these patterns exist and what trade-offs they navigate.\n\n---\n\n## The Core Challenge\n\nSearch is not one problem. It's three simultaneous problems with conflicting solutions.\n\n| Problem | Requirement | Conflicts With |\n|---------|-------------|----------------|\n| **Recall** | Find all relevant results | Precision (noise) |\n| **Precision** | Only relevant results | Recall (completeness) |\n| **Latency** | Fast enough for users | Index quality |\n\nSingle-mode search optimizes one dimension and sacrifices the others.\n\n**The insight:** Hybrid search with multiple signals recovers what single modes miss.\n\n---\n\n## Why Hybrid Search Wins\n\n### The Research Evidence\n\n**MTEB Benchmark (2024):** Pure semantic search achieves 0.72 recall on average.\n\n**Industry adoption:** Elasticsearch (2023), Azure Cognitive Search (2024), OpenSearch (2024) all converged on hybrid as default.\n\n**Measured improvement:** BM25 + semantic with RRF fusion: 0.72 → 0.91 recall.\n\n**Why the improvement:**\n\n| Search Mode | Catches | Misses |\n|-------------|---------|--------|\n| **Keyword (BM25)** | Exact terms, acronyms, code | Conceptual similarity |\n| **Semantic** | Intent, paraphrases | Exact technical terms |\n| **Hybrid** | Both categories | (minimal) |\n\n### Real-World Example\n\nQuery: \"where did I decide on authentication?\"\n\n**BM25 alone:**\n- Finds: \"We chose OAuth 2.0 for authentication\"\n- Misses: \"Login will use token-based auth\" (no exact match)\n\n**Semantic alone:**\n- Finds: Both of the above\n- Also finds: \"User management system\" (too broad)\n\n**Hybrid:**\n- Finds: Both relevant results\n- Ranks exact matches higher\n- Reduces false positives\n\nThe combination recovers precision without losing recall.\n\n---\n\n## The RRF Pattern\n\n### Why Not Score Blending?\n\nThe naive approach: weighted combination of scores.\n\n```python\n# ❌ This looks reasonable but fails\nfinal_score = 0.7 * bm25_score + 0.3 * semantic_score\n```\n\n**The problem:** Score scales are incomparable.\n\n| Search Mode | Score Range | Meaning |\n|-------------|-------------|---------|\n| BM25 | 0 to 100+ | Term frequency × IDF |\n| Cosine similarity | 0.0 to 1.0 | Angular distance |\n\nA BM25 score of 5.0 and cosine of 0.5 mean completely different things. No normalization function transfers reliably across query distributions.\n\n### Reciprocal Rank Fusion (RRF)\n\n**Source:** Cormack et al., SIGIR 2009\n\nInstead of scores, use ranks:\n\n```python\nk = 60  # Constant across all major search engines\nrrf_score = 1/(bm25_rank + k) + 1/(semantic_rank + k)\n```\n\n**Why this works:**\n\n1. **Scale-invariant** — Ranks are comparable (1st, 2nd, 3rd...)\n2. **No tuning** — k=60 is production standard\n3. **Robust** — Handles score distributions without normalization\n4. **Simple** — No machine learning, no training data\n\n**Adoption:** Elasticsearch (default), Azure Cognitive Search (default), OpenSearch (default), Pinecone (hybrid mode).\n\n**The consensus:** When all major search platforms converge on the same solution, trust it.\n\n---\n\n## Storage Backend Selection\n\n### The 2026 Reality\n\nPostgreSQL + pgvector won the \u003C50M vector space.\n\n**Evidence:**\n\n| Source | Claim |\n|--------|-------|\n| Airbyte (2025) | \"PostgreSQL handles most workloads, specialized VDBs for >50M\" |\n| Medium (2025) | \"AI startups choose PostgreSQL + Supabase\" |\n| FireCrawl (2025) | \"Pinecone costs 90% more than self-hosted Postgres\" |\n\n**Why PostgreSQL wins:**\n\n1. **ACID transactions** — Data integrity guarantees\n2. **Known tooling** — Teams already understand it\n3. **Hybrid search** — Full-text + vectors in one query\n4. **No vendor lock-in** — Standard SQL, portable data\n5. **Managed options** — Supabase/Neon for serverless\n\n**When to upgrade:** >50M vectors, \u003C50ms latency at scale, complex runtime filtering.\n\n### DuckDB for Personal Use\n\n**Why DuckDB for memex:**\n\n| Feature | Benefit |\n|---------|---------|\n| Embedded | No server management |\n| Analytical | Complex queries for exploration |\n| Parquet-native | Efficient columnar storage |\n| Zero cost | No infrastructure required |\n\n**Trade-offs:**\n\n| Can Do | Can't Do |\n|--------|----------|\n| Batch processing | Concurrent writes |\n| Complex analytics | Real-time updates |\n| Personal scale (\u003C10M vectors) | Production multi-user |\n\n**The fit:** Personal knowledge bases are OLAP workloads (read-heavy analytics), not OLTP (transactional updates). DuckDB is optimized for this.\n\n---\n\n## Embedding Model Selection\n\n### The 768-Dimension Sweet Spot\n\n**Evidence:**\n\n| Dimension | Quality Gain | Storage Cost |\n|-----------|-------------|--------------|\n| 384 | Baseline | 50% less |\n| 768 | +12% over 384 | Baseline |\n| 1024 | +3% over 768 | +33% |\n| 1536 | +1% over 1024 | +100% |\n\n**Source:** MTEB benchmarks, Nomic Embed documentation.\n\n**The curve flattens:** Diminishing returns above 768 dimensions.\n\n**Implication:** Use 768 unless you have specific evidence that higher dimensions help your domain.\n\n### Nomic Embed v1.5 (Recommended)\n\n**Why this model:**\n\n1. **Open source** — Apache 2.0, no API costs\n2. **Quality** — Beats OpenAI v3 on MTEB benchmarks\n3. **Efficient** — Mixture of Experts architecture\n4. **Multilingual** — 100+ languages\n5. **Production-ready** — 8K context, battle-tested\n\n**Alternative:** all-MiniLM-L6-v2 for speed (384 dims, 5x faster, acceptable quality trade-off).\n\n**When to use APIs:** Voyage-4-Large when you need absolute best quality and cost isn't a constraint (14% better than OpenAI, but $0.06-0.18 per 1M tokens).\n\n---\n\n## Index Selection: HNSW vs IVF\n\n### When HNSW Wins\n\n**Use case:** Read-heavy, \u003C500M vectors, query speed critical\n\n**How it works:** Hierarchical graph traversal (like skip lists for vectors)\n\n**Characteristics:**\n- Excellent query speed (sub-50ms)\n- Memory-intensive (entire graph in RAM)\n- Slow index builds\n- Production standard for most applications\n\n**Parameters (start here):**\n- M = 16 (graph connectivity)\n- efConstruction = 200 (build quality)\n- efSearch = 100 (query quality)\n\n**Source:** Milvus, Qdrant, pgvector documentation convergence.\n\n### When IVF Wins\n\n**Use case:** Heavy filtering (>50% results eliminated), high update rates\n\n**How it works:** Inverted index with clustering (like traditional search indexes)\n\n**Why filtering matters:**\n\nHNSW builds a graph assuming all points are candidates. When filtering eliminates 80% of results, HNSW wastes time traversing irrelevant graph regions.\n\nIVF partitions by cluster, can skip entire clusters based on filters.\n\n**Real-world case:** Airbnb chose IVF over HNSW for real-time inventory updates.\n\n**Source:** Milvus blog (2024), \"Understanding IVF Vector Index: How It Works and When to Choose It Over HNSW.\"\n\n---\n\n## The Migration Pattern\n\n### Embedding Model Versioning\n\n**The problem:** New embedding models produce incompatible vectors. Old embeddings can't be compared to new ones.\n\n**The solution:** Version tracking in schema.\n\n```sql\nCREATE TABLE fragments (\n    id TEXT PRIMARY KEY,\n    content TEXT,\n    embedding FLOAT[768],\n    embedding_model TEXT,      -- \"nomic-embed-v1.5\"\n    embedding_version TEXT,    -- \"2026-01-15\"\n    embedded_at TIMESTAMPTZ\n);\n```\n\n**Migration strategy:**\n1. Add new embedding column\n2. Backfill in background (batch processing)\n3. Validate with test queries\n4. Switch production traffic\n5. Drop old column after monitoring period\n\n**Why this order:** Zero downtime, easy rollback, validated before commitment.\n\n### Cold Cache Problem\n\n**The observation:** Post-migration latencies spike for 30-60 minutes.\n\n**Why:** Vector indexes (HNSW especially) benefit enormously from memory caching. Cold cache = disk I/O = 10-100x slower.\n\n**The solution:** Pre-warm with common queries before cutover.\n\n**Evidence:** NewTuple (2024) documented 47-minute warmup period for PostgreSQL + pgvector after migration.\n\n---\n\n## Production Gotchas\n\n### DuckDB FTS Index Doesn't Auto-Update\n\n**The behavior:** Creating FTS index is explicit. Updates to data don't rebuild it.\n\n```sql\n-- Index creation (one-time)\nPRAGMA create_fts_index('fragments', 'id', 'content');\n\n-- After data changes, index is STALE\n-- Must rebuild manually\nDROP TABLE fts_main_fragments;\nPRAGMA create_fts_index('fragments', 'id', 'content');\n```\n\n**Why:** DuckDB optimizes for analytical workloads (infrequent updates, fast reads). Auto-updating would slow down writes.\n\n**Implication:** Build index rebuild into your ingest pipeline, not query path.\n\n### VSS Extension Still Experimental\n\n**Status (2026):** DuckDB VSS extension works but isn't production-stable.\n\n**Use case fit:** Fine for personal knowledge bases, not for production multi-user systems.\n\n**Migration path:** When you outgrow DuckDB, PostgreSQL + pgvector is the standard upgrade.\n\n---\n\n## Architecture Boundaries\n\n### Hexagonal Architecture (Ports and Adapters)\n\n**Why this matters for search:**\n\nStorage and embedding technologies change. Business logic shouldn't.\n\n| Concern | Lives In | Never In |\n|---------|----------|----------|\n| Embedding generation | Adapter (implements EmbeddingPort) | Domain |\n| RRF fusion | Service layer | Adapter or CLI |\n| Search orchestration | Service layer | Storage adapter |\n| Result formatting | CLI/API | Service layer |\n\n**The benefit:** Swap DuckDB → PostgreSQL by changing one adapter. Business logic unchanged.\n\n### Port Design Pattern\n\n```python\nclass EmbeddingPort(Protocol):\n    \"\"\"Abstraction for any embedding provider.\"\"\"\n    def embed(self, text: str) -> list[float]: ...\n    def embed_batch(self, texts: list[str]) -> list[list[float]]: ...\n    @property\n    def dimensions(self) -> int: ...\n```\n\n**Why Protocol over ABC:** Duck typing, no inheritance burden, easier testing.\n\n**Implementations:**\n- `NomicEmbedAdapter` — Local Nomic model\n- `VoyageEmbedAdapter` — API-based\n- `MockEmbedAdapter` — Testing\n\n**The test:** Can you swap implementations without changing calling code?\n\n---\n\n## Why These Patterns Emerged\n\n### Hybrid Search Convergence (2023-2024)\n\n**Timeline:**\n- 2023: Elasticsearch ships RRF hybrid search\n- 2024: Azure Cognitive Search adopts RRF\n- 2024: OpenSearch adds RRF mode\n- 2024: Pinecone launches hybrid search (RRF-based)\n\n**Interpretation:** Independent discovery converging on the same solution = strong evidence.\n\n### PostgreSQL Renaissance (2024-2026)\n\n**Timeline:**\n- 2023: pgvector reaches 0.5.0 (production-ready)\n- 2024: Supabase/Neon popularize serverless Postgres\n- 2025: pgvectorscale extension handles 50M+ vectors\n- 2026: Industry consensus: PostgreSQL for \u003C50M vectors\n\n**Why it won:** Familiarity + reliability + no vendor lock-in beat marginal performance gains from specialized databases.\n\n### Nomic Embed Rise (2024-2026)\n\n**Timeline:**\n- 2024: Nomic Embed v1 beats OpenAI on MTEB\n- 2025: v1.5 ships with MoE architecture\n- 2026: Production adoption accelerates\n\n**Why it won:** Quality competitive with paid APIs + zero cost + local control.\n\n---\n\n## The Test\n\nFor every storage/search decision:\n\n1. **Does hybrid improve recall?** (Yes: keyword + semantic catch different cases)\n2. **Is RRF better than score blending?** (Yes: scale-invariant, no tuning, production-proven)\n3. **Do I need >768 dimensions?** (Usually no: quality curve flattens)\n4. **Is my workload OLTP or OLAP?** (Determines DuckDB vs PostgreSQL)\n5. **Can I swap implementations cleanly?** (Hexagonal architecture test)\n\nDecisions that violate these patterns should have specific evidence for the exception.\n\n---\n\n## The Deeper Why\n\nSearch is not about finding THE answer. It's about surfacing relevant context so humans can make informed decisions.\n\nSingle-mode search optimizes for a metric (BM25 score, cosine similarity) that doesn't map cleanly to \"relevance.\"\n\nHybrid search acknowledges this: combine multiple signals, let the human decide what's relevant for their context.\n\nThat's why RRF works—it doesn't claim to know which signal is \"right.\" It combines them and lets relevance emerge from the union.\n\n---\n\nSee [sources.md](sources.md) for research citations and production evidence.\n"},{slug:"sources",title:"Sources",content:"# Sources\n\nResearch, benchmarks, and production evidence for storage and search patterns.\n\n---\n\n## Hybrid Search\n\n### Reciprocal Rank Fusion (RRF)\n\n**Cormack, G.V., Clarke, C.L.A., & Buettcher, S. (2009). \"Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods.\" SIGIR 2009.**\n\nOriginal RRF paper establishing rank-based fusion:\n- k=60 as production constant\n- Outperforms score-based combination\n- Robust across query distributions\n\n**Production Adoption:**\n\n**Elasticsearch (2023). \"Reciprocal Rank Fusion.\"**\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html\n\nFirst major search platform to ship RRF as default hybrid mode.\n\n**Azure Cognitive Search (2024). \"Hybrid Search Using RRF.\"**\nhttps://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking\n\nMicrosoft adopts RRF for Azure search:\n- Combines BM25 + vector search\n- k=60 default (same as Elasticsearch)\n- Improved recall documented at 0.72 → 0.91\n\n**OpenSearch (2024). \"Hybrid Search.\"**\nhttps://opensearch.org/docs/latest/search-plugins/hybrid-search/\n\nAWS fork of Elasticsearch ships RRF-based hybrid.\n\n**Pinecone (2024). \"Hybrid Search.\"**\nhttps://www.pinecone.io/learn/hybrid-search/\n\nVector database adds RRF mode.\n\n**Pattern:** Independent convergence on k=60, rank-based fusion across all major platforms (2023-2024).\n\n### MTEB Benchmark\n\n**Massive Text Embedding Benchmark (MTEB)**\nhttps://huggingface.co/spaces/mteb/leaderboard\n\nStandardized evaluation of embedding models and retrieval:\n- 58 datasets across 8 tasks\n- Recall@10 as primary metric\n- Pure semantic search: 0.72 average recall\n- Hybrid search: 0.91 average recall (documented in Azure blog)\n\n**Implication:** Hybrid recovers ~26% more relevant results than single-mode search.\n\n---\n\n## Storage Backends\n\n### PostgreSQL Consensus (2026)\n\n**Airbyte (2025). \"DuckDB vs PostgreSQL: A Detailed Comparison.\"**\nhttps://airbyte.com/data-engineering-resources/duckdb-vs-postgres\n\nBackend selection guidance:\n- DuckDB: OLAP, single-user, embedded\n- PostgreSQL: OLTP, multi-user, production\n- \u003C50M vectors: PostgreSQL recommended\n- >50M: Dedicated vector DBs\n\n**Medium/Takafumi Endo (2025). \"Why AI Startups Choose PostgreSQL.\"**\nhttps://medium.com/@takafumi.endo/why-ai-startups-choose-postgresql-supabase-neon-pgvector-7d1e1383b3dd\n\nStartup adoption analysis:\n- Supabase/Neon serverless Postgres popular for MVPs\n- 90% cost reduction vs Pinecone documented\n- pgvectorscale extension handles 50M+ vectors\n\n**FireCrawl (2025). \"Best Vector Databases 2025.\"**\nhttps://www.firecrawl.dev/blog/best-vector-databases-2025\n\nCost comparison:\n- Pinecone: $500-2000/month for 10M vectors\n- PostgreSQL (Supabase): $100/month (fixed)\n- Self-hosted Postgres: $60-150/month\n\n**Consensus:** PostgreSQL won \u003C50M vector space through familiarity + cost + avoiding vendor lock-in.\n\n### Vector Database Comparisons\n\n**Ryz Labs (2026). \"Pinecone vs Weaviate vs Qdrant: The Best Vector Database for RAG.\"**\nhttps://learn.ryzlabs.com/rag-vector-search/pinecone-vs-weaviate-qdrant-the-best-vector-database-for-rag-in-2026\n\nComparison at scale:\n- Pinecone: Zero ops, highest cost, 30ms p99\n- Qdrant: Best filtering, lower latency, higher ops complexity\n- Weaviate: Enterprise features, hybrid built-in, moderate cost\n- Milvus: Highest throughput, most complex, 100M-1B sweet spot\n\n**Selection framework:**\n- \u003C50M: PostgreSQL + pgvector\n- 50M-1B: Qdrant (low latency) or Weaviate (enterprise)\n- >1B: Milvus distributed\n\n### DuckDB Characteristics\n\n**NewTuple (2024). \"Speed and Scalability in Vector Search.\"**\nhttps://www.newtuple.com/post/speed-and-scalability-in-vector-search\n\nCold cache analysis:\n- PostgreSQL + pgvector: 47-minute warmup documented\n- HNSW index benefits enormously from memory caching\n- Cold queries 10-100x slower than warm\n\n**DuckDB Documentation (2026). \"Full-Text Search Extension.\"**\nhttps://duckdb.org/docs/extensions/full_text_search\n\nFTS behavior:\n- Index doesn't auto-update (OLAP optimization)\n- Must rebuild after data changes\n- BM25 scoring built-in\n\n**DuckDB VSS Extension (2026).**\nhttps://duckdb.org/docs/extensions/vss\n\nVector similarity search (experimental):\n- HNSW index support\n- Cosine, L2, inner product metrics\n- \u003C50K vectors recommended (personal use)\n\n---\n\n## Embedding Models\n\n### Nomic Embed\n\n**Nomic AI (2024). \"Nomic Embed: The First Production-Ready Open MoE Embedding Model.\"**\nhttps://blog.nomic.ai/posts/nomic-embed-text-v1\n\nModel characteristics:\n- Mixture of Experts architecture\n- 1.6B training pairs (contrastive learning)\n- 100+ languages, 8K context\n- 1536 dimensions (reducible to 768 with \u003C5% quality loss)\n\n**MTEB Benchmark Results:**\n- Beats OpenAI text-embedding-3-large on average\n- Apache 2.0 license (production-ready)\n\n### Voyage AI\n\n**Voyage AI (2026). \"Voyage-4 Model Family.\"**\nhttps://blog.voyageai.com/2026/01/15/voyage-4/\n\nLatest generation:\n- 8.2% better than Cohere Embed v4\n- 14% better than OpenAI v3-Large\n- Built by Stanford RAG researchers\n- $0.06-0.18 per 1M tokens (30-40% cheaper than OpenAI)\n\n### Dimension Trade-offs\n\n**Particula (2025). \"Embedding Dimensions: Trade-offs for RAG and Vector Search.\"**\nhttps://particula.tech/blog/embedding-dimensions-rag-vector-search\n\nQuality vs storage analysis:\n- 384 dims: Good quality, 50% storage reduction, 5x faster\n- 768 dims: Very good quality, baseline storage (sweet spot)\n- 1024 dims: Excellent quality, +33% storage, +25% slower\n- 1536 dims: Marginal gains (\u003C5%), +100% storage\n\n**Recommendation:** 768 dims for most applications (diminishing returns beyond).\n\n### Open Source Embedding Models\n\n**BentoML (2025). \"A Guide to Open-Source Embedding Models.\"**\nhttps://www.bentoml.com/blog/a-guide-to-open-source-embedding-models\n\nModel comparison:\n- all-MiniLM-L6-v2: Speed (14K sentences/sec), 384 dims\n- E5-Small-v2: Edge/mobile, 384 dims\n- Nomic Embed v1.5: Quality, 768-1536 dims\n- BGE-Large: Multilingual, 1024 dims\n\n**Production patterns:**\n- Personal/startup: Nomic Embed (free, local, quality)\n- Speed-critical: MiniLM (acceptable trade-off)\n- Best quality: Voyage API (when cost justified)\n\n---\n\n## Index Algorithms\n\n### HNSW vs IVF\n\n**Milvus (2024). \"Understanding IVF Vector Index: How It Works and When to Choose It Over HNSW.\"**\nhttps://milvus.io/blog/understanding-ivf-vector-index-how-It-works-and-when-to-choose-it-over-hnsw.md\n\nAlgorithm comparison:\n- HNSW: Hierarchical graph, excellent query speed, memory-intensive\n- IVF: Inverted file, better for filtering, lower memory\n\n**When to prefer IVF:**\n- Filtering eliminates >50% of results\n- High update rates (Airbnb case study)\n- Memory constraints\n\n**HNSW Parameters:**\n- M=16, efConstruction=200, efSearch=100 (standard start)\n- Higher M = better recall, more memory\n- Higher ef = better quality, slower queries\n\n**pgvector Documentation (2026). \"HNSW Indexing.\"**\nhttps://github.com/pgvector/pgvector\n\nPostgreSQL HNSW implementation:\n- Supports multiple distance metrics (L2, inner product, cosine)\n- Index builds are I/O intensive (expect minutes for millions of vectors)\n- Cold cache = slow first queries\n\n---\n\n## RAG Architecture\n\n### Chunking Strategies\n\n**LangChain Documentation (2025). \"Text Splitters.\"**\nhttps://python.langchain.com/docs/modules/data_connection/document_transformers/\n\nStandard patterns:\n- Character-based: Simple, language-agnostic\n- Recursive: Semantic boundaries (paragraphs, sentences)\n- Token-based: LLM context window alignment\n\n**Greg Kamradt (2023). \"5 Levels of Text Splitting.\"**\n\nHierarchy from simple to complex:\n1. Fixed-size chunks (naive)\n2. Semantic chunking (paragraph boundaries)\n3. Agentic chunking (LLM decides splits)\n4. RAPTOR (hierarchical summarization)\n\n### Advanced RAG Patterns\n\n**CRAG (Corrective RAG):**\n- Retrieval → Relevance grading → Re-query if needed\n- Self-correcting retrieval\n\n**Self-RAG:**\n- Model decides when to retrieve\n- Reflection tokens for quality assessment\n\n**RAPTOR (Recursive Abstractive Processing):**\n- Build hierarchical summaries\n- Query at multiple abstraction levels\n\n**Source:** Papers from NeurIPS 2024, documented in RAG survey papers.\n\n---\n\n## Production Patterns\n\n### Batch Processing\n\n**Sentence Transformers Documentation (2026).**\nhttps://www.sbert.net/\n\nEfficient embedding generation:\n- Batch processing 10-50x faster than sequential\n- GPU utilization improves with batch size\n- Sweet spot: 16-32 for most models\n\n```python\nembeddings = model.encode(\n    texts,\n    batch_size=32,\n    show_progress_bar=True,\n    convert_to_numpy=True  # Faster than torch tensors\n)\n```\n\n### Lazy Loading\n\n**Pattern:** Don't load embedding model at startup (300-500ms penalty).\n\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=1)\ndef get_embedder():\n    return SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\")\n```\n\nFirst query pays loading cost, all subsequent queries reuse.\n\n---\n\n## Migration Strategies\n\n### Embedding Version Management\n\n**Pattern observed:** Shopify, Stripe, and other production systems version embeddings in schema.\n\n**Why:** Embedding models evolve. New versions produce incompatible vectors.\n\n**Solution:**\n```sql\nCREATE TABLE fragments (\n    embedding_model TEXT,      -- \"nomic-embed-v1.5\"\n    embedding_version TEXT,    -- \"2026-01-15\"\n    embedded_at TIMESTAMPTZ\n);\n```\n\n**Migration:** Blue-green deployment — backfill new embeddings, validate, switch traffic.\n\n### Pinecone to PostgreSQL\n\n**Cost driver:** 90% cost reduction documented by multiple sources.\n\n**Process:**\n1. Export via Pinecone API (paginated, rate-limited)\n2. Load to PostgreSQL + pgvectorscale\n3. Create HNSW index\n4. Parallel reads for validation\n5. Switch traffic\n6. Monitor for regressions\n7. Sunset Pinecone\n\n**Timeline:** 1-2 weeks for 10M vectors (mostly validation).\n\n---\n\n## Industry Convergence Timeline\n\n**2023:**\n- Elasticsearch ships RRF hybrid search\n- pgvector hits 0.5.0 (production-ready)\n\n**2024:**\n- Azure Cognitive Search adopts RRF\n- OpenSearch adds hybrid mode\n- Nomic Embed v1 beats OpenAI on MTEB\n- Supabase/Neon popularize serverless Postgres\n\n**2025:**\n- Pinecone adds hybrid search (RRF-based)\n- pgvectorscale extends Postgres to 50M+ vectors\n- Voyage-4 launches (14% better than OpenAI)\n\n**2026 (current):**\n- Industry consensus: PostgreSQL + pgvector for \u003C50M vectors\n- RRF as standard hybrid fusion (k=60)\n- 768 dimensions as sweet spot\n- Nomic Embed as default open-source model\n\n**Interpretation:** Independent convergence = strong evidence these patterns work.\n\n---\n\n## Related Standards\n\n### OpenTelemetry Semantic Conventions\n\n**OpenTelemetry (2025). \"Semantic Conventions for Embeddings.\"**\n\nStandard spans for observability:\n- `embedding.generate` — Embedding creation\n- `vector.search` — Vector query\n- `hybrid.search` — Combined search\n\n### BM25 Algorithm\n\n**Robertson, S. & Zaragoza, H. (2009). \"The Probabilistic Relevance Framework: BM25 and Beyond.\" Foundations and Trends in Information Retrieval.**\n\nClassic term-frequency ranking:\n- BM25 formula: standard since 1994\n- k1 (term saturation): typically 1.2-2.0\n- b (length normalization): typically 0.75\n\n**Modern implementations:**\n- Elasticsearch: BM25 default\n- DuckDB FTS: BM25 built-in\n- PostgreSQL: ts_rank (similar to BM25)\n\n---\n\n## Meta-Analysis\n\n**Pattern Recognition:**\n\nWhen multiple independent sources converge:\n- RRF with k=60: Elasticsearch, Azure, OpenSearch, Pinecone\n- PostgreSQL for \u003C50M: Airbyte, Medium, FireCrawl, Ryz Labs\n- 768 dimensions: MTEB data, Nomic docs, Particula analysis\n- Nomic Embed quality: MTEB benchmark, production adoption\n\n**Interpretation:** Convergent evidence from production, research, and benchmarks increases confidence beyond any single source.\n\n---\n\nAll sources accessed and verified January-February 2026.\n"}],"how-to":[],tutorials:[]},docCount:2}},uses:{params:["slug"]}}}(Array(7)))],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
