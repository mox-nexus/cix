<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../_app/immutable/assets/0.04mfEt8Y.css" rel="stylesheet">
		<link href="../_app/immutable/assets/CrossLinks.C4T6atsV.css" rel="stylesheet">
		<link href="../_app/immutable/assets/5.Bnuf7QZ4.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.BuOf6o_y.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/ILWqs1A4.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/iyO_HpW3.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_cEvoN5f.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D0iwhpLH.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/C_MjBE-7.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Cs0QLTHR.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.Ck5Yb-Na.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BPYyHSBz.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/2hgU3PCP.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/C4-dLuTv.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_Dz5Oby6.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DtLL33Bf.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/a8oTfHeT.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CFsY3MpS.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.B80nhfWh.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BwsKPAvP.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BbhyhsYm.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DDW_fuwV.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/5.qDIZb3A1.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BFxfVKZg.js"><!--12qhfyh--><meta name="description" content="Extensions that enhance human capability, not replace it."/><!----><!--wzm642--><meta name="description" content="Build extensions that enable effective human-AI collaboration. Use when: creating skills, agents, hooks, commands, MCPs, or tools/APIs. Patterns for transparency, control, observability, diversity."/><!----><title>craft-extensions — cix</title>
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><a href="#main" class="skip-link">Skip to content</a> <!--[--><nav class="site-nav svelte-qgym72" aria-label="Site navigation"><a href="../" class="nav-wordmark svelte-qgym72">cix</a> <div class="nav-links svelte-qgym72"><!--[--><a href="../ethos" class="nav-link svelte-qgym72">ethos</a><a href="../catalog" class="nav-link svelte-qgym72">catalog</a><a href="../library" class="nav-link svelte-qgym72">library</a><!--]--></div></nav><!--]--> <div class="page svelte-12qhfyh has-nav"><!--[!--><!----><main id="main" class="detail-page svelte-wzm642" style="--variant-color: var(--emergence-core)"><nav class="detail-back svelte-wzm642"><a href="../catalog" class="svelte-wzm642">← catalog</a></nav> <header class="detail-header svelte-wzm642"><div class="header-top svelte-wzm642"><h1 class="svelte-wzm642">craft-extensions</h1> <span class="detail-kind svelte-wzm642">plugin</span> <span class="detail-version svelte-wzm642">0.1.0</span></div> <p class="detail-description svelte-wzm642">Build extensions that enable effective human-AI collaboration. Use when: creating skills, agents, hooks, commands, MCPs, or tools/APIs. Patterns for transparency, control, observability, diversity.</p> <!--[--><div class="detail-inventory svelte-wzm642"><!--[--><span class="inv-item svelte-wzm642">2 agents</span><span class="inv-item svelte-wzm642">2 skills</span><!--]--></div><!--]--> <!--[--><div class="detail-tags svelte-wzm642"><!--[--><span class="tag svelte-wzm642">extensions</span><span class="tag svelte-wzm642">skills</span><span class="tag svelte-wzm642">agents</span><span class="tag svelte-wzm642">hooks</span><span class="tag svelte-wzm642">commands</span><span class="tag svelte-wzm642">mcp</span><span class="tag svelte-wzm642">transparency</span><span class="tag svelte-wzm642">control</span><span class="tag svelte-wzm642">observability</span><span class="tag svelte-wzm642">collaborative-intelligence</span><!--]--></div><!--]--></header> <!--[--><nav class="detail-tabs svelte-wzm642" role="tablist"><!--[--><button role="tab" class="tab svelte-wzm642 active" aria-selected="true">README <!--[!--><!--]--></button><button role="tab" class="tab svelte-wzm642" aria-selected="false">Explanation <!--[--><span class="tab-count svelte-wzm642">6</span><!--]--></button><!--]--></nav><!--]--> <article class="detail-content svelte-wzm642 prose"><!--[--><!----><h1>craft-extensions</h1>
<p>Build extensions that enable effective human-AI collaboration.</p>
<h2>Skills</h2>
<table>
<thead>
<tr>
<th>Skill</th>
<th>Purpose</th>
<th>Use When</th>
</tr>
</thead>
<tbody><tr>
<td><strong>craft-plugins</strong></td>
<td>Claude Code extensions (skills, commands, agents, hooks, MCP)</td>
<td>Creating any Claude Code component</td>
</tr>
<tr>
<td><strong>craft-tools</strong></td>
<td>Software tools — CLIs, APIs, libraries</td>
<td>Creating CLIs, APIs, packages, or improving tool DX</td>
</tr>
</tbody></table>
<h2>Relationship</h2>
<pre><code>craft-tools (general patterns)
       ↓
craft-plugins (Claude Code specifics)
       ↓
plugin-dev (templates &amp; structure)
</code></pre>
<ul>
<li><strong>craft-tools:</strong> Universal patterns (CLI DX, API design, progressive disclosure, packaging)</li>
<li><strong>craft-plugins:</strong> Claude Code specific (skills, agents, hooks, commands, MCP, composition)</li>
<li><strong>plugin-dev:</strong> Templates and directory structure (referenced, not duplicated)</li>
</ul>
<h2>When to Use</h2>
<table>
<thead>
<tr>
<th>Need</th>
<th>Use</th>
</tr>
</thead>
<tbody><tr>
<td>Creating a Claude Code skill</td>
<td>craft-plugins</td>
</tr>
<tr>
<td>Creating a Claude Code agent</td>
<td>craft-plugins</td>
</tr>
<tr>
<td>Creating a hook or command</td>
<td>craft-plugins</td>
</tr>
<tr>
<td>Building an MCP server</td>
<td>craft-plugins</td>
</tr>
<tr>
<td>Building a CLI tool</td>
<td>craft-tools</td>
</tr>
<tr>
<td>Building an API</td>
<td>craft-tools</td>
</tr>
<tr>
<td>Improving CLI developer experience</td>
<td>craft-tools</td>
</tr>
<tr>
<td>Plugin directory structure</td>
<td>plugin-dev</td>
</tr>
</tbody></table>
<h2>Agents</h2>
<table>
<thead>
<tr>
<th>Agent</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td>evaluator</td>
<td>Quality validation for extensions</td>
</tr>
<tr>
<td>optimizer</td>
<td>Fix identified issues</td>
</tr>
</tbody></table>
<h2>Structure</h2>
<pre><code>craft-extensions/
├── .claude-plugin/plugin.json
├── docs/
│   └── explanation/              # Human-optimized (WHY)
│       ├── methodology.md        # Research base &amp; design rationale
│       ├── sources.md            # Full bibliography
│       ├── observability.md      # Why observability matters
│       ├── eval-first-design.md  # Eval-first philosophy
│       ├── evaluator.md          # Evaluator design rationale
│       └── optimizer.md          # Optimizer design rationale
├── agents/
│   ├── evaluator.md              # Quality validation (7 gates)
│   └── optimizer.md              # Targeted fix patterns
├── skills/
│   ├── craft-plugins/            # Claude Code extensions
│   │   ├── SKILL.md
│   │   └── references/
│   │       ├── observability.md  # OTel instrumentation
│   │       └── evidence-workflow.md  # Research methodology
│   └── craft-tools/              # Software tools (CLIs, APIs, libraries)
│       ├── SKILL.md
│       └── references/
│           └── cli-dx.md         # CLI DX patterns (six laws)
└── README.md
</code></pre>
<h2>License</h2>
<p>MIT</p>
<!----><!--]--></article></main><!----><!--]--><!----></div> <div class="experimental-tag svelte-12qhfyh" aria-hidden="true">experimental</div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_1xqp067 = {
						base: new URL("..", location).pathname.slice(0, -1),
						assets: "/cix/pr-preview/pr-25"
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.BuOf6o_y.js"),
						import("../_app/immutable/entry/app.Ck5Yb-Na.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 5],
							data: [null,(function(a){a[0]="extensions";a[1]="skills";a[2]="agents";a[3]="hooks";a[4]="commands";a[5]="mcp";a[6]="transparency";a[7]="control";a[8]="observability";a[9]="collaborative-intelligence";return {type:"data",data:{extension:{slug:"craft-extensions",kind:"plugin",manifest:{name:"craft-extensions",version:"0.1.0",description:"Build extensions that enable effective human-AI collaboration. Use when: creating skills, agents, hooks, commands, MCPs, or tools/APIs. Patterns for transparency, control, observability, diversity.",author:{name:"Mox Labs",email:"mox.rnd@gmail.com"},license:"MIT",keywords:a},tagline:"Build extensions that enable effective human-AI collaboration.",readme:"# craft-extensions\n\nBuild extensions that enable effective human-AI collaboration.\n\n## Skills\n\n| Skill | Purpose | Use When |\n|-------|---------|----------|\n| **craft-plugins** | Claude Code extensions (skills, commands, agents, hooks, MCP) | Creating any Claude Code component |\n| **craft-tools** | Software tools — CLIs, APIs, libraries | Creating CLIs, APIs, packages, or improving tool DX |\n\n## Relationship\n\n```\ncraft-tools (general patterns)\n       ↓\ncraft-plugins (Claude Code specifics)\n       ↓\nplugin-dev (templates & structure)\n```\n\n- **craft-tools:** Universal patterns (CLI DX, API design, progressive disclosure, packaging)\n- **craft-plugins:** Claude Code specific (skills, agents, hooks, commands, MCP, composition)\n- **plugin-dev:** Templates and directory structure (referenced, not duplicated)\n\n## When to Use\n\n| Need | Use |\n|------|-----|\n| Creating a Claude Code skill | craft-plugins |\n| Creating a Claude Code agent | craft-plugins |\n| Creating a hook or command | craft-plugins |\n| Building an MCP server | craft-plugins |\n| Building a CLI tool | craft-tools |\n| Building an API | craft-tools |\n| Improving CLI developer experience | craft-tools |\n| Plugin directory structure | plugin-dev |\n\n## Agents\n\n| Agent | Purpose |\n|-------|---------|\n| evaluator | Quality validation for extensions |\n| optimizer | Fix identified issues |\n\n## Structure\n\n```\ncraft-extensions/\n├── .claude-plugin/plugin.json\n├── docs/\n│   └── explanation/              # Human-optimized (WHY)\n│       ├── methodology.md        # Research base & design rationale\n│       ├── sources.md            # Full bibliography\n│       ├── observability.md      # Why observability matters\n│       ├── eval-first-design.md  # Eval-first philosophy\n│       ├── evaluator.md          # Evaluator design rationale\n│       └── optimizer.md          # Optimizer design rationale\n├── agents/\n│   ├── evaluator.md              # Quality validation (7 gates)\n│   └── optimizer.md              # Targeted fix patterns\n├── skills/\n│   ├── craft-plugins/            # Claude Code extensions\n│   │   ├── SKILL.md\n│   │   └── references/\n│   │       ├── observability.md  # OTel instrumentation\n│   │       └── evidence-workflow.md  # Research methodology\n│   └── craft-tools/              # Software tools (CLIs, APIs, libraries)\n│       ├── SKILL.md\n│       └── references/\n│           └── cli-dx.md         # CLI DX patterns (six laws)\n└── README.md\n```\n\n## License\n\nMIT\n",components:{agents:2,skills:2,hooks:0,commands:0},variant:"emergence",tags:a,docs:{explanation:[{slug:"eval-first-design",title:"Eval-First Design",content:"# Eval-First Design\n\nWhy measurement comes before shipping, not after.\n\n---\n\n## Contents\n\n- [The Problem](#the-problem)\n- [Eval-First Philosophy](#eval-first-philosophy)\n- [Eval-First in Practice](#eval-first-in-practice)\n- [What to Evaluate](#what-to-evaluate)\n- [The Dual Failure Modes](#the-dual-failure-modes)\n- [Why Single Metrics Lie](#why-single-metrics-lie)\n- [The TDD Parallel](#the-tdd-parallel)\n- [Connection to craft-evals](#connection-to-craft-evals)\n\n---\n\n## The Problem\n\nExtensions are invisible. A broken button is obvious. A skill that silently ignores 80% of the prompts it should handle — that's invisible failure. Unlike a UI component (where you can see if it's broken), an extension's failure modes are subtle:\n\n- A skill with vague triggers activates on the wrong prompts — or never activates at all\n- An agent that sounds confident but gives generic advice passes casual review\n- A hook that blocks too aggressively gets disabled rather than fixed\n\nWithout measurement, you discover these problems through user frustration, not systematic detection.\n\n---\n\n## Eval-First Philosophy\n\n**Measure before you ship, not after.**\n\nThe principle: define what \"good\" looks like before building, then verify the extension meets that bar. This is the same discipline as test-driven development, applied to AI extensions.\n\n| Without Evals | With Evals |\n|---------------|------------|\n| \"Seems good\" | Activation F1 = 0.85 |\n| \"Users like it\" | Methodology adherence = 78% |\n| \"It works for me\" | Content efficiency: 0 lines of tutorial |\n\nThe eval-first approach catches problems during development when they're cheap to fix, not in production when they've already eroded user trust.\n\n---\n\n## Eval-First in Practice\n\nHere's the cycle applied to a real skill:\n\n**1. Define \"good\"** — The `data-store` skill should activate when users ask about databases, search, embeddings, or RAG. It should NOT activate for general coding questions, deployment, or authentication.\n\n**2. Measure** — Run activation eval against 20 test prompts (10 should-activate, 10 should-not):\n\n```\nResults:\n  True Positives:  8/10  (activated correctly)\n  False Positives: 3/10  (activated incorrectly)\n  Precision: 0.73  |  Recall: 0.80  |  F1: 0.76\n```\n\n**3. Diagnose** — Precision is low (0.73): the skill fires on \"How do I store user sessions?\" (authentication, not data-store). Recall gap: misses \"What embedding model should I use?\" (no \"database\" keyword in prompt).\n\n**4. Fix** — Update the skill description:\n- Add: \"embedding models, vector search, RAG pipelines\"\n- Add exclusion: \"Not for: authentication, session management, caching strategies\"\n\n**5. Re-measure** — F1 improves from 0.76 to 0.88. Ship it.\n\nWithout eval-first, the precision problem (firing on auth questions) would surface as user complaints weeks later. The recall problem (missing embedding queries) might never surface — users just wouldn't know the capability existed.\n\n---\n\n## What to Evaluate\n\n### Activation Quality\n\nDoes the extension activate on the right prompts and stay quiet on the wrong ones?\n\n| Metric | What It Measures |\n|--------|-----------------|\n| **Precision** | Of activations, how many were correct? (Signal vs noise) |\n| **Recall** | Of relevant situations, how many activated? (Coverage) |\n| **F1** | Harmonic mean of precision and recall |\n\n### Methodology Adherence\n\nDoes the extension follow its own stated methodology?\n\n- LLM-as-judge evaluation against skill principles\n- Spot-check outputs for transparency, control, reasoning quality\n- Verify sources are cited, alternatives shown, uncertainty acknowledged\n\n### Content Efficiency\n\nDoes every line justify its token cost?\n\n- Zero tutorial content (Claude already knows basics)\n- Decisions with recommendations, not option lists\n- References that change behavior, not explain concepts\n\n---\n\n## The Dual Failure Modes\n\nExtensions fail in two opposite directions:\n\n### Over-Activation (noise)\n\nThe extension fires on prompts it shouldn't handle. Users see irrelevant guidance, learn to ignore the extension, and eventually disable it.\n\n**Signal:** High recall, low precision. \"It activates on everything.\"\n\n### Under-Activation (missed value)\n\nThe extension stays silent when it should help. Users don't know the capability exists. The extension provides zero value for the use cases it was designed for.\n\n**Signal:** High precision, low recall. \"It never activates.\"\n\nBoth failures are invisible without measurement. A skill with 95% precision but 20% recall looks great when it fires — but misses 80% of the situations where it should help.\n\n---\n\n## Why Single Metrics Lie\n\nOptimizing for one metric hides problems in the other:\n\n| Optimize For | What Happens | What You Miss |\n|-------------|--------------|---------------|\n| Precision only | Narrow triggers, few false positives | Misses most relevant cases |\n| Recall only | Broad triggers, catches everything | Floods user with noise |\n| Accuracy only | Looks great on balanced data | Fails on real-world distributions |\n\nYou need **both** precision and recall (or their composite, F1) to understand activation quality. The same principle applies across all eval dimensions — a single number rarely tells the full story.\n\n---\n\n## The TDD Parallel\n\nIf you know test-driven development, eval-first follows the same discipline:\n\n| TDD | Eval-First |\n|-----|------------|\n| Red: Write a failing test | Define: Write test prompts that should/shouldn't activate |\n| Green: Make it pass | Measure: Run eval, see the score |\n| Refactor: Improve without breaking | Fix: Tighten triggers, re-measure |\n\nThe difference: TDD tests binary correctness (pass/fail). Eval-first tests dimensional quality (precision, recall, methodology adherence). Both catch problems before users do.\n\n---\n\n## Connection to craft-evals\n\nThis document covers the philosophy. For operational eval frameworks — choosing tools, building test suites, measuring agent effectiveness — see the **craft-evals** plugin.\n\n| Need | Where |\n|------|-------|\n| Why eval-first matters | This document |\n| How to build evals | `craft-evals` plugin |\n| Quality gates for extensions | `craft-extensions` evaluator agent |\n\nThe evaluator agent in `craft-extensions` checks 7 quality gates (content, transparency, control, observability, activation, expert value, content efficiency). The `craft-evals` plugin provides the broader framework for building automated test suites.\n\n---\n\nSee [sources.md](sources.md) for full bibliography.\n"},{slug:"evaluator",title:"Why the Evaluator Works This Way",content:"# Why the Evaluator Works This Way\n\nThe evaluator agent checks extensions against quality gates. This document explains why those specific gates exist.\n\n---\n\n## The Six Gates\n\n### Gate 1: Content Quality\n\n**What it checks:** Does this fill gaps Claude doesn't already know?\n\n**Why it matters:** Claude is already very smart. Teaching basics wastes tokens and creates noise. The value is in non-obvious insights, gotchas, and decision frameworks.\n\n**Research backing:** Anthropic's skill authoring guidelines emphasize that context window is a public good. Every token must justify its cost.\n\n### Gate 2: Transparency\n\n**What it checks:** Is reasoning visible? Are sources cited?\n\n**Why it matters:** Transparency (β = 0.415) is the second-strongest predictor of positive AI collaboration outcomes. Without visible reasoning, users can't evaluate, learn, or calibrate trust.\n\n**Research backing:** Blaurock et al. (2025, n=654). Also: Bansal et al. showed explanations alone increase acceptance regardless of correctness—so transparency must be genuine, not theater.\n\n### Gate 3: Control\n\n**What it checks:** Does the extension preserve user agency?\n\n**Why it matters:** Control (β = 0.507) is the strongest predictor. Rigid mandates remove agency. Decision frameworks preserve it by showing HOW to decide, not WHAT to decide.\n\n**Research backing:** Blaurock meta-analysis. Also: Engagement features (b = -0.555) backfire—paternalism hurts frequent users.\n\n### Gate 4: Observability\n\n**What it checks:** Can behavior be traced? Are there logs, spans, exit codes?\n\n**Why it matters:** Without observability, you can't debug, improve, or verify. Silent systems prevent learning and intervention.\n\n**Research backing:** Standard software engineering practice. For AI specifically: observable agents support team situational awareness.\n\n### Gate 5: Activation\n\n**What it checks:** Does the extension trigger on the right prompts?\n\n**Why it matters:** Over-activation wastes context. Under-activation means the extension never helps. \"Use when:\" patterns ensure precise triggering.\n\n**Research backing:** Anthropic's skill authoring guidelines on description specificity.\n\n### Gate 6: Expert Value\n\n**What it checks:** Would someone who knows this domain find it useful?\n\n**Why it matters:** Extensions should make humans MORE capable, not dependent. If an expert finds no value, the extension is teaching basics Claude already knows—creating dependency without capability.\n\n**Research backing:**\n- Mastery orientation preserves capability (ACU research)\n- Skill formation gap (Anthropic 2026): wrong patterns create 17pp learning gap\n- Creative scar (Zhou 2025): users don't truly acquire ability\n\n---\n\n## Scoring Philosophy\n\n| Score | Meaning |\n|-------|---------|\n| 3 | Exceeds—would recommend as exemplar |\n| 2 | Meets—does the job |\n| 1 | Partial—has issues but salvageable |\n| 0 | Fails—fundamental problems |\n\n**Minimum passing:** 2 on all gates (12/18 total)\n\nWhy this threshold? Extensions below this level either:\n- Teach basics (wasting tokens)\n- Hide reasoning (preventing learning)\n- Remove control (creating dependency)\n- Fail to activate correctly (providing no value)\n\n---\n\n## Anti-Pattern Detection\n\nThe evaluator flags:\n- **LLM tell-tales:** \"delve\", \"leverage\", \"robust\" → signal low-effort content\n- **Options without picks:** \"You could use A, B, or C\" → no decision guidance\n- **Tutorial content:** Installation instructions, basic syntax → Claude knows this\n- **Vague activation:** \"Helps with code\" → fires on everything or nothing\n\nThese patterns indicate substitutive rather than complementary design.\n\n---\n\n## Honest Over Nice\n\nThe evaluator is deliberately ruthless because flattery wastes everyone's time.\n\nAn extension that passes on false praise:\n- Wastes user context on low-value content\n- Creates dependency through substitutive patterns\n- Fails to improve through honest feedback\n\nAn extension that fails honestly:\n- Gets specific feedback for improvement\n- Has a clear path to quality\n- Becomes genuinely useful after fixes\n\nThe goal is effective extensions, not feel-good evaluations.\n"},{slug:"methodology",title:"Building Extensions: Methodology",content:"# Building Extensions: Methodology\n\nWhy these patterns exist and the research behind them.\n\n---\n\n## Contents\n\n- [The Core Problem](#the-core-problem)\n- [Mastery vs Performance Orientation](#mastery-vs-performance-orientation)\n- [The Review Pattern](#the-review-pattern)\n- [Why These Principles](#why-these-principles)\n- [Collaborative Intelligence Design Patterns: The Why](#collaborative-intelligence-design-patterns-the-why)\n- [Effect Sizes](#effect-sizes)\n- [The Test](#the-test)\n- [The Three Modes](#the-three-modes-complementary-constitutive-substitutive)\n- [Study Limitations](#study-limitations)\n- [The Deeper Why](#the-deeper-why)\n\n---\n\n## The Core Problem\n\nAI creates a dissociation: **system performance goes up while human competence goes down**.\n\n| What Improves | What Degrades |\n|---------------|---------------|\n| Output speed | Critical thinking (r = -0.75) |\n| Task completion | Procedural knowledge |\n| Artifact quality | Debugging intuition |\n\nThis isn't speculation. Multiple peer-reviewed studies from CHI, PNAS, and The Lancet document this pattern across domains—from legal reasoning to medical procedures to creative writing.\n\nThe implication: if we build AI tools the naive way (maximize immediate output), we create tools that make humans progressively less capable of working without them. That's dependency, not augmentation.\n\n---\n\n## Mastery vs Performance Orientation\n\nResearch on goal orientation shows two distinct patterns:\n\n**Mastery orientation**: User treats AI as collaborator, questions output, maintains critical evaluation. These users maintain capability over time.\n\n**Performance orientation**: User treats AI as oracle, accepts output, focuses on task completion. These users show skill degradation.\n\nThe extension's design determines which orientation it encourages. Speed-optimized interfaces push users toward performance orientation.\n\n---\n\n## The Review Pattern\n\nClaude generates. The human reviews. The critical variable is whether the human **can** evaluate.\n\n| Pattern | Effect |\n|---------|--------|\n| Output only | Human rubber-stamps (no basis for evaluation) |\n| Output + reasoning | Human can evaluate logic |\n| Output + alternatives + tradeoffs | Human makes informed judgment |\n\nExtensions must provide the information needed for genuine evaluation, not just acceptance.\n\n---\n\n## Why These Principles\n\n### Transparency (β = 0.415)\n\nWhen users can see reasoning, they can:\n- Evaluate whether the reasoning is sound\n- Learn the underlying framework\n- Calibrate trust appropriately\n- Catch errors before they propagate\n\nThis isn't \"explainability theater\"—it's genuine visibility into the decision process.\n\n### Control (β = 0.507, strongest)\n\nControl doesn't mean approval bottlenecks. It means:\n- Users can observe what's happening\n- Users can steer the direction\n- Users can override when needed\n- Users retain agency throughout\n\nPerceived control correlates more strongly with positive outcomes than any other feature.\n\n### Engagement Features (b = -0.555, negative)\n\nPaternalistic \"helpfulness\" backfires. Blaurock found engagement features have **negative effects** for frequent users.\n\nThis is why hooks should suggest, not block:\n- Blocking removes agency (hurts control)\n- Paternalistic engagement annoys frequent users\n- Provide value (suggestion) without removing choice (blocking)\n\n### Collaborative Learning\n\nUsers with mastery orientation maintain critical thinking; those with performance orientation show degradation.\n\nTool design should encourage mastery:\n- Explain WHY, not just WHAT\n- Invite verification\n- Build mental models, not dependency\n- Make users more capable, not the tool more capable\n\n### Provenance\n\nWithout provenance:\n- Errors compound invisibly\n- Users can't verify or learn\n- Trust becomes binary (accept/reject)\n- Hallucinations propagate\n\nChain-of-Verification (CoVE) reduces hallucination by 50-70% through independent verification.\n\n### Non-Conformity (g = -0.863 diversity loss)\n\n**The Artificial Hivemind problem** (NeurIPS 2025 Best Paper): 70+ LLMs converge on the same outputs. When 25 models write \"a metaphor about time,\" only 2 clusters emerge. Temperature and ensembling don't help—RLHF punishes diversity.\n\n**Meta-analysis** (28 studies, n=8,214): Diversity reduction effect size is **g = -0.863**. Individual performance goes up, collective diversity crashes.\n\nWhy this matters for extensions:\n- If everyone uses the same AI the same way, outputs homogenize\n- Hong & Page: diverse groups outperform best-ability groups\n- Homogenization destroys the collective intelligence that makes teams effective\n\nExtension design should preserve diversity:\n- Encourage multiple perspectives (orthogonal agents)\n- Support questioning and dissent\n- Use diverse personas/framings where possible\n- Don't optimize for consensus—optimize for useful disagreement\n\n**Mitigation evidence**: Wan & Kalman (2025) showed diverse AI personas eliminate homogenization. Design CAN preserve diversity.\n\n### The Perception Paradox (Anthropic, Jan 2026)\n\nSharma et al. analyzed ~1.5M Claude.ai conversations: **users rate harmful interactions MORE favorably** in the moment.\n\nBut when users acted on AI outputs, satisfaction dropped below baseline. Users expressed regret.\n\nImplication: Users can't reliably self-correct. Short-term satisfaction ≠ long-term benefit. Design must compensate—transparency and control aren't optional, they're necessary because the feedback loop is broken.\n\n---\n\n## Collaborative Intelligence Design Patterns: The Why\n\n### 1. Cognitive Friction\n\n**Pattern**: Add friction at decision points (e.g., `confirm --reason \"...\"`)\n\n**Why it works**: Frictionless interfaces promote cognitive offloading. Requiring articulation maintains engagement. The friction forces the user to think before accepting.\n\n**When to use**: Irreversible actions, high-stakes decisions, learning contexts.\n\n### 2. Contrastive Explanations\n\n**Pattern**: \"X instead of Y because Z\" rather than \"Use X\"\n\n**Why it works**: Contrastive framing triggers analytic processing rather than heuristic acceptance. The brain naturally evaluates tradeoffs when alternatives are visible.\n\n### 3. Glass Box\n\n**Pattern**: Show reasoning traces, make logic inspectable\n\n**Why it works**: Opaque systems prevent learning. Visibility enables:\n- Verification of logic\n- Learning from the process\n- Modification and improvement\n\n### 4. Cognitive Velcro\n\n**Pattern**: Natural language confidence (\"I am guessing...\" vs \"I have verified...\")\n\n**Why it works**: Smooth interfaces provide nothing for the mind to grip. Texture from uncertainty markers, alternatives considered, and explicit caveats creates mental hooks for evaluation.\n\n### 5. Observable Agents\n\n**Pattern**: Work aloud—emit observations, conclusions, uncertainty\n\n**Why it works**: Silent agents break team situational awareness. Humans need to know what's happening to maintain context and intervene when needed.\n\n### 6. Reflection Prompts\n\n**Pattern**: \"What is the primary risk of this operation?\"\n\n**Why it works**: Metacognitive pauses force users to engage before proceeding. If they can't answer, they're not ready—and the system knows to provide more context.\n\n### 7. Socratic Mode\n\n**Pattern**: Guiding questions rather than direct answers\n\n**Why it works**: For learning contexts, building mental models through guided discovery creates more durable understanding than receiving answers.\n\n---\n\n## Effect Sizes\n\n### Design Levers (Blaurock et al. 2025, n=654)\n\n| Lever | Effect Size | Implication |\n|-------|------------|-------------|\n| Control (user agency) | β = 0.507 | Strongest design lever |\n| Transparency | β = 0.415 | Second strongest |\n| Engagement features | b = -0.555 | Paternalistic \"help\" backfires |\n\n### Harm Evidence (Multiple Sources)\n\n| Finding | Effect Size | Source |\n|---------|------------|--------|\n| Diversity reduction | g = -0.863 | Meta-analysis (28 studies) |\n| Skill formation gap | d = 0.738 (17pp) | Anthropic RCT (2026) |\n| AI ↔ critical thinking | r = -0.68 | Gerlich (2025) |\n| Offloading ↔ CT | r = -0.75 | Gerlich (2025) |\n| Learning harm (no guardrails) | -17% | Bastani PNAS (2025) |\n| Skill degradation | -20% | Budzyń, Lancet (2025) |\n\n---\n\n## The Test\n\nFor every extension feature:\n\n1. **Does this make users more capable or more dependent?**\n2. **Does this preserve the generative step?** (User does the thinking)\n3. **Does this encourage mastery or performance orientation?**\n4. **Would users be helpless after extended use?**\n5. **Does this preserve diversity or push toward homogeneity?**\n\nExtensions that fail these questions may boost short-term productivity while eroding long-term capability and collective intelligence.\n\n---\n\n## The Three Modes: Complementary, Constitutive, Substitutive\n\nThe fundamental design choice:\n\n| Mode | Definition | Outcome |\n|------|------------|---------|\n| **Complementary** | AI amplifies existing human capability | User more capable after use |\n| **Constitutive** | Enables capability neither could achieve alone | Novel collaboration emerges |\n| **Substitutive** | AI replaces human thinking | Dependency, skill atrophy |\n\n### Complementary\n\nAI as amplifier. The human retains the generative step—doing the thinking—while AI provides leverage.\n\n- \"Here's how to think about this\" (not \"here's the answer\")\n- Human capability compounds over time\n- User could work without the tool (but chooses not to)\n\n### Constitutive\n\nAI enables genuinely new capability through collaboration. Neither human nor AI could do it alone.\n\n- Human + AI together can do what neither could separately\n- The collaboration itself is the capability\n- Examples: real-time code review during pair programming, exploratory search through massive corpora\n\n### Substitutive (Avoid)\n\nAI does the work, human approves. This is the pattern that causes skill atrophy.\n\n- \"Here's the answer\" (human just accepts)\n- Human capability degrades over time\n- User becomes helpless without the tool\n\n**Every extension should be complementary or constitutive. Never substitutive.**\n\n---\n\n## The Deeper Why\n\nAn AI tool that makes humans dependent has failed, regardless of how much immediate productivity it provides.\n\nThe goal isn't to build better tools. The goal is to build tools that make better humans—more capable, more knowledgeable, more autonomous.\n\nThat's what Collaborative Intelligence means.\n\n---\n\n## Study Limitations\n\nFor collaborators enhancing these extensions: the research provides directional guidance, not universal laws. Each study has specific limitations that matter when applying findings.\n\n| Study | Limitation | What It Means |\n|-------|-----------|---------------|\n| Budzyń (Lancet, 2025) | Medical domain (colonoscopy), observational | Skill atrophy rate (-20%) may not transfer directly to software |\n| Gerlich (2025) | Cross-sectional, self-reported | Correlations (r = -0.68, -0.75) show association, not causation |\n| Lee et al. (CHI, 2025) | Self-reported critical thinking | Perceptions may differ from actual cognitive performance |\n| Blaurock (n=654) | Service contexts (not all software), scenario experiments | Effect sizes (beta = 0.507, 0.415) are directional for extension design |\n| Bastani (PNAS, 2025) | Education domain (Turkish math) | -17% learning harm may vary across skill types and populations |\n| Mastery OR = 35.7 | Single study | Large effect size from one study needs replication |\n| Sharma (Anthropic, 2026) | Claude.ai conversations only | Perception paradox may differ across AI systems |\n\n### What This Means\n\nThe direction is consistent across studies: transparency and control help, cognitive offloading harms learning, observable processes support better collaboration. But the specific magnitudes (beta = 0.507, r = -0.75, g = -0.863) should be treated as approximate, not precise.\n\nWhen enhancing extensions, use these findings to inform design decisions — but don't treat them as engineering constants. The research tells us WHAT to optimize for (control, transparency, mastery orientation). The exact numbers tell us roughly HOW MUCH each lever matters relative to others.\n\n---\n\nSee [sources.md](sources.md) for full bibliography.\n"},{slug:"observability",title:"Observability for Extensions",content:"# Observability for Extensions\n\nWhy observability matters and how it connects to extension design.\n\n---\n\n## Contents\n\n- [The Broken Feedback Loop](#the-broken-feedback-loop)\n- [The Glass Box Pattern](#the-glass-box-pattern)\n- [Hook Design Philosophy](#hook-design-philosophy)\n- [Connection to Effect Sizes](#connection-to-effect-sizes)\n- [What Observable Means](#what-observable-means)\n\n---\n\n## The Broken Feedback Loop\n\nSharma et al. (Anthropic, Jan 2026) analyzed ~1.5M Claude.ai conversations and found users rate harmful interactions MORE favorably in the moment. Satisfaction dropped below baseline only when users acted on outputs and saw consequences.\n\nThe implication: **users can't reliably self-correct.** Short-term satisfaction is not a reliable signal.\n\nWithout observability, extensions operate as black boxes. Nobody — not the user, not the team, not the extension author — can tell whether the extension is helping or harming. The feedback loop that should catch problems (use → observe → adjust) never fires.\n\nObservability restores that loop by making extension behavior visible and measurable.\n\n---\n\n## The Glass Box Pattern\n\nOpaque systems prevent learning. The Glass Box pattern makes the process inspectable:\n\n| Property | What It Means |\n|----------|---------------|\n| **Readable** | Plaintext reasoning, no hidden state |\n| **Verifiable** | Claims trace to sources |\n| **Observable** | See what the tool does, not just what it produces |\n| **Forkable** | Copy, modify, make your own |\n\nFor extensions, this means:\n\n- **Agents** emit observations, conclusions, and uncertainty as they work (\"work aloud\")\n- **Hooks** log what they detected and what they suggested (structured, not silent)\n- **Skills** produce traceable reasoning chains, not just outputs\n- **MCP servers** expose call traces and error context\n\nSilent agents break team situational awareness. When humans can't see what's happening, they lose context and can't intervene at the right moment.\n\n---\n\n## Hook Design Philosophy\n\nHooks have two distinct patterns, and the distinction matters:\n\n### Validation Hooks (suggest, don't block)\n\nMost hooks should preserve agency:\n\n```json\n{\"decision\": \"allow\", \"message\": \"Consider X instead of Y. Proceeding with Y.\"}\n```\n\n**Why:** Blocking removes agency, which hurts the control lever (the strongest design factor at beta = 0.507). Paternalistic engagement features show negative effects for frequent users (b = -0.555, Blaurock meta-analysis). Suggestions provide value without removing choice.\n\n### Action-Triggering Hooks (directive by design)\n\nSome hooks exist specifically to interrupt patterns:\n\n```json\n{\"decision\": \"allow\", \"message\": \"PATTERN DETECTED. You MUST now: 1) [action] 2) [action]\"}\n```\n\n**Why:** The pattern itself is the problem being solved. When a developer is going in circles or about to retry a failed approach, the whole point of the hook is to interrupt and redirect. Gentle suggestion would be ignored — the user is already in a cognitive loop.\n\n**The key distinction:** Validation hooks protect agency over individual actions. Action-triggering hooks protect the user from metacognitive blind spots. Both serve the user, but through different mechanisms.\n\n---\n\n## Connection to Effect Sizes\n\nObservability connects directly to the two strongest design levers from Blaurock et al. (2025, n=654):\n\n| Lever | Effect Size | How Observability Helps |\n|-------|------------|------------------------|\n| **Control** | beta = 0.507 | Users can observe, steer, and override when they can see what's happening |\n| **Transparency** | beta = 0.415 | Visible reasoning enables genuine evaluation, not rubber-stamping |\n\nWithout observability, both levers are disabled. The user approves outputs they can't evaluate, trust becomes binary (accept/reject), and the collaboration degrades to substitution.\n\nObservable extensions support **calibrated trust** — users know when to rely on the extension and when to verify, because they can see the reasoning that produced the output.\n\n---\n\n## What Observable Means\n\nFor extension authors building new extensions:\n\n| Extension Type | Observable When |\n|----------------|----------------|\n| **Agent** | Emits reasoning traces, logs tool calls, reports token usage |\n| **MCP Server** | Traces calls with context, exposes errors with fix guidance |\n| **Hook** | Logs trigger conditions, records suggestions made |\n| **Skill** | Produces verifiable reasoning chains (not just answers) |\n\nThe technical implementation uses OpenTelemetry (see `references/observability.md` for the Claude-optimized how-to). But the philosophy matters more than the tooling: **make the process inspectable so humans can learn, verify, and improve.**\n\n---\n\nSee [sources.md](sources.md) for full bibliography.\n"},{slug:"optimizer",title:"Why the Optimizer Works This Way",content:"# Why the Optimizer Works This Way\n\nThe optimizer agent fixes issues identified by the evaluator. This document explains the fix strategies and why they work.\n\n---\n\n## Fix Philosophy\n\n### Surgical, Not Sweeping\n\nThe optimizer makes **minimal targeted changes**. Why?\n\n1. **Preserve intent:** Large rewrites lose the author's original purpose\n2. **Verify incrementally:** Small changes can be verified; large changes can't\n3. **Reduce risk:** Minimal changes have minimal blast radius\n\n### Priority Order\n\n```\nCritical → Major → Minor\n```\n\nWhy this order?\n- Critical issues block functionality or violate core principles\n- Major issues reduce effectiveness but don't block\n- Minor issues are polish—important but not urgent\n\nFixing in order prevents wasted effort on polish when foundations are broken.\n\n---\n\n## Anti-Pattern Fixes\n\n### LLM Tell-Tales\n\n| Avoid | Use Instead |\n|-------|-------------|\n| delve | explore, examine |\n| leverage | use |\n| robust | reliable, solid |\n| comprehensive | complete, full |\n| utilize | use |\n| facilitate | help, enable |\n\n**Why these matter:** Tell-tales signal low-effort AI-generated content. They don't add meaning and waste tokens. Plain language is clearer and more trustworthy.\n\n### Options Without Picks\n\n**Before:**\n```markdown\nYou could use A, B, or C.\n```\n\n**After:**\n```markdown\n| Situation | Use | Why |\n|-----------|-----|-----|\n| Default | A | Fastest, best maintained |\n| Legacy | B | Works with Node 14 |\n```\n\n**Why this matters:** Options without picks create decision paralysis. Extensions should provide decision frameworks—HOW to choose—not just catalogs. This preserves user agency (Control β = 0.507) while providing value.\n\n### Missing Sources\n\n**Before:**\n```markdown\nThis approach is 10x faster.\n```\n\n**After:**\n```markdown\nThis approach is 10x faster (Source: [Benchmark](url)).\n```\n\n**Why this matters:** Unsourced claims can't be verified or trusted. Transparency (β = 0.415) requires traceability. Users need to know where claims come from to calibrate trust.\n\n### Rigid Mandates\n\n**Before:**\n```markdown\nAlways use X. Never use Y.\n```\n\n**After:**\n```markdown\n| Context | Use | Why |\n|---------|-----|-----|\n| High throughput | X | Better under load |\n| Simple scripts | Y | Less overhead |\n```\n\n**Why this matters:** Rigid mandates remove agency (Control β = 0.507). Decision frameworks preserve it by showing WHEN and WHY, not just WHAT.\n\n---\n\n## Escalation Rules\n\n### Escalate When\n\n- **Contradictory requirements:** Can't satisfy both without human decision\n- **Missing domain expertise:** Fix requires knowledge optimizer doesn't have\n- **Major structural change:** Scope exceeds targeted fixes\n- **Unclear what to cut:** Removing content requires author intent\n- **Sources can't be found:** Can't verify or cite without research\n\n### Don't Escalate\n\n- **Mechanical fixes:** Typos, formatting, boilerplate\n- **Clear anti-patterns:** Tell-tales, missing picks, vague triggers\n- **Obvious improvements:** Adding \"Use when:\", citing known sources\n\nThe boundary: optimizer fixes execution problems, not strategy problems. Strategy requires human judgment.\n\n---\n\n## Verification\n\nAfter each fix:\n\n1. **SKILL.md still \u003C 500 lines?** Token budget matters\n2. **Frontmatter intact?** Metadata must remain valid\n3. **No broken references?** Links and paths still work\n4. **Fix actually addresses issue?** Re-check against evaluator criteria\n\nWhy verify? Fixes can introduce new problems. Incremental verification catches them early.\n\n---\n\n## The Report\n\nThe optimizer produces a report because:\n\n1. **Transparency:** Human can see what changed and why\n2. **Learning:** Human can understand the patterns being fixed\n3. **Audit:** Changes are traceable and reversible\n4. **Handoff:** Deferred issues are clearly documented\n\nThis follows the Glass Box pattern—make the process inspectable, not just the output.\n"},{slug:"sources",title:"Sources",content:"# Sources\n\n## Skill Authoring\n\n**Anthropic. Skill Authoring Best Practices.**\nhttps://platform.claude.com/docs/en/agents-and-tools/agent-skills/best-practices\n\nCore guidance on writing effective Skills: progressive disclosure, conciseness, testing with multiple models, avoiding anti-patterns.\n\n---\n\n## Research Foundations\n\n### Transparency and Control\n\n**Blaurock, M., Büttgen, M., & Schepers, J. (2025). [\"Designing Collaborative Intelligence Systems for Employee-AI Service Co-Production.\"](https://journals.sagepub.com/doi/10.1177/10946705241238751) Journal of Service Research, 28(4), 544-562.**\n\nTwo scenario-based experiments (n=654: 309 financial services + 345 HR professionals). Not a meta-analysis.\n- Transparency: β = 0.415 (from full text; not independently verified from public sources)\n- Process Control: β = 0.507 (strongest predictor)\n- Engagement features: b = -0.555 (negative for frequent users)\n- Directional findings confirmed: control and transparency dominate, engagement backfires\n\n### Cognitive Effects\n\n**Lee H.P. et al. (2025). \"The Impact of Generative AI on Critical Thinking.\" CHI 2025.**\n- AI confidence negatively correlates with critical thinking (β = -0.69)\n\n**Gerlich (2025). \"AI Use and Critical Thinking.\" Societies.**\n- Strong negative correlation (r = -0.75)\n\n**Kosmyna et al. (2025). \"Your Brain on ChatGPT: Cognitive Debt.\" MIT Media Lab.**\n- 83% of AI users couldn't recall content from AI-assisted work\n\n### Skill Formation (Anthropic, January 2026)\n\n**Shen & Tamkin (2026). [\"How AI Impacts Skill Formation.\"](https://arxiv.org/abs/2601.20245) Anthropic.**\n- RCT with 52 developers learning Python Trio\n- AI group: 50% quiz score vs Control: 67% (Cohen's d = 0.738)\n- 17 percentage point gap ≈ 2 letter grades\n- Six interaction patterns identified: 3 preserve learning, 3 don't\n\n### Disempowerment (Anthropic, January 2026)\n\n**Sharma et al. (2026). [\"Who's in Charge? Disempowerment Patterns.\"](https://arxiv.org/abs/2601.19062) Anthropic.**\n- ~1.5 million Claude.ai conversations analyzed\n- Users rate disempowering interactions MORE favorably\n- Satisfaction drops below baseline when users act on AI outputs\n- \"I should have listened to my own intuition\"\n\n### Skill Degradation\n\n**Budzyń, B. et al. (2025). [\"Endoscopist deskilling risk after AI exposure in colonoscopy.\"](https://www.thelancet.com/journals/langas/article/PIIS2468-1253(25)00133-5/abstract) Lancet Gastroenterol. Hepatol.**\n- 20% relative decline in unaided ADR (28.4% → 22.4%) after AI-assisted colonoscopy introduced. Multicentre observational, 19 endoscopists.\n\n**Zhou et al. (2025). [\"Creative Scar.\"](https://www.sciencedirect.com/science/article/abs/pii/S0160791X25002775) Technology in Society.**\n- 7-day lab experiment with 2-month follow-up (longitudinal!)\n- Creativity drops on AI withdrawal, homogeneity persists months later\n- \"Users do not truly acquire ability, just lose it when AI gone\"\n\n**Fernandes et al. (2025). \"LSAT Performance with AI.\" CHI 2025.**\n- Performance gain d = 1.23 with AI, capability not retained\n\n### Learning Orientation\n\n**ACU (2025). \"Mastery vs Performance Orientation.\" Australian Catholic University.**\n- Mastery orientation correlates with maintained critical thinking\n- Performance orientation correlates with degraded critical thinking\n- *Note: Single institutional study; pattern consistent with literature but effect sizes need replication*\n\n### Code Security\n\n**Tihanyi et al. (2024). \"AI Code Vulnerabilities.\" Empirical Software Engineering.**\n- 62% of AI-generated code contains vulnerabilities\n\n**Fu et al. (2024). \"Python CWE Weaknesses.\" ACM TOSEM.**\n- 29.5% of AI Python code has CWE weaknesses\n\n**Perry et al. (2025). \"7,703 AI-Generated Files.\"**\n- Security degrades 3x through iteration\n\n### Verification\n\n**Dhuliawala et al. (2024). \"Chain-of-Verification Reduces Hallucination.\" ACL 2024.**\n- CoVE achieves 50-70% hallucination reduction\n\n### Homogenization & Diversity\n\n**Jiang et al. (2025). [\"Artificial Hivemind.\"](https://arxiv.org/abs/2510.22954) NeurIPS 2025 Best Paper.**\n- 70+ LLMs tested on 26,000 queries\n- 25 models writing \"metaphor about time\" → only 2 clusters emerged\n- Temperature/ensembling don't help; RLHF punishes diversity\n\n**Meta-analysis (2025). [\"Generative AI and Creativity.\"](https://arxiv.org/abs/2505.17241) arXiv.**\n- 28 studies, n=8,214 participants\n- Diversity reduction: **g = -0.863** (large negative effect)\n- Individual performance up (+0.27), collective diversity down hard\n\n**Doshi & Hauser (2024). [\"Individual Creativity vs Collective Diversity.\"](https://www.science.org/doi/10.1126/sciadv.adn5290) Science Advances.**\n- Individual novelty: +8.1%\n- Story similarity: +10.7%\n- \"Social dilemma: individually better off, collectively homogenized\"\n\n**Hintze et al. (2026). [\"Visual Elevator Music.\"](https://www.cell.com/patterns/fulltext/S2666-3899(25)00299-5) Patterns/Cell.**\n- 700 AI image runs converged to just 12 motifs\n- \"Bland, pop culture, generic - opposite of creative\"\n\n**Mitigation: Wan & Kalman (2025). [\"AI Personas Preserve Diversity.\"](https://arxiv.org/abs/2504.13868)**\n- 10 diverse AI personas eliminated homogenization effect\n- Design CAN preserve diversity\n\n---\n\n## Statistical Glossary\n\n| Statistic | Meaning |\n|-----------|---------|\n| **d** (Cohen's d) | Effect size (0.2 small, 0.5 medium, 0.8 large) |\n| **β** (beta) | Regression coefficient (strength/direction) |\n| **r** (correlation) | Linear relationship (0.3 moderate, 0.5 strong) |\n| **OR** (odds ratio) | Likelihood ratio (OR=35.7 = 35.7x more likely) |\n\n## Caveats\n\n- Most studies are not software-developer specific (Anthropic 2026 is developer-focused)\n- Longitudinal studies now emerging (Zhou 2025: creative scar persists 2 months)\n- Effect sizes transfer imperfectly across domains\n- Research provides direction, not precision\n- Homogenization research is strong (NeurIPS Best Paper, Science Advances, Nature)\n"}],"how-to":[],tutorials:[]},docCount:6}},uses:{params:["slug"]}}}(Array(10)))],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
