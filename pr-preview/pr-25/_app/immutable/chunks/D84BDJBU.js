import{a as r,f as a}from"./BKQXIwCx.js";import"./BLJ2k1vR.js";import{y as s}from"./Cfark-2s.js";var n=a(`<h1 id="collaboration-design-evidence">Collaboration Design Evidence</h1> <p>Research synthesis on what makes human-AI collaboration effective — the design principles that determine whether AI amplifies or erodes capability.</p> <hr/> <h2 id="sources">Sources</h2> <ul><li><a href="https://journals.sagepub.com/doi/10.1177/10946705241238751" rel="nofollow">Blaurock et al. (2025). Designing CI Systems for Employee-AI Service Co-Production. Journal of Service Research.</a></li> <li><a href="https://www.tandfonline.com" rel="nofollow">Ma et al. (2025). Contrastive Explanations in Human-AI Collaboration. Taylor & Francis.</a></li> <li><a href="https://dl.acm.org/doi/10.1145/3411764.3445717" rel="nofollow">Bansal et al. (2021). Does the Whole Exceed its Parts? CHI.</a></li> <li><a href="https://survey.stackoverflow.co/2025/" rel="nofollow">Stack Overflow Developer Survey (2025).</a></li> <li><a href="https://arxiv.org/abs/2601.19062" rel="nofollow">Sharma et al. (2026). Who’s in Charge? Disempowerment Patterns. Anthropic.</a></li></ul> <hr/> <h2 id="the-strongest-levers">The Strongest Levers</h2> <h3 id="blaurock-et-al-2025-">Blaurock et al. (2025) <span class="ev ev-moderate" title="Scenario experiments, n=654, J. Service Research">◐</span></h3> <p><strong>Design:</strong> Two scenario-based experiments with 654 professionals (309 financial services + 345 HR).</p> <p><strong>Key findings:</strong></p> <table><thead><tr><th>Factor</th><th>Effect Size</th><th>What It Means</th></tr></thead><tbody><tr><td>Process control</td><td><strong>β = 0.507</strong></td><td>User agency — ability to direct, override, shape collaboration</td></tr><tr><td>Transparency</td><td><strong>β = 0.415</strong></td><td>Understanding how AI reached conclusions</td></tr><tr><td>Task complexity</td><td>β = 0.247</td><td>AI helps more on complex tasks</td></tr><tr><td>Perceived competence</td><td>β = 0.227</td><td>User confidence in their ability to evaluate</td></tr><tr><td>Engagement features</td><td><strong>b = -0.555</strong></td><td>Each added feature <em>reduces</em> trust</td></tr></tbody></table> <p><strong>Control and transparency dominate.</strong> Not AI capability. Not speed. Whether the human can direct the collaboration and understand its reasoning.</p> <p><strong>The engagement paradox:</strong> Adding engagement features — gamification, personalization, social elements — reduces trust (b = -0.555). Each feature added for “better user experience” measurably degrades the collaboration. Users want control and understanding, not friction disguised as interaction.</p> <p><strong>Limitations:</strong> Scenario-based experiments, not field study. Effect sizes from full text not independently verified from public sources. Service context (financial services, HR), not software development specifically.</p> <hr/> <h2 id="why-explanations-backfire">Why Explanations Backfire</h2> <h3 id="bansal-et-al-chi-2021-">Bansal et al. (CHI 2021) <span class="ev ev-strong" title="CHI peer-reviewed, controlled experiment">●</span></h3> <p><strong>Design:</strong> Controlled experiment comparing AI with and without explanations, across correct and incorrect AI outputs.</p> <p><strong>Key finding:</strong> Explanations increase acceptance regardless of correctness.</p> <table><thead><tr><th>Condition</th><th>Effect</th></tr></thead><tbody><tr><td>Correct AI + explanation</td><td>Small improvement in outcomes</td></tr><tr><td>Incorrect AI + explanation</td><td><strong>Performance degraded</strong></td></tr></tbody></table> <p>When AI is right, explanations help slightly. When AI is wrong, explanations build false confidence and suppress verification. The explanation creates trust without calibrating it.</p> <p><strong>Implication:</strong> Transparency alone is insufficient. Explanations must be paired with verification prompts, or they become persuasion tools rather than calibration tools.</p> <hr/> <h2 id="contrastive-explanations">Contrastive Explanations</h2> <h3 id="ma-et-al-2025-">Ma et al. (2025) <span class="ev ev-moderate" title="Taylor &amp; Francis, controlled study">◐</span></h3> <p><strong>Key finding:</strong> Framing shifts change how humans process AI recommendations.</p> <table><thead><tr><th>Framing</th><th>Example</th><th>Cognitive Mode</th></tr></thead><tbody><tr><td>Prescriptive</td><td>“Use Redis for this cache.”</td><td>Heuristic acceptance</td></tr><tr><td>Contrastive</td><td>“Redis instead of Memcached because you need data structures beyond key-value.”</td><td>Analytic evaluation</td></tr></tbody></table> <p><strong>Mechanism:</strong></p> <ul><li>Shows alternatives were considered</li> <li>Makes tradeoffs visible</li> <li>Activates comparison rather than acceptance</li> <li>Teaches decision frameworks, not just decisions</li></ul> <p>Prescription invites blind trust. Contrast invites evaluation.</p> <p><strong>Limitations:</strong> Single study. Effect size not fully quantified.</p> <hr/> <h2 id="the-why--how-principle">The WHY > HOW Principle</h2> <p>A security study compared two approaches to teaching developers:</p> <table><thead><tr><th>Approach</th><th>Outcome</th></tr></thead><tbody><tr><td>Prescribe HOW (“Always use prepared statements”)</td><td>30% secure-by-construction</td></tr><tr><td>Explain WHY (“SQL injection occurs when user input is treated as code…“)</td><td><strong>80%</strong> secure-by-construction</td></tr></tbody></table> <p><strong>2.5x improvement</strong> from explaining motivation rather than mandating method. <span class="ev ev-moderate" title="Single study, security coding domain">◐</span></p> <p><strong>Mechanism:</strong> HOW prescriptions create brittle rules applied in narrow contexts. WHY explanations build transferable frameworks that generalize.</p> <hr/> <h2 id="the-senior-junior-gap">The Senior-Junior Gap</h2> <h3 id="stack-overflow-2025-">Stack Overflow (2025) <span class="ev ev-moderate" title="Stack Overflow survey, large N, observational">◐</span></h3> <table><thead><tr><th>Behavior</th><th>Seniors</th><th>Juniors</th></tr></thead><tbody><tr><td>Trust AI output</td><td>2.5%</td><td>17%</td></tr><tr><td>Ship AI code to production</td><td>32%</td><td>13%</td></tr><tr><td>Edit AI suggestions</td><td>Substantial</td><td>Minor or none</td></tr></tbody></table> <p><strong>The paradox:</strong> Seniors trust AI least but ship most AI code. They treat AI output as a first draft from a junior developer — read carefully, check edge cases, verify against production constraints, refactor for codebase patterns.</p> <p>Juniors trust more and ship less because they lack judgment to evaluate.</p> <p><strong>Design implication:</strong> Systems optimized for seniors (who verify regardless) fail juniors (who need scaffolding for verification).</p> <hr/> <h2 id="users-misjudge-what-helps-them">Users Misjudge What Helps Them</h2> <h3 id="sharma-et-al-anthropic-2026-">Sharma et al. (Anthropic, 2026) <span class="ev ev-moderate" title="Large-scale observational study, ~1.5M conversations, single platform">◐</span></h3> <p><strong>Design:</strong> Analysis of ~1.5 million Claude.ai conversations.</p> <p><strong>Key finding:</strong> Users rate disempowering interactions MORE favorably in the moment. Interactions that distorted reality, value judgments, or actions felt good.</p> <p>But when users actually acted on AI outputs, satisfaction dropped below baseline. Users expressed regret: “I should have listened to my own intuition.”</p> <p><strong>Implication:</strong> Short-term satisfaction ≠ long-term benefit. The feedback loop is broken — harm feels helpful. Design must compensate for miscalibrated user preferences.</p> <hr/> <h2 id="trust-gradients">Trust Gradients</h2> <p>Not all outputs need the same verification depth:</p> <table><thead><tr><th>Output Type</th><th>Trust</th><th>Verification</th></tr></thead><tbody><tr><td>Formatting, syntax</td><td>High</td><td>Glance</td></tr><tr><td>Library usage, API calls</td><td>Medium</td><td>Check docs for edge cases</td></tr><tr><td>Business logic</td><td>Low</td><td>Full review against requirements</td></tr><tr><td>Security-sensitive code</td><td>Very low</td><td>Dedicated security review</td></tr><tr><td>Architecture decisions</td><td>Very low</td><td>Multiple perspectives</td></tr></tbody></table> <p><strong>Correction rate as calibration metric:</strong></p> <table><thead><tr><th>Rate</th><th>Signal</th></tr></thead><tbody><tr><td>&lt; 5%</td><td>Under-reviewing — automation bias risk</td></tr><tr><td>10-30%</td><td>Healthy calibration</td></tr><tr><td>&gt; 50%</td><td>AI not effective for this task</td></tr></tbody></table> <hr/> <h2 id="verification-decay-pattern">Verification Decay Pattern</h2> <p>Trust calibration degrades without maintenance:</p> <pre class="shiki github-dark" style="background-color:#24292e;color:#e1e4e8" tabindex="0"><code><span class="line"><span>Day 1: Carefully review every suggestion</span></span>
<span class="line"><span>Day 7: Skim, spot-check occasionally</span></span>
<span class="line"><span>Day 30: Accept if it "looks right"</span></span>
<span class="line"><span>Day 90: Auto-accept until things break</span></span></code></pre> <p><strong>Counter-patterns:</strong></p> <ul><li>Structured verification checklist (&lt; 30 seconds, applied consistently)</li> <li>Spot audits (randomly deep-verify even when confident)</li> <li>Red team rotations (assume output is wrong, find the error)</li> <li>Track verification catch rate</li></ul> <hr/> <h2 id="evidence-summary">Evidence Summary</h2> <table><thead><tr><th>Finding</th><th>Effect Size</th><th>Evidence Level</th><th>Source</th></tr></thead><tbody><tr><td>Control strongest lever</td><td>β = 0.507</td><td><span class="ev ev-moderate">◐</span> Moderate</td><td>Blaurock et al. 2025</td></tr><tr><td>Transparency second strongest</td><td>β = 0.415</td><td><span class="ev ev-moderate">◐</span> Moderate</td><td>Blaurock et al. 2025</td></tr><tr><td>Engagement features backfire</td><td>b = -0.555</td><td><span class="ev ev-moderate">◐</span> Moderate</td><td>Blaurock et al. 2025</td></tr><tr><td>Explanations increase blind trust</td><td>Significant</td><td><span class="ev ev-strong">●</span> Strong</td><td>Bansal CHI 2021</td></tr><tr><td>WHY > HOW for learning</td><td>30% → 80%</td><td><span class="ev ev-moderate">◐</span> Moderate</td><td>Security study</td></tr><tr><td>Seniors verify, juniors accept</td><td>2.5% vs 17% trust</td><td><span class="ev ev-moderate">◐</span> Moderate</td><td>Stack Overflow</td></tr><tr><td>Users prefer disempowering interactions</td><td>Observed at scale</td><td><span class="ev ev-moderate">◐</span> Moderate</td><td>Sharma, Anthropic</td></tr><tr><td>Contrastive > prescriptive framing</td><td>Qualitative shift</td><td><span class="ev ev-moderate">◐</span> Moderate</td><td>Ma et al. 2025</td></tr></tbody></table> <hr/> <p><em>Full citations in <a href="bibliography">bibliography</a></em></p>`,1);function l(t){var e=n();s(134),r(t,e)}export{l as default};
