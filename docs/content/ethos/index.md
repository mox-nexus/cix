# Why This Approach

METR ran a randomized controlled trial with 16 experienced developers on mature codebases. AI tools made them 19% slower. They predicted being 24% faster.

That 43-point perception gap isn't an anomaly. It's the starting point for understanding how AI collaboration goes wrong — and what makes it go right.

---

## The productivity illusion

Developers believe AI helps. Measurements say otherwise.

Trust in AI coding accuracy dropped from 43% to 33% in one year (Stack Overflow 2024-2025). Adoption rose to 84% over the same period. People use tools they don't trust, perceive benefits they don't get.

The illusion has a mechanism. AI shifts work from generation to verification. Coding feels easier because writing from scratch is gone. But catching subtle errors in mostly-correct code takes longer and demands sustained attention.

30% of seniors edit AI output enough to offset time savings vs 17% of juniors. Seniors ship 2.5x more AI code to production despite lower trust. They can verify; juniors can't.

---

## The hollowing problem

Beyond productivity, there's capability.

| Study | Finding | Timeframe |
|-------|---------|-----------|
| Lee CHI 2025 | Higher AI confidence → less critical thinking | Cross-sectional |
| Budzyń Lancet 2025 | 20% skill degradation after AI removal | 3 months |
| Kosmyna MIT 2025 | 83% couldn't recall AI-assisted content | Immediate |
| Bastani PNAS 2025 | Unrestricted AI → 17% worse exam performance | Single course |

The Budzyń study is clearest. After AI-assisted colonoscopy was introduced across four centres, endoscopists' unaided detection rate declined from 28.4% to 22.4% on their non-AI cases. The skill atrophied measurably.

No equivalent study exists for developers — the technology is too new. But the cognitive mechanisms are the same. You don't maintain skills you stop exercising.

---

## What makes collaboration work

Blaurock et al. (Journal of Service Research, 2024) studied collaborative intelligence through interviews and experiments with 654 professionals. What predicted good outcomes:

| Factor | Effect | What it means |
|--------|--------|---------------|
| Transparency | Strong positive | User sees AI reasoning → better outcomes |
| Process control | Strongest positive | User shapes how AI works → better outcomes |
| Outcome control | Strong positive | User shapes what AI produces → better outcomes |
| Reciprocity | Strong positive | User grows through collaboration → better outcomes |
| Engagement features | Significant negative | AI asks questions → worse for frequent users |

The engagement finding surprised people. Making AI conversational was supposed to build trust. Instead, for frequent users, engagement features significantly hurt perceived quality.

The pattern: showing reasoning and giving control work. Prompting for interaction doesn't.

---

## Complementary vs substitutive

AI can extend capability three ways:

| Type | Human role | Outcome |
|------|------------|---------|
| **Complementary** | Learns, guides, improves | Better with and without AI |
| **Constitutive** | Enables capability impossible alone | New capability emerges |
| **Substitutive** | Passively consumes output | Skills atrophy |

The distinction isn't what task you're doing. It's how you're doing it.

What makes something substitutive: accepting output without understanding. Treating AI as oracle rather than collaborator.

Bastani's PNAS study makes this concrete. Same AI, same students, different design:
- Unrestricted access: -17% exam performance
- Scaffolded access with guardrails: no significant harm

The tool didn't change. The interaction pattern did.

---

## Foundations compound

Why this matters: patterns established now get scaled up.

If the foundation is complementary — learning, guiding, growing through collaboration — capability compounds. Each cycle builds on the last.

If the foundation is substitutive — checking out, consuming, offloading without understanding — atrophy compounds. The strong negative correlation between AI confidence and critical thinking isn't a one-time effect. It's a trajectory.

AI capability is increasing faster than our frameworks for using it well. Extensions designed now shape whether engineers in five years are more capable than ever or can't function without their tools.

---

## Core principles

From the research:

**Complementary** — AI amplifies, doesn't replace. Human remains central.

**Constitutive** — enables new capability through collaboration. The whole is other than the sum of its parts.

**Transparent by default** — provenance, traceability, explanations, observability. Show reasoning at every step.

**Compounding mastery** — each interaction makes both human and AI more capable. Learning compounds.

**Enabling control** — user agency is the strongest lever (β = 0.507). Shape how, not just what.

**Non-conformity** — preserve intellectual diversity. Resist homogenization. Different perspectives enable collective intelligence.

---

## The goal

Make the user more capable, not more dependent.

An extension succeeds when the human is better at the domain after using it. An extension fails when the human can't function without it.

---

## Sources

Full bibliography in [library/reference/bibliography](../library/reference/bibliography.md).
