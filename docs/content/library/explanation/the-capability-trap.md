# When the Tool Goes Down

Six months in, you're fast. Faster than you've ever been. Your PR count is up. You've shipped things that would've taken a week in two days. AI has made you a noticeably better developer — or at least that's how it feels.

Then one afternoon, Claude's API is down. Or your company blocks Cursor on the production debugging box. Or you're on a flight with no internet.

You open the editor. You look at the problem. And there's a pause that wasn't there a year ago.

---

This is the capability trap. Not a theory — something measurable.

METR, an AI safety research lab, ran an experiment with 16 experienced developers working in their own codebases on tasks they knew well. Before the study, the developers predicted they'd be 24% faster with AI. Measured performance showed they were 19% slower. <span class="ev ev-moderate" title="METR RCT, n=16, within-subject design">◐</span> That's a 43-point gap between perception and reality.

But that's about speed. The capability question is different.

## What offloading actually does

Think about how you learned to navigate a city before GPS. You looked at maps. You made wrong turns and figured out where you went wrong. Over months you built a mental map — not just directions, but a feel for how the city hangs together. You knew which streets ran parallel. You could improvise when a road was closed.

Then you started using GPS for everything. The directions are always correct. You get where you're going. And three years later, you can't navigate the city you've lived in without pulling out your phone.

The GPS didn't make you worse at navigation. It just stopped you from practicing it. The skill didn't get used, so it didn't get built.

Cognitive offloading works the same way. When AI handles the reasoning — the debugging, the design thinking, the "what's actually wrong here" — your brain isn't doing those reps. The task completes. The capability doesn't compound.

Researchers measuring this directly looked at endoscopists who started using AI assistance for detecting polyps during colonoscopies. After three months, their unaided detection rate — on cases where the AI wasn't assisting — dropped 20%. <span class="ev ev-moderate" title="Budzyń et al. Lancet 2025, multicentre observational, 19 endoscopists">◐</span> They weren't doing anything wrong. The AI was genuinely helping. But the practice of detecting without AI was disappearing, and with it, the skill itself.

## It's structural, not personal

Here's the thing: this isn't a discipline failure. You're not doing something wrong by reaching for the tool that works. The trap is built into how the interaction is designed.

When AI answers your question, you don't have to reason through it. That's the whole point — it's faster. But "faster" and "builds capability" are two different metrics. Most AI tools optimize for one of them.

The interaction pattern determines which one you're getting. A direct answer is fast and stops your brain from doing the work. A prompt that makes you think first — "what have you tried?" — is slower and preserves the practice.

Same technology. Different design. Different trajectory.

That trajectory is where the next article picks up.

---

[What the protective interaction looks like →](./the-four-minute-difference)
