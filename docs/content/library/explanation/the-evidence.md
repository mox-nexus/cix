# The Evidence

When independent researchers study AI from different angles, they keep finding the same pattern.

---

## Five Converging Findings

### Same Tool, Opposite Outcomes

The first surprise: identical AI access produces radically different results depending on how people engage with it. Anthropic researchers tracked 52 engineers learning a new library with AI help. Six patterns emerged. The highest-mastery pattern — generate code first, then ask follow-up questions — led to 86% task success. The lowest — use AI to iteratively debug errors — produced 24% success. <span class="ev ev-moderate" title="RCT, n=52, single study">◐</span>

This wasn't about the AI's capability. Everyone used the same tool. The interaction model determined whether people learned or offloaded.

A larger education study confirmed this at scale. When 1,000 students used unrestricted ChatGPT for practice problems, they scored 17% worse on exams. When the same technology included guardrails that required active engagement, exam scores matched students who never used AI at all. <span class="ev ev-strong" title="RCT, n=1,000, PNAS peer-reviewed">●</span>

Design shapes outcome. The hypothesis isn't "AI harms learning" — it's "substitutive engagement harms learning while complementary engagement preserves it."

[Read the full skill formation evidence →](../reference/skill-formation-evidence)

### Measurable Cognitive Shifts

When researchers measure what happens inside people's heads during AI use, they find systematic changes. Knowledge workers who trust AI more show measurably less critical thinking (β = -0.69). EEG studies during AI-assisted writing reveal reduced memory encoding — 83% of users couldn't recall quotes from their own AI-assisted essays. Medical studies show endoscopists' unaided polyp detection dropping 20% after three months of AI assistance.

Four independent studies, different methods, different domains. The pattern holds: cognitive offloading is real and measurable. Neural connectivity scales down. Skills atrophy from disuse. The engagement shift from execution to oversight happens without corresponding increases in verification rigor.

[Read the full cognitive effects evidence →](../reference/cognitive-effects-evidence)

### The Gap Between Perception and Reality

Here's the problem with measurable harm: people don't notice it. An experiment with 16 experienced developers predicted they'd be 24% faster with AI. Actual result? 19% slower. <span class="ev ev-moderate" title="RCT, n=16, rigorous design">◐</span> The 43-point gap between prediction and reality prevents self-correction.

At scale, Stack Overflow surveys show the paradox: trust in AI accuracy dropped 10 percentage points while adoption rose 8 points. People use tools they don't trust and perceive benefits they don't receive.

Anthropic's analysis of 1.5 million conversations revealed something deeper: users rate disempowering interactions more favorably in the moment. Interactions that distorted reality or substituted judgment felt helpful. But when users acted on those outputs, satisfaction dropped below baseline. The feedback loop is broken — harm feels helpful, so people can't learn from experience.

[Read the full productivity evidence →](../reference/productivity-evidence)

### What Actually Works

A meta-analysis of 106 studies with 654 professionals found two factors that predict successful AI collaboration: process control (β = 0.507) and transparency (β = 0.415). <span class="ev ev-strong" title="Meta-analysis, 106 studies">●</span> Control is the strongest lever. Transparency matters. Explanations alone don't help — they increase acceptance regardless of correctness, building trust without calibrating it.

The implication: design must actively counter the perception gap. Users can't reliably judge what helps them because short-term satisfaction diverges from long-term benefit. Transparency and control aren't just nice features — they're necessary to compensate for broken feedback loops.

[Read the full collaboration design evidence →](../reference/collaboration-design-evidence)

### Convergence Toward Consensus

When 70+ different language models answer 26,000 open-ended questions, their outputs cluster into just a few dominant patterns. Temperature and ensembling don't restore diversity — RLHF training penalizes valid but idiosyncratic responses. <span class="ev ev-strong" title="NeurIPS Best Paper, 70+ models">●</span>

A meta-analysis across 28 studies and 8,214 participants quantified the effect: individual creative performance rises slightly while collective diversity drops hard (g = -0.863, a large negative effect). Image generation studies found all 700 iterative loops converged to just 12 motifs regardless of starting prompt. Cultural analysis showed AI pushes writing toward Western norms, dropping classification accuracy from 90.6% to 83.5%.

The diversity that enables collective intelligence — Hong and Page showed diverse groups outperform best-ability groups — disappears when everyone uses the same AI. Individual gains, collective loss.

One study offers hope: using 10 diverse AI personas eliminated the homogenization effect. Design can preserve diversity, but it requires deliberate choice.

[Read the full homogenization evidence →](../reference/homogenization-evidence)

---

## The Pattern

These aren't separate problems. They're facets of the same dynamic:

**Substitutive AI** (does the work for you) improves immediate output while degrading the cognitive foundations that enable long-term capability.

**Complementary AI** (amplifies your work) shows neutral or positive effects on both output and capability.

The evidence comes from different researchers, different methods, different domains. It converges because it's measuring the same underlying pattern from different angles.

One longitudinal study tracked participants for two months after a 7-day AI experiment. Creativity dropped remarkably when AI was withdrawn, and homogeneity kept climbing even months later. The "creative scar" persisted. Users didn't truly acquire capability — they borrowed it, and lost their own in the process.

The research doesn't say AI is harmful. It says the design choice between substitution and complementarity determines whether AI makes humans more capable or more dependent.

Control and transparency aren't just usability features. They're the mechanisms that enable complementary collaboration. The question isn't whether to use AI — it's whether the collaboration is structured to compound human capability or substitute for it.
