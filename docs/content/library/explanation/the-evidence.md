# The Evidence

The pattern appears in every independent study that measures it.

Five research groups, different methods, different domains — not one building on another, but genuinely independent — found the same underlying dynamic. When you use AI as a substitute for thinking, output improves while capability erodes. When you use it as an amplifier that requires your reasoning to be good, output improves and capability is maintained or grows.

That's not a hypothesis; it's a pattern with five independent measurements.

---

## Interaction design determines the outcome

The most direct evidence: same tool, same people, same timeframe, opposite results.

52 engineers learning a new Python library. Six different patterns of AI engagement emerged naturally. Engineers who generated code with AI and then asked follow-up questions to understand it reached 86% task mastery. Those who used AI to iteratively debug errors — delegating the reasoning — reached 24%. The gap opened across four extra minutes of comprehension per session. (Shen & Tamkin, Anthropic, 2026 — randomized controlled trial.) <span class="ev ev-moderate" title="RCT, n=52, single study">◐</span>

A larger study replicated this at the population level. 1,000 students using AI tutors, split between unrestricted access and a tutor designed to scaffold rather than answer. Unrestricted AI produced 17% worse exam performance. The scaffolded version showed no degradation. Same technology, same students — the interaction design was the variable. (Bastani et al., PNAS 2025.) <span class="ev ev-strong" title="RCT, n=1,000, PNAS peer-reviewed">●</span>

[Skill formation evidence →](../reference/skill-formation-evidence)

---

## The cognitive shift shows up in measurement

The interaction pattern affects behavior — and that shows up in how people think.

Researchers measuring critical thinking found a direct correlation: the more confidence workers placed in AI output, the less critical thinking they enacted (β = -0.69, Lee et al., CHI 2025). Trust calibrated down, not up. EEG studies during AI-assisted writing showed reduced memory encoding compared to independent writing — 83% of users couldn't recall quotes from their own AI-assisted essays (Kosmyna et al., MIT, 2025). Medical studies measured the effect at the skill level: after three months of AI-assisted colonoscopy, endoscopists' unaided polyp detection dropped 20% (Budzyń et al., Lancet 2025).

Different instruments measuring the same thing: offloading changes how the brain engages with the work.

[Cognitive effects evidence →](../reference/cognitive-effects-evidence)

---

## The harm is invisible from the inside

Here's the problem with measurable harm: it's not felt as harm.

16 experienced developers predicted AI would make them 24% faster. Rigorous measurement showed they were 19% slower — a 43-point gap between expectation and reality. (METR RCT.) <span class="ev ev-moderate" title="RCT, n=16, rigorous design">◐</span> Stack Overflow data tells the same story at scale: trust in AI accuracy dropped 10 percentage points over a year while adoption rose 8 points. People use tools they don't fully trust and perceive benefits that measurement doesn't confirm.

Anthropic's analysis of 1.5 million conversations found the deepest version of this: users rated interactions that substituted their judgment as helpful in the moment, but when they acted on those outputs, satisfaction dropped below baseline. The feedback loop that would let you learn from bad outcomes is broken — harm that feels like help can't be corrected from experience.

[Productivity evidence →](../reference/productivity-evidence)

---

## Control and transparency are the mechanism

One study measured what differentiates effective from ineffective AI collaboration, rather than just observing outcomes.

654 professionals across different AI collaboration contexts. Two factors predicted positive outcomes: process control (β = 0.507) and transparency (β = 0.415). The user's ability to direct the interaction was the strongest lever. Transparency mattered, but with an important nuance: explanations alone increased acceptance regardless of correctness — they built confidence without calibrating judgment. The transparency that helped was transparency that gave users something to evaluate and act on. (Blaurock et al., Journal of Service Research, 2025.) <span class="ev ev-moderate" title="Blaurock et al. 2025, scenario experiments, n=654">◐</span>

[Collaboration design evidence →](../reference/collaboration-design-evidence)

---

## At scale: homogenization

When the same substitution dynamic plays out across millions of users, there's a collective effect beyond individual skill erosion.

70+ language models answering 26,000 open-ended questions converge to the same handful of patterns. Temperature scaling and ensembling don't restore variety — RLHF training systematically penalizes valid but idiosyncratic responses. A meta-analysis across 28 studies and 8,214 participants found individual performance rises slightly while collective diversity drops significantly (g = -0.863, NeurIPS Best Paper). <span class="ev ev-strong" title="NeurIPS Best Paper, 70+ models">●</span> When everyone uses the same AI, solution variety collapses.

The problem isn't aesthetic. Diverse groups outperform best-ability homogeneous groups on complex problems — the variety that disappeared is functionally necessary for handling unexpected failures.

[Homogenization evidence →](../reference/homogenization-evidence)

---

## The pattern

These findings don't describe five separate problems. They're the same dynamic measured from different angles.

Substitutive AI — interaction designs where the AI does the reasoning and the human approves — improves immediate output while degrading the cognitive foundations that enable long-term capability. The degradation is invisible from the inside because satisfaction stays high and productivity feels real. At scale, individual capability loss becomes collective capability loss as homogenized outputs erode the diversity that makes groups more capable than their best members.

Complementary AI — designed to require the human's reasoning to be good, to give users control and transparent evidence, to scaffold rather than substitute — shows neutral or positive effects on both output and capability.

Five independent studies. The same finding. The design axis between substitution and complementarity is real, measurable, and consequential.
