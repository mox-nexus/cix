# Why It Matters

Patterns established now compound. The foundations being set in 2025-2026 determine whether AI makes engineers more capable or unable to function without their tools.

---

## Sources

- [Shen & Tamkin (2026). How AI Impacts Skill Formation. Anthropic.](https://arxiv.org/pdf/2601.20245)
- [Blaurock et al. (2024). AI-Based Service Contingencies. Journal of Service Research.](https://journals.sagepub.com/doi/10.1177/10946705241253322)
- [Haldane (2009, 2016). Financial Monoculture. Bank of England.](https://www.bankofengland.co.uk/speech/2009/rethinking-the-financial-network)
- [Hong & Page (2004). Groups of Diverse Problem Solvers. PNAS.](https://www.pnas.org/doi/10.1073/pnas.0403723101)
- [Ashby (1956). Law of Requisite Variety. An Introduction to Cybernetics.](https://archive.org/details/introductiontocy0000ashb)

---

## Abstract

Cognitive offloading compounds. Each cycle of delegation makes the next easier to accept. Each reduction in critical thinking makes verification less likely. Each skill that atrophies makes AI feel more essential.

The positive case also compounds. Mastery-oriented users — those who focus on learning rather than output — show 35.7x odds of maintaining critical thinking (ACU Research Bank 2025). Control and transparency (β = 0.507, 0.415) create virtuous cycles where each interaction builds capability.

The stakes are systemic. When everyone uses the same AI, outputs converge. Hong & Page showed diverse groups outperform best-ability groups. Haldane documented how monoculture thinking caused the 2008 financial crisis. Ashby's Law: only variety absorbs variety. Homogenization reduces collective capacity to handle complexity.

---

## Explanation

### Compounding works both directions

Negative compounding is visible in the research. The Lancet study showed 20% skill degradation in 3 months. The Lee study showed confidence in AI correlates negatively with critical thinking (β = -0.69). The perception gap means people don't notice the degradation. Each effect feeds the next.

Positive compounding is also visible. Shen & Tamkin's "Generation-Then-Comprehension" users achieved 86% mastery — they generated code, then asked follow-up questions to understand it. Each cycle built capability for the next. Blaurock's mastery-oriented users maintained critical thinking at 35.7x the odds of performance-oriented users.

The same tool, used differently, produces opposite trajectories.

### Foundations scale

Current AI adoption patterns will propagate. Juniors learning with AI become seniors teaching with AI. Interaction patterns institutionalize. Defaults become norms.

If the foundation is substitutive — accept output, skip verification, optimize for speed — the pattern persists. Each generation learns from the previous. The baseline shifts.

If the foundation is complementary — understand output, verify reasoning, build judgment — capability compounds across generations. The baseline rises.

Shen & Tamkin showed this starkly. Junior developers showed the largest productivity gains (27-39%) and also the largest learning deficits. Seniors showed smaller productivity gains (7-16%) but maintained capability. The pattern isn't about individual skill — it's about whether the collaboration builds or erodes the foundation.

### Diversity is not optional

Collective intelligence requires diversity. Hong & Page proved this formally: randomly selected diverse groups outperformed best-ability homogeneous groups. The mechanism is mathematical — diverse perspectives search the solution space more effectively.

AI threatens this at scale. When everyone uses the same models trained on the same data, outputs converge. Xu et al. found idiosyncratic plot elements echoing across different LLMs. Zhang et al. found AI-assisted responses showed 67-75% content overlap.

Haldane's analysis of the 2008 crisis is instructive. By 2008, hedge fund strategies showed average pairwise correlation of ~0.35. Everyone optimized for the same signals. When the market turned, they all failed together.

Ashby's Law of Requisite Variety: a control system must possess at least as much variety as the disturbances it must handle. If problems are varied but solutions are homogeneous, the system cannot cope.

AI homogenization creates intellectual monoculture. The risk isn't that individuals become worse — it's that everyone becomes the same. The variety that enables collective problem-solving disappears.

### The window

AI capability is increasing faster than frameworks for using it well. Every month, tools get more capable. Every month, more people adopt them with patterns that erode rather than build capability.

This isn't a call to use AI less. The productivity gains are real. The efficiency is real. The question is whether the patterns we establish now create positive or negative compounding.

The research suggests specific design principles work: transparency (show reasoning), control (user shapes direction), mastery orientation (learning over output). These aren't expensive to implement. They require intentionality.

The foundations set in this window determine the trajectory. Engineers in five years will either be more capable than ever — collaboration having built capability iteration by iteration — or unable to function without tools they don't understand.

Design determines which.
