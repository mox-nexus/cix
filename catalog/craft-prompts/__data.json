{"type":"data","nodes":[null,{"type":"data","data":[{"extension":1},{"slug":2,"kind":3,"manifest":4,"tagline":16,"readme":17,"components":18,"variant":21,"tags":11,"docs":22,"docCount":34},"craft-prompts","plugin",{"name":2,"version":5,"description":6,"author":7,"license":10,"keywords":11},"0.1.0","Prompt engineering for reasoning models, deep research, and paper synthesis. Evidence-backed techniques for different model architectures.",{"name":8,"email":9},"Mox Labs","mox.rnd@gmail.com","MIT",[12,13,14,15],"prompt-engineering","reasoning","research","synthesis","Prompt engineering grounded in research. Different model architectures need different techniques.","# craft-prompts\n\nPrompt engineering grounded in research. Different model architectures need different techniques.\n\n## Skills\n\n| Skill | Use When |\n|-------|----------|\n| `deep-reasoning` | Prompting o1, o3, Gemini Deep Think |\n| `deep-research` | AI-assisted research with citation accuracy |\n| `synthesize-papers` | Multi-paper analysis and literature review |\n\n## Philosophy\n\n- **Evidence over intuition** — techniques backed by research\n- **Architecture-aware** — what works for Claude ≠ what works for o1\n- **Composable** — each skill is orthogonal\n\n## Sources\n\nResearch from 2024-2026 including:\n- Many-Shot ICL (ACL/NeurIPS 2024)\n- Chain-of-Verification (Meta Research)\n- Deep reasoning model techniques (o1, o3, Gemini Deep Think)\n",{"agents":19,"skills":20,"hooks":19,"commands":19},0,3,"constraint",{"explanation":23,"how-to":32,"tutorials":33},[24,28],{"slug":25,"title":26,"content":27},"methodology","Prompt Engineering: Methodology","# Prompt Engineering: Methodology\n\nWhy these patterns exist and the research behind them.\n\n---\n\n## The Core Problem\n\nDifferent model architectures need different prompting techniques. What works for autoregressive models (Claude, GPT-4) fails for reasoning models (o1, Deep Think), and vice versa.\n\n| Model Type | How It Works | Prompting Approach |\n|------------|--------------|-------------------|\n| Autoregressive | Token-by-token generation | Guide the path (CoT, examples) |\n| Reasoning/RL | Internal search with verification | Constrain the search space |\n\n## Who This Is For\n\nYou're building a skill or prompt that needs to work across model architectures. You've noticed that the same prompt works brilliantly on Claude but fails on o1—or vice versa. This explains *why* that happens so you can pick the right technique for each architecture.\n\n---\n\n## Why Architecture Matters\n\n### Autoregressive Models (Claude, GPT-4)\n\nThese models generate sequentially. Each token depends on previous tokens. They benefit from:\n- Chain-of-thought (shows the path)\n- Examples (demonstrates patterns)\n- Structured output (guides format)\n\nThe prompt **leads** the generation.\n\n### Reasoning Models (o1, Deep Think)\n\nThese models have internal reinforcement learning. They explore solution spaces and verify internally. They need:\n- Destination definition (not path)\n- Hard constraints (electric fences)\n- Negative constraints (remove easy outs)\n\nThe prompt **bounds** the search.\n\n**Key insight:** Telling o1 \"think step by step\" is redundant—it already does. You're wasting tokens on instructions the model doesn't need.\n\n## Before/After: Same Task, Different Architecture\n\n### Autoregressive (Claude, GPT-4)\n\n**Before** (no guidance):\n```\nSummarize this research paper.\n```\n\n**After** (guided path):\n```\nSummarize this research paper. Structure your response as:\n1. Core claim (1 sentence)\n2. Key evidence (3 bullet points with specific findings)\n3. Limitations the authors acknowledge\n4. One thing the paper doesn't address\n```\n\n**Why it works:** Autoregressive models generate token-by-token. The structured prompt *leads* the sequential generation through a specific path. Without it, the model picks its own path—which may not be yours.\n\n### Reasoning (o1, Deep Think)\n\n**Before** (over-guided):\n```\nSummarize this research paper. Think step by step:\n1. First read the abstract\n2. Then identify the methodology\n3. Then extract findings\n4. Then synthesize\n```\n\n**After** (bounded search):\n```\nSummarize this research paper. Requirements:\n- Every claim must cite a specific section or figure\n- Under 200 words\n- No speculation beyond what the paper states\n- Flag if the sample size is under 100\n```\n\n**Why it works:** Reasoning models already think step-by-step internally. Telling them *how* to think is redundant—it wastes tokens and can degrade performance. Instead, define *what success looks like* and let the model find its own path within those bounds.\n\n---\n\n## Evidence Hierarchy\n\nAll technique recommendations follow this hierarchy:\n\n| Level | Source Type | Example |\n|-------|-------------|---------|\n| **Strong** | Peer-reviewed, replicated | Many-shot ICL paper (ACL/NeurIPS 2024) |\n| **Moderate** | Single quality study, converging evidence | Prompt repetition (Google, Dec 2025) |\n| **Weak** | Practitioner consensus, theoretical | Style analysis patterns |\n| **Speculative** | Reasonable inference | Combined technique effects |\n\nWhen evidence is limited, we say so.\n\n---\n\n## Key Research Findings\n\n### Many-Shot ICL (2024)\n\n**Finding:** Performance consistently improves with more examples, often peaking at 50-100 shots rather than the traditional 3-5.\n\n**Implication:** \"Few-shot\" is a misnomer. More examples = better, up to context limits.\n\n**Source:** [Many-Shot In-Context Learning](https://arxiv.org/abs/2404.11018) — ACL/NeurIPS 2024\n\n### Prompt Repetition (Dec 2025)\n\n**Finding:** Simply repeating the prompt (`\u003CQUERY>\u003CQUERY>`) improves accuracy. 47 wins out of 70 benchmark-model tests, 0 losses.\n\n**Why it works:** Causal models process sequentially—earlier tokens can't attend to later ones. Repetition allows full cross-attention.\n\n**Source:** [Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/abs/2512.14982) — Google Research\n\n### Chain-of-Verification (CoVe)\n\n**Finding:** Independent verification of claims reduces hallucination by 50-70%.\n\n**Why it works:** Decoupling verification from generation prevents confirmation bias.\n\n**Source:** Meta Research, ACL 2024\n\n### Style Transfer Limits\n\n**Finding:** LLMs have persistent \"fingerprints\"—noun-heavy, formal, underuse of discourse markers—that prompting can shift but not eliminate entirely.\n\n**Implication:** ~80% style match ceiling with prompting alone. Fine-tuning needed for higher fidelity.\n\n**Source:** Multiple papers on LLM stylistic fingerprints (2024-2025)\n\n---\n\n## Design Principles\n\n### 1. Technique Selection by Architecture\n\nDon't apply autoregressive techniques to reasoning models or vice versa.\n\n| Technique | Autoregressive | Reasoning |\n|-----------|---------------|-----------|\n| Chain-of-thought | Helps | Redundant |\n| Few-shot examples | Helps | Can hurt |\n| Detailed prompts | Helps | Can hurt |\n| Hard constraints | Optional | Essential |\n\n### 2. Evidence Over Intuition\n\nEvery technique claim should have:\n- The claim (what it does)\n- The effect size (how much)\n- The evidence level (how confident)\n- The source (where from)\n\n### 3. Show Uncertainty\n\nWhen evidence is limited:\n```\n✅ \"This is speculative—no direct research, but analogous findings suggest...\"\n❌ \"Research shows...\" (when it doesn't)\n```\n\n### 4. Composability\n\nSkills are orthogonal. Each covers one domain:\n- `deep-reasoning` — Reasoning model constraints\n- `deep-research` — Hallucination reduction, verification\n- `synthesize-papers` — Multi-paper analysis\n\nCombine as needed for complex tasks.\n\n---\n\n## The Verification Imperative\n\nClaims made without verification propagate errors. Every skill includes verification patterns:\n\n- **deep-research**: Chain-of-Verification (CoVe) for factual claims\n- **synthesize-papers**: Dual-LLM cross-critique for extraction\n\nVerification isn't optional—it's how you know the technique worked.\n\n---\n\nSee [sources.md](sources.md) for full bibliography.\n",{"slug":29,"title":30,"content":31},"sources","Sources","# Sources\n\nFull bibliography for craft-prompts techniques.\n\n---\n\n## Core Research\n\n### Many-Shot In-Context Learning\n- **Citation:** Agarwal et al. (2024). \"Many-Shot In-Context Learning.\" ACL/NeurIPS.\n- **URL:** https://arxiv.org/abs/2404.11018\n- **Key finding:** Performance improves consistently up to 50-100+ examples, contradicting \"few-shot\" assumptions.\n- **Used in:** deep-research\n\n### Chain-of-Verification (CoVe)\n- **Citation:** Dhuliawala et al. (2024). \"Chain-of-Verification Reduces Hallucination in Large Language Models.\" ACL.\n- **URL:** https://arxiv.org/abs/2309.11495\n- **Key finding:** 50-70% hallucination reduction through independent verification.\n- **Used in:** deep-research\n\n---\n\n## Reasoning Models\n\n### o1 Prompting Patterns\n- **Citation:** Lenny's Newsletter (2025). \"Prompt Engineering for o1.\"\n- **Key finding:** o1 performs worse with examples; over-detailed prompts counterproductive.\n- **Used in:** deep-reasoning\n\n### Role Prompting Effectiveness\n- **Citation:** Academic meta-analysis (2024).\n- **Key finding:** Role prompting has \"little to no effect on correctness\" for advanced models.\n- **Used in:** deep-reasoning\n\n---\n\n## Paper Synthesis\n\n### Dual-LLM Cross-Critique\n- **Citation:** (2024). Multi-document synthesis research.\n- **Key finding:** 0.94 accuracy on concordant extractions; 51% of discordant responses become concordant after cross-critique.\n- **Used in:** synthesize-papers\n\n### Plan-Based Synthesis (LitLLMs)\n- **Citation:** (2024). LitLLMs paper.\n- **Key finding:** Plan-first synthesis outperforms direct generation, fewer hallucinations.\n- **Used in:** synthesize-papers\n\n### Claimify Pipeline\n- **Citation:** (2024). Claim extraction research.\n- **Key finding:** 99% claim entailment through selection → disambiguation → decomposition.\n- **Used in:** synthesize-papers\n\n---\n\n## Platform Research\n\n### Hallucination Rates\n- **Citation:** Nature Communications (2025), Omniscience Index, Deakin University studies.\n- **Key finding:** 50-90% of LLM responses not fully supported by cited sources.\n- **Used in:** deep-research\n\n### Citation Accuracy\n- **Citation:** Multiple sources (2025).\n- **Key finding:** Claude ~90% precision, Gemini 66% DOI errors, OpenAI 62% claim-sourced.\n- **Used in:** deep-research\n\n---\n\n## Prompt Engineering Guides\n\n### General\n- [Prompt Engineering Guide](https://www.promptingguide.ai/) — Comprehensive techniques reference\n- [Lakera Prompt Engineering Guide 2026](https://www.lakera.ai/blog/prompt-engineering-guide) — Current best practices\n\n### Chain-of-Thought\n- [Chain-of-Thought Prompting](https://www.promptingguide.ai/techniques/cot) — CoT technique documentation\n\n---\n\n## Meta-Research\n\n### 1,500 Papers Analysis\n- **Citation:** Aakash Gupta analysis (2024).\n- **Key finding:** Over-detailed prompts counterproductive with sophisticated models.\n- **Used in:** deep-reasoning\n\n---\n\n*Last updated: February 2026*\n",[],[],2],"uses":{"params":["slug"]}}]}
