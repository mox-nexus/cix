<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../_app/immutable/assets/0.04mfEt8Y.css" rel="stylesheet">
		<link href="../_app/immutable/assets/CrossLinks.C4T6atsV.css" rel="stylesheet">
		<link href="../_app/immutable/assets/5.Bnuf7QZ4.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.-UEOWCqj.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DJm6SIWl.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B4rBkemD.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BIl_gPaX.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D0iwhpLH.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CgMEB7k0.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BrIrVKyt.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.CPeuuT5b.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CDHW4-dF.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BN47ziLw.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BoBPFXpg.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DoYPt3WR.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/G6a9SXxM.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DOGfeGBa.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/EaAB93f5.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.DavClAbv.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CWj1OF8I.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BvVmrJMT.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Cs1z-Vah.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/5.B99d7FmO.js"><!--12qhfyh--><meta name="description" content="Extensions that enhance human capability, not replace it."/><!----><!--wzm642--><meta name="description" content="Eval methodology for AI systems. Use when: writing evals for agents, skills, MCP servers, prompts, or measuring AI behavior."/><!----><title>craft-evals — cix</title>
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><a href="#main" class="skip-link">Skip to content</a> <!--[--><nav class="site-nav svelte-qgym72" aria-label="Site navigation"><a href="../" class="nav-wordmark svelte-qgym72">cix</a> <div class="nav-links svelte-qgym72"><!--[--><a href="../catalog" class="nav-link svelte-qgym72">catalog</a><a href="../library" class="nav-link svelte-qgym72">library</a><!--]--></div></nav><!--]--> <div class="page svelte-12qhfyh has-nav"><!--[!--><!----><main id="main" class="detail-page svelte-wzm642" style="--variant-color: var(--ci-red)"><nav class="detail-back svelte-wzm642"><a href="../catalog" class="svelte-wzm642">← catalog</a></nav> <header class="detail-header svelte-wzm642"><div class="header-top svelte-wzm642"><h1 class="svelte-wzm642">craft-evals</h1> <span class="detail-kind svelte-wzm642">plugin</span> <span class="detail-version svelte-wzm642">0.3.0</span></div> <p class="detail-description svelte-wzm642">Eval methodology for AI systems. Use when: writing evals for agents, skills, MCP servers, prompts, or measuring AI behavior.</p> <!--[--><div class="detail-inventory svelte-wzm642"><!--[--><span class="inv-item svelte-wzm642">1 skill</span><!--]--></div><!--]--> <!--[--><div class="detail-tags svelte-wzm642"><!--[--><span class="tag svelte-wzm642">evals</span><span class="tag svelte-wzm642">evaluation</span><span class="tag svelte-wzm642">testing</span><span class="tag svelte-wzm642">agents</span><span class="tag svelte-wzm642">skills</span><span class="tag svelte-wzm642">mcp</span><!--]--></div><!--]--></header> <!--[--><nav class="detail-tabs svelte-wzm642" role="tablist"><!--[--><button role="tab" class="tab svelte-wzm642 active" aria-selected="true">README <!--[!--><!--]--></button><button role="tab" class="tab svelte-wzm642" aria-selected="false">Explanation <!--[--><span class="tab-count svelte-wzm642">2</span><!--]--></button><button role="tab" class="tab svelte-wzm642" aria-selected="false">How-To <!--[--><span class="tab-count svelte-wzm642">4</span><!--]--></button><button role="tab" class="tab svelte-wzm642" aria-selected="false">Tutorials <!--[--><span class="tab-count svelte-wzm642">2</span><!--]--></button><!--]--></nav><!--]--> <article class="detail-content svelte-wzm642 prose"><!--[--><!----><h1>craft-evals</h1>
<p>Eval methodology for AI systems. Write evals that measure what matters, not vanity metrics.</p>
<h2>What This Is</h2>
<p>A decision framework for evaluating AI agents, skills, MCP servers, prompts, and multi-agent systems. One skill with 14 deep-dive references, a self-evaluation harness with 31 test cases, and human-optimized documentation. Grounded in Anthropic&#39;s 2026 agent evaluation guidance and peer-reviewed research (KDD 2025, MCPGauge, SWE-bench).</p>
<h2>When It Activates</h2>
<ul>
<li>Writing evals for agents, skills, MCP servers, or prompts</li>
<li>Measuring agent effectiveness or reliability</li>
<li>Evaluating multi-agent coordination</li>
<li>Choosing eval frameworks (DeepEval, Braintrust, RAGAS, Promptfoo)</li>
<li>Designing graders (code-based, model-based, human)</li>
<li>Handling non-determinism (pass@k, pass^k, iterative metrics)</li>
</ul>
<h2>What You Get</h2>
<h3>Skills</h3>
<p><strong>build-eval</strong> -- Eval methodology covering:</p>
<ul>
<li>Three grader types (code, model, human) with selection guidance</li>
<li>Agent type matching (coding, conversational, research, computer use, multi-agent, pipeline)</li>
<li>Non-determinism handling (pass@k, pass^k, iterative/Ralph pattern)</li>
<li>Classification metrics (precision, recall, F1) with confusion matrix</li>
<li>Framework selection (DeepEval, Braintrust, RAGAS, Promptfoo, Phoenix)</li>
<li>Domain routing to 14 reference files for on-demand depth</li>
<li>Cost awareness for model-based grading</li>
</ul>
<h3>Self-Eval Harness</h3>
<p>The plugin evaluates itself using the methodology it teaches:</p>
<table>
<thead>
<tr>
<th>Level</th>
<th>What It Tests</th>
<th>Suite</th>
<th>Cases</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Activation (F1)</strong></td>
<td>Does the skill trigger on the right prompts?</td>
<td><code>activation-suite.json</code></td>
<td>27</td>
</tr>
<tr>
<td><strong>Methodology (Rubric)</strong></td>
<td>Does Claude follow eval methodology when activated?</td>
<td><code>methodology-rubric.json</code></td>
<td>4</td>
</tr>
</tbody></table>
<pre><code class="language-bash">python evals/run_eval.py activation    # Level 1: F1
python evals/run_eval.py methodology   # Level 2: rubric adherence
python evals/run_eval.py all           # Both
python evals/run_eval.py all --dry-run # Mock results (no API calls)
</code></pre>
<h2>Structure</h2>
<pre><code>craft-evals/
├── skills/
│   └── build-eval/
│       ├── SKILL.md                   # Decision framework (&lt; 260 lines)
│       └── references/                # On-demand depth (14 files)
│           ├── agents.md              # Agent eval patterns + OTel
│           ├── benchmarks.md          # SWE-bench, WebArena, etc.
│           ├── cost.md                # Token tracking + budget
│           ├── datasets.md            # Test case design + labeling
│           ├── frameworks.md          # DeepEval, Braintrust, RAGAS
│           ├── iterative.md           # Ralph pattern, recovery_rate
│           ├── mcp.md                 # MCPGauge + tool call metrics
│           ├── methodology.md         # Design rationale
│           ├── multi-agent.md         # Coordination + pipeline eval
│           ├── observability.md       # OTel spans + Phoenix
│           ├── prompts.md             # LLM-as-judge + rubrics
│           ├── security.md            # Red teaming + attack categories
│           ├── skills.md              # Activation F1 + testing modes
│           └── sources.md             # Citation index
├── evals/                             # Self-evaluation harness
│   ├── README.md                      # Harness documentation
│   ├── activation-suite.json          # 27 labeled test cases
│   ├── methodology-rubric.json        # 6-criterion rubric
│   └── run_eval.py                    # Eval runner (Claude SDK + Anthropic API)
├── docs/
│   ├── explanation/                   # Human-optimized (WHY)
│   │   ├── methodology.md            # Design philosophy
│   │   └── sources.md                # Full citations
│   ├── how-to/                        # Human-optimized (HOW)
│   │   ├── write-agent-evals.md       # End-to-end agent eval
│   │   ├── tune-skill-activation.md   # Precision/recall diagnosis
│   │   ├── set-up-eval-harness.md     # Harness setup + run
│   │   └── design-eval-graders.md     # Grader type selection
│   └── tutorials/                     # Human-optimized (LEARN)
│       ├── first-eval-suite.md        # Build a skill eval from scratch
│       └── evaluating-a-coding-agent.md # Full coding agent eval
└── .claude-plugin/
    └── plugin.json
</code></pre>
<h2>Key Concepts</h2>
<p><strong>Three grader types:</strong> Code-based (deterministic, preferred), Model-based (LLM rubric, flexible), Human (gold standard, expensive).</p>
<p><strong>Non-determinism:</strong> LLMs are stochastic. Use pass@k (at least 1 success in k trials) for exploration and pass^k (all k succeed) for production reliability. Run 5+ trials per task.</p>
<p><strong>Iterative metrics (Ralph pattern):</strong> pass@1 is not the ceiling. Feed failures back. recovery_rate tells you whether to deploy with retry loops or improve prompts.</p>
<p><strong>Two-sided testing:</strong> Every metric has two failure modes. 100% recall with 50% precision means your eval triggers on everything (useless). Measure both.</p>
<p><strong>Defense in depth:</strong> No single eval catches everything. Layer: automated evals + production monitoring + A/B testing + transcript review + human studies.</p>
<h2>Philosophy</h2>
<p>This plugin teaches eval frameworks, not eval answers. It makes humans better at measuring AI systems rather than prescribing a single measurement approach. Every recommendation is grounded in research and traceable to sources.</p>
<!----><!--]--></article></main><!----><!--]--><!----></div> <div class="experimental-tag svelte-12qhfyh" aria-hidden="true">experimental</div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_1r3cixp = {
						base: new URL("..", location).pathname.slice(0, -1),
						assets: "/cix"
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.-UEOWCqj.js"),
						import("../_app/immutable/entry/app.CPeuuT5b.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 5],
							data: [null,(function(a){a[0]="evals";a[1]="evaluation";a[2]="testing";a[3]="agents";a[4]="skills";a[5]="mcp";return {type:"data",data:{extension:{slug:"craft-evals",kind:"plugin",manifest:{name:"craft-evals",version:"0.3.0",description:"Eval methodology for AI systems. Use when: writing evals for agents, skills, MCP servers, prompts, or measuring AI behavior.",author:{name:"Mox Labs",email:"mox.rnd@gmail.com"},license:"MIT",keywords:a},tagline:"Eval methodology for AI systems. Write evals that measure what matters, not vanity metrics.",readme:"# craft-evals\n\nEval methodology for AI systems. Write evals that measure what matters, not vanity metrics.\n\n## What This Is\n\nA decision framework for evaluating AI agents, skills, MCP servers, prompts, and multi-agent systems. One skill with 14 deep-dive references, a self-evaluation harness with 31 test cases, and human-optimized documentation. Grounded in Anthropic's 2026 agent evaluation guidance and peer-reviewed research (KDD 2025, MCPGauge, SWE-bench).\n\n## When It Activates\n\n- Writing evals for agents, skills, MCP servers, or prompts\n- Measuring agent effectiveness or reliability\n- Evaluating multi-agent coordination\n- Choosing eval frameworks (DeepEval, Braintrust, RAGAS, Promptfoo)\n- Designing graders (code-based, model-based, human)\n- Handling non-determinism (pass@k, pass^k, iterative metrics)\n\n## What You Get\n\n### Skills\n\n**build-eval** -- Eval methodology covering:\n- Three grader types (code, model, human) with selection guidance\n- Agent type matching (coding, conversational, research, computer use, multi-agent, pipeline)\n- Non-determinism handling (pass@k, pass^k, iterative/Ralph pattern)\n- Classification metrics (precision, recall, F1) with confusion matrix\n- Framework selection (DeepEval, Braintrust, RAGAS, Promptfoo, Phoenix)\n- Domain routing to 14 reference files for on-demand depth\n- Cost awareness for model-based grading\n\n### Self-Eval Harness\n\nThe plugin evaluates itself using the methodology it teaches:\n\n| Level | What It Tests | Suite | Cases |\n|-------|--------------|-------|-------|\n| **Activation (F1)** | Does the skill trigger on the right prompts? | `activation-suite.json` | 27 |\n| **Methodology (Rubric)** | Does Claude follow eval methodology when activated? | `methodology-rubric.json` | 4 |\n\n```bash\npython evals/run_eval.py activation    # Level 1: F1\npython evals/run_eval.py methodology   # Level 2: rubric adherence\npython evals/run_eval.py all           # Both\npython evals/run_eval.py all --dry-run # Mock results (no API calls)\n```\n\n## Structure\n\n```\ncraft-evals/\n├── skills/\n│   └── build-eval/\n│       ├── SKILL.md                   # Decision framework (\u003C 260 lines)\n│       └── references/                # On-demand depth (14 files)\n│           ├── agents.md              # Agent eval patterns + OTel\n│           ├── benchmarks.md          # SWE-bench, WebArena, etc.\n│           ├── cost.md                # Token tracking + budget\n│           ├── datasets.md            # Test case design + labeling\n│           ├── frameworks.md          # DeepEval, Braintrust, RAGAS\n│           ├── iterative.md           # Ralph pattern, recovery_rate\n│           ├── mcp.md                 # MCPGauge + tool call metrics\n│           ├── methodology.md         # Design rationale\n│           ├── multi-agent.md         # Coordination + pipeline eval\n│           ├── observability.md       # OTel spans + Phoenix\n│           ├── prompts.md             # LLM-as-judge + rubrics\n│           ├── security.md            # Red teaming + attack categories\n│           ├── skills.md              # Activation F1 + testing modes\n│           └── sources.md             # Citation index\n├── evals/                             # Self-evaluation harness\n│   ├── README.md                      # Harness documentation\n│   ├── activation-suite.json          # 27 labeled test cases\n│   ├── methodology-rubric.json        # 6-criterion rubric\n│   └── run_eval.py                    # Eval runner (Claude SDK + Anthropic API)\n├── docs/\n│   ├── explanation/                   # Human-optimized (WHY)\n│   │   ├── methodology.md            # Design philosophy\n│   │   └── sources.md                # Full citations\n│   ├── how-to/                        # Human-optimized (HOW)\n│   │   ├── write-agent-evals.md       # End-to-end agent eval\n│   │   ├── tune-skill-activation.md   # Precision/recall diagnosis\n│   │   ├── set-up-eval-harness.md     # Harness setup + run\n│   │   └── design-eval-graders.md     # Grader type selection\n│   └── tutorials/                     # Human-optimized (LEARN)\n│       ├── first-eval-suite.md        # Build a skill eval from scratch\n│       └── evaluating-a-coding-agent.md # Full coding agent eval\n└── .claude-plugin/\n    └── plugin.json\n```\n\n## Key Concepts\n\n**Three grader types:** Code-based (deterministic, preferred), Model-based (LLM rubric, flexible), Human (gold standard, expensive).\n\n**Non-determinism:** LLMs are stochastic. Use pass@k (at least 1 success in k trials) for exploration and pass^k (all k succeed) for production reliability. Run 5+ trials per task.\n\n**Iterative metrics (Ralph pattern):** pass@1 is not the ceiling. Feed failures back. recovery_rate tells you whether to deploy with retry loops or improve prompts.\n\n**Two-sided testing:** Every metric has two failure modes. 100% recall with 50% precision means your eval triggers on everything (useless). Measure both.\n\n**Defense in depth:** No single eval catches everything. Layer: automated evals + production monitoring + A/B testing + transcript review + human studies.\n\n## Philosophy\n\nThis plugin teaches eval frameworks, not eval answers. It makes humans better at measuring AI systems rather than prescribing a single measurement approach. Every recommendation is grounded in research and traceable to sources.\n",components:{agents:0,skills:1,hooks:0,commands:0},variant:"constraint",tags:a,docs:{explanation:[{slug:"methodology",title:"Eval Design: Methodology",content:"# Eval Design: Methodology\n\nWhy these evaluation patterns exist and what they measure.\n\n---\n\n## The Core Problem\n\nSingle metrics mislead. Evaluation that doesn't match usage fails.\n\nA skill with 100% activation rate sounds impressive until you realize it activates on every prompt—including when it shouldn't. A test suite that passes 95% of the time tells you nothing about the 5% of failures that corrupt production data.\n\n**The failure mode:** Measuring what's easy instead of what matters.\n\nThis is why eval design centers on two questions:\n1. What can go wrong?\n2. How do we detect it before users do?\n\n## Who This Is For\n\nYou're shipping an agent or skill and want to know if it actually works — not \"seems good in testing\" but measurably performs. If you've ever shipped an extension that silently failed for 80% of cases, or spent weeks optimizing something that was already at its ceiling, this methodology prevents that.\n\nIf you already believe measurement matters, skip to the metrics. If you're not sure evals are worth the investment, start with [The Deeper Why](#the-deeper-why).\n\n---\n\n## Measurement Fundamentals\n\n### The Two-Sided Test\n\nEvery capability has two failure modes:\n\n| Failure Mode | Example | Detection |\n|--------------|---------|-----------|\n| **False negative** | Should trigger, doesn't | Recall metric |\n| **False positive** | Shouldn't trigger, does | Precision metric |\n\nTesting only one side creates blind spots.\n\n**Example from skill evaluation:**\n\nA Rust skill that activates on every coding question has perfect recall (never misses a valid case) but terrible precision (fires when writing Python, discussing architecture, writing docs).\n\nThe F1 metric forces you to balance both:\n```\nF1 = 2 × (Precision × Recall) / (Precision + Recall)\n```\n\nYou can't game it by optimizing one side.\n\n## Reading Your Metrics\n\nWhen you see these patterns, here's what they mean and what to do:\n\n| You See | It Means | Do This |\n|---------|----------|---------|\n| F1 = 0.95, pass rate = 100% | Eval is saturated — only catches regressions | Add harder test cases or new failure dimensions |\n| Recall = 0.95, Precision = 0.40 | Skill fires on everything (noise) | Tighten trigger description with explicit exclusions |\n| Precision = 0.95, Recall = 0.30 | Skill barely fires (invisible) | Broaden trigger with more intent variations |\n| pass@1 = 0.60, pass@5 = 0.90 | Agent can solve it but needs exploration | Deploy with retry logic, not better prompts |\n| pass@1 = 0.60, pass@5 = 0.63 | Agent is at capability ceiling | Need better model or different approach |\n\nThese patterns are your diagnostic toolkit. Every eval run should end with: \"Which row am I in?\"\n\n### Why pass@k Exists\n\nLLMs are stochastic. Single-run metrics lie about capability.\n\n**The pattern:**\n- 75% success rate per attempt\n- pass@3 (any success in 3 tries): ~98%\n- pass^3 (all 3 succeed): ~42%\n\nSame model, radically different reliability depending on whether you need \"works once\" or \"works every time.\"\n\nThis matters for deployment decisions:\n\n| Use Case | Metric | Why |\n|----------|--------|-----|\n| Exploration | pass@k | One good solution is enough |\n| Production | pass^k | Every execution must work |\n\n---\n\n## The Iterative Pattern (Ralph Discovery)\n\nTraditional pass@k assumes independent trials. Real agents learn from feedback.\n\n**The insight:** An agent with 60% first-try success might reach 95% with error feedback. That's a fundamentally different capability profile.\n\n| Metric | Question Answered |\n|--------|-------------------|\n| **pass@1** | How good is the first attempt? |\n| **pass@k (iterative)** | Can it recover from failures? |\n| **recovery_rate** | What percentage of failures become successes? |\n| **iterations_to_pass** | How fast does it learn? |\n\nThis changes deployment strategy:\n\n```\nAgent A: pass@1 = 80%, recovery_rate = 20%\n→ Deploy with better prompts\n\nAgent B: pass@1 = 60%, recovery_rate = 85%\n→ Deploy with retry loop + feedback\n```\n\nSame apparent capability (both can hit 90%+), completely different implementation patterns.\n\n---\n\n## Multi-Agent Evaluation\n\nSingle-agent metrics miss the collaboration layer.\n\n**The blind spot:** Each agent works perfectly in isolation, but handoffs fail. Task scores look good, but work is duplicated or dropped.\n\n### Why Task Scores Aren't Enough\n\nYou need to measure the gaps between agents:\n\n| Metric | Catches |\n|--------|---------|\n| **Handoff success** | Work dropped during transfers |\n| **Communication efficiency** | Noise drowning signal |\n| **Role adherence** | Agents doing each other's work |\n| **Theory of Mind** | Misunderstanding collaborator state |\n\n**Example failure mode:**\n\n```\nAgent A: Research task score = 90%\nAgent B: Writing task score = 92%\nHandoff success: 45%\n\nOverall system: Task completion = 41%\n```\n\nEach component looks good. The system fails.\n\n---\n\n## Why Grader Types Matter\n\n### Code-Based Graders (Preferred)\n\nDeterministic, fast, objective.\n\n```python\n# ✅ Unambiguous\nassert result.status == \"success\"\nassert result.files_modified == [\"config.yaml\"]\n```\n\n**Use when:** You can specify exact expected state.\n\n### Model-Based Graders (When Necessary)\n\nLLM judges for open-ended tasks.\n\n**The trap:** LLMs can't reliably grade their own outputs without bias.\n\n**The solution:** Multi-judge agreement or human spot-checks.\n\n**Use when:**\n- Output varies legitimately (creative writing, explanations)\n- Outcome matters more than path\n- Applying partial credit for multi-component tasks\n\n### The Grading Principle\n\n> \"Grade what the agent produced, not the path it took.\"\n\nIf 5 different correct solutions exist, your grader must accept all 5. Path-dependent grading creates false failures.\n\n---\n\n## Real-World Benchmark Patterns\n\nProduction benchmarks reveal design principles:\n\n| Benchmark | Domain | Key Insight |\n|-----------|--------|-------------|\n| **SWE-bench Verified** | Code agents | Test suites are ground truth; 500 tasks >> 2294 unverified |\n| **τ2-Bench** | Conversation | Multi-turn state tracking; turn limits prevent rambling |\n| **WebArena** | Web agents | Screenshot + state inspection; must verify GUI changes |\n| **OSWorld** | Computer use | File system state is proof of work |\n\nThe pattern: Verify actual state changes, not LLM claims about state changes.\n\n---\n\n## The Saturation Problem\n\n> \"100% pass rate means the eval only catches regressions.\"\n\n**Why it matters:** You've stopped measuring improvement. The test set is too easy.\n\n**Anthropic guidance:** Target 60-80% pass rates for active development. This leaves headroom to measure progress.\n\n**The fix when saturated:**\n1. Add harder tasks from recent failures\n2. Expand to edge cases\n3. Increase complexity (multi-step → long-horizon)\n\nEvals are living artifacts. They must grow with capability.\n\n---\n\n## Defense in Depth\n\nNo single eval catches everything.\n\n| Layer | Speed | Coverage | Catches |\n|-------|-------|----------|---------|\n| **Automated regression tests** | Fast | Narrow | Breaking changes |\n| **Production monitoring** | Real-time | Broad | Unexpected real-world behavior |\n| **A/B testing** | Days | Statistical | Outcome differences |\n| **Transcript review** | Slow | Deep | Reasoning failures |\n| **Human studies** | Very slow | Gold standard | Subjective quality |\n\nThe Swiss Cheese model: Each layer has holes. Safety comes from holes rarely aligning.\n\n**Example composite strategy:**\n```\nPre-deploy: Automated evals (regression prevention)\nPost-deploy: A/B test (outcome validation)\nWeekly: Transcript review (failure pattern discovery)\nQuarterly: Human study (subjective quality check)\n```\n\n---\n\n## Why These Principles\n\n### Start Early (Step 0)\n\nBuilding evals after the agent is like writing tests after shipping.\n\n**The pattern:** 20-50 tasks from actual failures becomes your foundation. Real problems, not hypothetical ones.\n\n### Unambiguous Tasks (Step 1)\n\n> \"If an expert would debate whether the agent succeeded, the task is poorly specified.\"\n\nVague goals create grader arguments. Evals measure grader quality, not agent quality.\n\n### Balanced Datasets (Step 2)\n\nPositive-only test sets let agents overfit to \"always yes.\"\n\n**The fix:** Include negative examples (should_not_trigger cases) to measure precision.\n\n### Environment Isolation (Step 3)\n\nShared state between test runs creates hidden dependencies.\n\n**The failure:** Test 5 passes because Test 3 created a file. Reordering breaks everything.\n\n**The fix:** Clean environment per trial.\n\n### Read Transcripts (Step 5)\n\nMetrics tell you where. Transcripts tell you why.\n\nPatterns emerge: \"The agent always fails when X and Y both happen.\" That becomes your next test case.\n\n### Maintain as Artifact (Step 7)\n\nEvals decay without ownership. Tests bitrot. Benchmarks saturate.\n\n**The fix:** Dedicated ownership. Someone responsible for keeping evals relevant.\n\n---\n\n## Effect Sizes That Matter\n\nFrom Anthropic's production deployment research:\n\n| Change | Impact |\n|--------|--------|\n| Adding behavioral evals (Bloom) | Caught alignment issues automated tests missed |\n| Transcript review | Identified grader bugs in 15% of evals |\n| Pass@5 vs pass@1 | 98% vs 75% apparent reliability (same model) |\n\nThe pattern: Multiple measurement modes, multiple angles, continuous refinement.\n\n---\n\n## The Test\n\nFor every eval you write:\n\n1. **Does it test both failure modes?** (false positive AND false negative)\n2. **Is the task unambiguous?** (would experts agree on success?)\n3. **Does it measure outcome, not path?** (accepts all valid solutions)\n4. **Can you explain what it catches?** (if not, it's not testing anything meaningful)\n5. **Will it survive model improvements?** (or saturate immediately)\n\nEvals that fail these questions waste time or create false confidence.\n\n---\n\n## The Deeper Why\n\nEvaluation isn't about proving the agent works. It's about understanding where it doesn't—before users find out.\n\n### When Measurement Was Missing\n\nA skill activated on 95% of relevant prompts (high recall) — looked great. But it also activated on 60% of irrelevant prompts (low precision). Users saw noise, learned to ignore it, and eventually disabled the plugin entirely. Accuracy was 70% — a \"passing grade\" that hid a fatal flaw. F1 would have caught it: 0.58, clearly below threshold.\n\nSingle metrics lie. Dimensional measurement tells the truth.\n\n---\n\nThe goal is compounding insight: each test reveals a failure mode, each failure mode becomes a test, the suite becomes a map of how the system can go wrong.\n\nThat map is how you ship with confidence.\n\n---\n\nSee [sources.md](sources.md) for research citations and framework documentation.\n"},{slug:"sources",title:"Sources",content:"# Sources\n\nResearch, frameworks, and production evidence behind eval patterns.\n\n---\n\n## Research Synthesis\n\nThe evaluation landscape in 2026 has converged on three principles: single metrics mislead (dimensional measurement required), deterministic graders alone miss nuance (layered grading needed), and saturation is the enemy of useful evals (60-80% target range). The sources below provide the evidence base, frameworks, and statistical methods that support these principles.\n\n---\n\n## Primary Guidance\n\n**Anthropic (2026). \"Demystifying Evals for AI Agents.\"**\nhttps://www.anthropic.com/engineering/demystifying-evals-for-ai-agents\n\nThe canonical guide to agent evaluation. Covers the 7-step eval building process, grader design, task specification, and saturation management.\n\n**Key insights:**\n- Prefer code-based graders over model-based when possible\n- Start with 20-50 tasks from actual failures\n- Target 60-80% pass rates to measure improvement\n- \"Grade what the agent produced, not the path it took\"\n\n**Anthropic (2025). \"Building Effective Agents.\"**\nhttps://www.anthropic.com/engineering/building-effective-agents\n\nAgent design philosophy with measurement guidance.\n\n**Key quote:** \"The key to success is measuring performance and iterating.\"\n\n---\n\n## Behavioral Evaluation\n\n**Anthropic (2025). \"Bloom: Automated Behavioral Evaluations.\"**\nhttps://alignment.anthropic.com/2025/bloom-auto-evals/\n\nFramework for testing alignment properties at scale.\n\n**Targets:** Sycophancy, self-preservation, power-seeking, sabotage, deception.\n\n**Why it matters:** Automated tests catch behavioral issues that functional tests miss.\n\n---\n\n## Research Foundations\n\n**arxiv:2507.21504 — \"LLM Agent Evaluation: A Survey and Framework\" (KDD 2025)**\nhttps://arxiv.org/abs/2507.21504\n\nComprehensive survey of agent evaluation approaches.\n\n**Coverage:**\n- Task completion metrics\n- Multi-agent coordination evaluation\n- Long-horizon reasoning assessment\n- Human preference alignment\n\n**arxiv:2506.07540 — \"MCPGauge: A Framework for Evaluating MCP Servers\" (2025)**\nhttps://arxiv.org/abs/2506.07540\n\nFour-dimensional evaluation framework for tool servers:\n1. **Proactivity** — Does the server take appropriate initiative?\n2. **Compliance** — Does it follow constraints and user intent?\n3. **Effectiveness** — Does it actually accomplish the task?\n4. **Overhead** — How much coordination cost does it impose?\n\n**Scott Spence (2025). \"How to Make Claude Code Skills Activate Reliably\"**\nhttps://scottspence.com/posts/how-to-make-claude-code-skills-activate-reliably\n\nEmpirical study with 200+ activation tests.\n\n**Findings:**\n- Skill descriptions must be intent-driven, not tool-centric\n- Metadata \"Use when:\" triggers activation\n- Forced mode inflates activation scores\n\n---\n\n## Statistical Methods\n\n**Anthropic (2024). \"Statistical Approach to Model Evals.\"**\nhttps://www.anthropic.com/research/statistical-approach-to-model-evals\n\nMethodological rigor for comparing model performance.\n\n**Covers:**\n- Paired differences for A/B testing\n- Clustering for test set quality\n- Sample size requirements\n- Confidence intervals\n\n---\n\n## Frameworks\n\n### Evaluation Platforms\n\n**DeepEval**\nhttps://deepeval.com/docs/metrics-task-completion\n\nPython-first evaluation framework.\n\n**Key metrics:**\n- `TaskCompletionMetric` — Multi-component task scoring\n- `ToolCorrectnessMetric` — Tool call validation\n- `GEval` — LLM-as-judge with rubrics\n\n**Braintrust**\nhttps://www.braintrust.dev/\n\nUnified platform for offline evals + production observability.\n\n**Why it matters:** Identical API for Python and TypeScript; bridges dev and production.\n\n**RAGAS**\nhttps://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/\n\nRAG and agent-specific metrics.\n\n**Key metrics:**\n- `ToolCallAccuracy` — Precision of tool selection\n- `ToolCallF1` — Balanced tool usage\n- Context relevance/precision/recall for RAG\n\n### Observability\n\n**Phoenix (Arize)**\nhttps://docs.arize.com/phoenix\n\nLocal-first LLM observability and tracing.\n\n**Why use:** Privacy-preserving, runs locally, integrates with OpenTelemetry.\n\n**Promptfoo**\nhttps://promptfoo.dev/\n\nYAML-based assertions and red teaming.\n\n**Use case:** Quick smoke tests, adversarial evaluation, non-Python workflows.\n\n**Harbor**\nhttps://github.com/ai-anchor/harbor\n\nContainerized agent task execution.\n\n**Use case:** Isolated environments, security boundaries, reproducible builds.\n\n---\n\n## Benchmarks\n\n**SWE-bench Verified**\nhttps://www.swebench.com/\n\n500 verified code agent tasks from real GitHub issues.\n\n**Why \"Verified\":** Original 2,294 tasks had quality issues. Verified subset is gold standard.\n\n**τ2-Bench**\nhttps://github.com/tau-bench/tau-bench\n\nMulti-turn conversational agent evaluation (retail, airline domains).\n\n**Key insight:** State tracking across turns; turn limits prevent unbounded conversation.\n\n**WebArena**\nhttps://webarena.dev/\n\n812 tasks for web agents. Realistic websites, multi-step flows.\n\n**Evaluation mode:** Screenshot + DOM state inspection.\n\n**OSWorld**\nhttps://os-world.github.io/\n\nComputer use agent benchmark.\n\n**Evaluation mode:** File system state verification.\n\n---\n\n## Observability Standards\n\n**OpenTelemetry GenAI Semantic Conventions**\nhttps://opentelemetry.io/blog/2025/ai-agent-observability/\n\nStandard spans for AI agent instrumentation:\n- `agent_run` — Top-level agent execution\n- `llm_call` — Model invocation\n- `tool_call` — External tool usage\n- `skill_check`, `skill_match` — Skill activation\n\n**Claude Agent SDK**\nhttps://code.claude.com/docs/en/monitoring-usage\n\nNative OpenTelemetry support.\n\n**Google Agent Developer Kit (ADK)**\nhttps://google.github.io/adk-docs/observability/cloud-trace/\n\nOTel + Cloud Trace integration.\n\n---\n\n## Security Evaluation\n\n**garak — LLM Vulnerability Scanner**\nhttps://github.com/leondz/garak\n\nProbes for:\n- Prompt injection\n- Jailbreaking\n- Data leakage\n- Toxic output\n\n**OWASP LLM Top 10**\nhttps://owasp.org/www-project-top-10-for-large-language-model-applications/\n\nSecurity risk catalog for LLM applications:\n1. Prompt injection\n2. Insecure output handling\n3. Training data poisoning\n4. Model denial of service\n5. Supply chain vulnerabilities\n6. Sensitive information disclosure\n7. Insecure plugin design\n8. Excessive agency\n9. Overreliance\n10. Model theft\n\n---\n\n## Production Case Studies\n\n### Iterative Evaluation (Ralph Pattern)\n\n**Source:** Internal discovery from multi-agent eval work\n\n**Finding:** Agents with mediocre pass@1 often have excellent recovery rates with feedback loops.\n\n**Implication:** Traditional pass@k misses learning capability. Measure iterations_to_pass, recovery_rate, feedback_sensitivity.\n\n### Multi-Agent Coordination\n\n**Framework:** MultiAgentBench (implied from multi-agent.md references)\n\n**Metrics discovered:**\n- Handoff success rate\n- Communication efficiency\n- Role adherence\n- Work duplication detection\n\n**Real-world failure:** Agents with 90%+ individual task scores achieving \u003C50% system completion due to coordination failures.\n\n---\n\n## Anti-Pattern Evidence\n\n**Forced Mode Inflation**\n**Source:** Scott Spence empirical testing (200+ trials)\n\n**Finding:** Forcing skill activation in tests produces inflated scores. Skills appear reliable under forced activation but fail to trigger naturally.\n\n**Fix:** Test activation in realistic conditions.\n\n**Grader Bias**\n**Source:** Anthropic transcript review guidance\n\n**Finding:** 15% of evals had grader bugs discovered through manual transcript review.\n\n**Implication:** Metrics tell you where failures happen. Transcripts tell you why.\n\n---\n\n## Related Frameworks\n\n**LangSmith**\nhttps://smith.langchain.com/\n\nLangChain-integrated evaluation platform.\n\n**Langfuse**\nhttps://langfuse.com/\n\nSelf-hosted alternative to LangSmith.\n\n---\n\n## The State of the Field (2026)\n\n**Consensus:**\n- Code-based graders preferred over LLM judges (speed, objectivity)\n- OpenTelemetry as standard for observability\n- Pass@k insufficient for production (need iterative metrics)\n- Multi-agent evaluation requires coordination-specific metrics\n- Behavioral evals (Bloom) catch issues functional tests miss\n\n**Open Questions:**\n- Standardized multi-agent benchmarks (field still fragmenting)\n- Long-term skill retention evaluation (most studies are \u003C1 week)\n- Transfer learning between eval frameworks\n\n---\n\n## Meta-Sources\n\n**Metric Definitions:**\n\n| Metric | Formula | Source |\n|--------|---------|--------|\n| F1 | 2×(P×R)/(P+R) | Standard ML literature |\n| pass@k | P(≥1 success in k trials) | Chen et al. (Codex paper) |\n| pass^k | P(all k succeed) | Extension of pass@k |\n| RRF | 1/(rank+k) fusion | Cormack et al. (SIGIR) |\n\n**Effect Size Standards:**\n\n| Statistic | Interpretation | Source |\n|-----------|----------------|--------|\n| Cohen's d | 0.2 small, 0.5 medium, 0.8 large | Cohen (1988) |\n| Correlation (r) | 0.3 moderate, 0.5 strong | Standard psych |\n| Beta (β) | Regression coefficient | Standard stats |\n\n---\n\nAll links accessed and verified January 2026.\n"}],"how-to":[{slug:"design-eval-graders",title:"How to Design Eval Graders",content:"# How to Design Eval Graders\n\nWhen you build evals, choosing the right grader determines whether your results are reliable. This guide shows you how to match grader types to task requirements.\n\n## The Core Problem\n\nYou need to grade agent outputs, but a single approach fails. Code-based graders are fast but rigid. Model-based graders handle nuance but hallucinate. Human graders are accurate but don't scale.\n\nThe solution: layer grader types so each catches what others miss.\n\n## Start with Code-Based Graders\n\nDeterministic checks are fast, free, and objective. Use them first.\n\n### When Code-Based Works\n\n| Task Type | Grader Pattern | Example |\n|-----------|---------------|---------|\n| Test execution | Test suite pass/fail | `pytest` exit code = 0 |\n| Format validation | Regex match | JSON parses without error |\n| State verification | File exists, DB state | `assert os.path.exists(path)` |\n| String matching | Exact or contains | Output includes expected phrase |\n\n### Example: Test Suite Grader\n\n```python\ndef grade_code_fix(task_id: str, generated_patch: str) -> bool:\n    \"\"\"SWE-bench pattern: apply patch, run tests.\"\"\"\n\n    # Apply patch to isolated environment\n    repo = clone_repo(task_id)\n    apply_patch(repo, generated_patch)\n\n    # Run test suite\n    result = subprocess.run(\n        [\"pytest\", \"tests/\"],\n        cwd=repo,\n        capture_output=True,\n        timeout=300\n    )\n\n    # Pass = all tests pass\n    return result.returncode == 0\n```\n\n**Why this works:** Tests are the ground truth. If tests pass, the fix is correct. No interpretation needed.\n\n### Example: State Verification\n\n```python\ndef grade_file_creation(task: str, trace: dict) -> dict:\n    \"\"\"Grade by checking expected side effects.\"\"\"\n\n    checks = {\n        \"file_created\": os.path.exists(\"/tmp/output.txt\"),\n        \"correct_content\": open(\"/tmp/output.txt\").read() == expected,\n        \"no_errors\": \"error\" not in trace[\"final_message\"].lower()\n    }\n\n    # All must pass\n    return {\n        \"passed\": all(checks.values()),\n        \"details\": checks\n    }\n```\n\n**Anthropic's advice:** \"Prefer code-based grading; fall back to model-based when necessary.\"\n\n## When to Use Model-Based Graders\n\nOpen-ended outputs where deterministic checks fail.\n\n### The Threshold Question\n\nCan you write deterministic rules that cover 80% of valid outputs?\n- **Yes** → Use code-based graders\n- **No** → Use model-based graders\n\n### When Model-Based Works\n\n| Task Type | Why Deterministic Fails | LLM-as-Judge Pattern |\n|-----------|------------------------|----------------------|\n| Natural language quality | Infinite valid phrasings | Rubric with quality criteria |\n| Multi-component tasks | Partial credit needed | Score per component, sum |\n| Reasoning chains | Correct answer via different paths | Grade outcome, not path |\n\n### Example: Rubric Grader\n\n```python\nRUBRIC = \"\"\"\nRate the agent's research summary on a scale 0-3:\n\n3 = Exceeds: All claims cited, synthesis insightful, sources authoritative\n2 = Meets: Most claims cited, synthesis clear, sources credible\n1 = Partial: Some claims cited, synthesis present but weak\n0 = Fails: Few citations, no synthesis, or unreliable sources\n\nProvide:\n- Score (0-3)\n- Reasoning (one sentence per criterion)\n\"\"\"\n\ndef grade_research_output(input: str, output: str) -> dict:\n    response = llm_judge(\n        prompt=f\"{RUBRIC}\\n\\nInput: {input}\\n\\nOutput: {output}\"\n    )\n\n    # Parse structured response\n    return {\n        \"score\": extract_score(response),\n        \"reasoning\": extract_reasoning(response)\n    }\n```\n\n**Why explicit rubric matters:** Vague criteria (\"Is it good?\") produce unreliable scores. Specific criteria (citations, synthesis, sources) are reproducible.\n\n### Example: Partial Credit\n\n```python\ndef grade_multi_step_task(output: str) -> dict:\n    \"\"\"Each component scored independently.\"\"\"\n\n    components = {\n        \"gathered_data\": llm_grade(output, \"Did agent gather required data?\"),\n        \"analyzed_correctly\": llm_grade(output, \"Is analysis methodology sound?\"),\n        \"formatted_output\": llm_grade(output, \"Is output properly formatted?\")\n    }\n\n    # Weighted sum\n    weights = {\"gathered_data\": 0.4, \"analyzed_correctly\": 0.4, \"formatted_output\": 0.2}\n    total_score = sum(components[k] * weights[k] for k in components)\n\n    return {\n        \"score\": total_score,\n        \"components\": components\n    }\n```\n\n**Trade-off:** Model-based graders are flexible but consume tokens. For high-volume evals (1000+ runs), cost adds up fast.\n\n## When to Add Human Grading\n\nGold standard for subjective quality.\n\n### The 10% Rule\n\nYou can't human-grade everything, but you can spot-check:\n\n```python\n# Randomly sample 10% of outputs for human review\nsample_size = max(10, len(outputs) // 10)\nhuman_review_set = random.sample(outputs, sample_size)\n\n# Send to human reviewers\nfor output in human_review_set:\n    output[\"human_score\"] = request_human_review(output)\n```\n\n### Use Human Grading To\n\n1. **Calibrate model-based graders**\n   ```python\n   # Compare human vs LLM-as-judge\n   correlation = pearsonr(human_scores, llm_scores)\n\n   # If correlation \u003C 0.7, revise LLM rubric\n   ```\n\n2. **Catch edge cases deterministic graders miss**\n   ```python\n   # Code-based grader passed, but human caught subtle issue\n   if human_score \u003C 3 and code_grader_passed:\n       add_to_failure_dataset(output)\n   ```\n\n3. **Validate controversial decisions**\n   ```python\n   # When LLM-as-judge scores vary widely (σ > 0.5), request human review\n   if stdev(llm_scores) > 0.5:\n       request_human_review(output)\n   ```\n\n**Cost vs value:** Human grading is expensive but catches what automated graders miss. Use it strategically, not universally.\n\n## Combining Approaches\n\nBest practice: layer graders in sequence.\n\n### The Filter-Judge-Verify Pattern\n\n```\n1. Rules filter     → Fast elimination (format errors, forbidden content)\n2. Model judge      → Quality assessment (nuanced criteria)\n3. Human spot-check → Calibration (10% sample)\n```\n\n### Example: Layered Grading\n\n```python\ndef grade_agent_output(output: str) -> dict:\n    # Layer 1: Code-based filters\n    if not passes_format_checks(output):\n        return {\"score\": 0, \"reason\": \"Format invalid\"}\n\n    if contains_forbidden_content(output):\n        return {\"score\": 0, \"reason\": \"Policy violation\"}\n\n    # Layer 2: LLM-as-judge for quality\n    llm_score = llm_grade_quality(output)\n\n    # Layer 3: Human review (10% sample)\n    if random.random() \u003C 0.1:\n        human_score = request_human_review(output)\n        log_human_vs_llm(human_score, llm_score)\n\n    return {\"score\": llm_score, \"grader\": \"llm\"}\n```\n\n**Why this works:** Each layer catches different failure modes. Rules catch format errors. LLM catches quality issues. Humans catch subtle failures both miss.\n\n## Grade the Outcome, Not the Path\n\nAgents solve tasks differently than humans expect. Don't penalize valid alternative approaches.\n\n### The Anti-Pattern\n\n```python\n# BAD: Requires specific tool sequence\nexpected_tools = [\"search_flights\", \"compare_prices\", \"book_flight\"]\nactual_tools = [\"aggregate_search\", \"book_flight\"]  # Skipped comparison, used better tool\n\n# This fails even though task succeeded\nassert actual_tools == expected_tools  # ❌ Too rigid\n```\n\n### The Fix\n\n```python\n# GOOD: Grade outcome state\ndef grade_flight_booking(task: str, trace: dict) -> bool:\n    # Check: was flight booked?\n    booking_confirmed = \"confirmation_number\" in trace[\"final_output\"]\n\n    # Check: was it cheapest available?\n    booked_price = extract_price(trace)\n    cheapest_price = query_actual_cheapest(task)\n    price_acceptable = booked_price \u003C= cheapest_price * 1.1  # 10% tolerance\n\n    return booking_confirmed and price_acceptable\n```\n\n**Anthropic's principle:** \"Grade what the agent produced, not the path it took.\" An agent that skips expected steps but achieves the goal has succeeded.\n\n## Validate Your Graders\n\nGraders have bugs. Find them before they corrupt your eval results.\n\n### Read Transcripts\n\nAnthropic reports 15% of evals have grader bugs discovered by reading agent transcripts.\n\n```python\n# For each failed test, manually inspect:\n# 1. Did grader correctly identify failure?\n# 2. Or did agent actually succeed but grader failed?\n\nfor test_case in failed_cases[:20]:  # Review first 20 failures\n    print(f\"Task: {test_case.input}\")\n    print(f\"Output: {test_case.output}\")\n    print(f\"Grader reason: {test_case.grade_reasoning}\")\n    input(\"Does grader decision look correct? [Enter to continue]\")\n```\n\n### Check for False Passes\n\nMore dangerous than false failures: grader passes a bad output.\n\n```python\n# Inject known-bad outputs\nbad_outputs = [\n    {\"input\": \"Book flight\", \"output\": \"Error: API failed\", \"should_fail\": True},\n    {\"input\": \"Summarize\", \"output\": \"...\", \"should_fail\": True}  # Incomplete\n]\n\nfor test in bad_outputs:\n    score = grader(test[\"input\"], test[\"output\"])\n    assert score == 0, f\"Grader passed a known-bad output: {test}\"\n```\n\n**If grader passes known-bad outputs, your eval results are unreliable.** Fix the grader before running full eval suite.\n\n## Summary Checklist\n\nBefore finalizing your grader design:\n\n- [ ] Started with code-based graders (deterministic, fast, free)\n- [ ] Used model-based only where deterministic fails (open-ended quality)\n- [ ] Defined explicit rubric with 0-3 scale and clear criteria\n- [ ] Planned human spot-check for 10% of outputs\n- [ ] Grading outcome state, not tool call sequence\n- [ ] Validated grader with known-good and known-bad cases\n- [ ] Read transcripts to catch grader bugs\n\nIf any box is unchecked, revisit that aspect before running evals at scale.\n"},{slug:"set-up-eval-harness",title:"How to Set Up and Run the Eval Harness",content:"# How to Set Up and Run the Eval Harness\n\nWhen you build an LLM skill or agent, you need empirical measurement of whether it works. This guide shows you how to run the craft-evals self-evaluation harness.\n\n## The Core Problem\n\nYou've built the craft-evals plugin. Now you want to verify it actually works before shipping. The harness tests two questions:\n\n1. **Activation:** Does the skill trigger on eval-related questions?\n2. **Methodology:** When triggered, does Claude follow the methodology?\n\n## Prerequisites\n\nYou need Python 3.10+ and the Anthropic SDK.\n\n```bash\n# Check Python version\npython --version  # Should be 3.10+\n\n# Install dependencies (using uv)\nuv pip install anthropic\n\n# Set API key\nexport ANTHROPIC_API_KEY=your-key-here\n```\n\nIf you don't have `uv`, standard pip works: `pip install anthropic`\n\n## Run the Activation Suite\n\nThis tests whether the skill triggers on the right prompts.\n\n```bash\ncd plugins/craft-evals/evals\npython run_eval.py activation\n```\n\nYou'll see output like:\n\n```\nRunning activation eval (24 cases, 5 trials)...\n  must-001: OK (rate=100%)\n  must-002: OK (rate=100%)\n  should-not-003: FAIL (rate=60%)  # Triggered when it shouldn't\n  ...\n\n============================================================\n  ACTIVATION EVALUATION RESULTS\n============================================================\n\nPrecision: 85%\nRecall:    90%\nF1:        87%\n\nConfusion Matrix:\n  TP=9 FP=2\n  FN=1 TN=10\n\nStatus: GOOD\n```\n\n### Options\n\nControl trials and behavior:\n\n```bash\n# More trials for stability (default: 5)\npython run_eval.py activation --trials 10\n\n# Test without API calls (mock results)\npython run_eval.py activation --dry-run\n```\n\nMore trials reduce variance from LLM stochasticity but cost more API calls.\n\n## Run the Methodology Suite\n\nThis tests whether Claude follows the craft-evals methodology when activated.\n\n```bash\npython run_eval.py methodology\n```\n\nOutput shows per-criterion scoring:\n\n```\nRunning methodology eval (8 cases)...\n  method-001: PASS (score=82%)\n  method-002: FAIL (score=65%)\n  ...\n\n============================================================\n  METHODOLOGY EVALUATION RESULTS\n============================================================\n\nAverage Score: 78%\nPass Rate:     75%\n\nCriterion Averages (0-3 scale):\n  metric_selection: 2.5 [OK]\n  failure_modes: 1.8 [WEAK]      # Needs attention\n  non_determinism: 2.2 [OK]\n  framework_guidance: 2.0 [OK]\n  pitfall_awareness: 1.9 [WEAK]\n  actionability: 2.4 [OK]\n\nStatus: GOOD\n\nWeak Areas:\n  - failure_modes\n  - pitfall_awareness\n\nSuggestions:\n  - Emphasize dual failure mode thinking in core sections\n  - Add more entries to the common pitfalls table\n```\n\n## Run Both Suites\n\n```bash\npython run_eval.py all\n```\n\nThis runs activation first, then methodology. Use this before shipping changes to the skill.\n\n## Interpret Results\n\n### Activation Metrics\n\n| Metric | Meaning | Target |\n|--------|---------|--------|\n| **Precision** | When skill fires, is it relevant? | 85%+ |\n| **Recall** | When it should fire, does it? | 85%+ |\n| **F1** | Balanced score | 85%+ |\n\n**Status thresholds:**\n\n| F1 Score | Status | Action |\n|----------|--------|--------|\n| 85%+ | Excellent | No action needed |\n| 70-85% | Good | Minor tuning |\n| 50-70% | Needs work | Review triggers |\n| Less than 50% | Poor | Significant changes needed |\n\n### Methodology Metrics\n\nScores use a weighted rubric (0-3 scale per criterion):\n\n| Criterion | Weight | What It Tests |\n|-----------|--------|---------------|\n| metric_selection | 25% | Recommends appropriate metrics |\n| failure_modes | 20% | Addresses precision and recall |\n| non_determinism | 15% | Accounts for LLM stochasticity |\n| framework_guidance | 15% | Actionable framework recommendations |\n| pitfall_awareness | 15% | Warns about common traps |\n| actionability | 10% | Provides concrete steps or code |\n\n**Per-criterion thresholds:**\n- 2.5-3.0: Excellent\n- 2.0-2.5: OK\n- 1.5-2.0: Weak (needs improvement)\n- Below 1.5: Poor (urgent fix)\n\nWeighted average above 85% = excellent, 70-85% = good.\n\n## Add Test Cases\n\nTest cases live in JSON files in the `evals/` directory.\n\n### Activation Cases\n\nEdit `activation-suite.json`:\n\n```json\n{\n  \"id\": \"must-016\",\n  \"prompt\": \"How do I measure if my agent is working?\",\n  \"expectation\": \"must_trigger\",\n  \"rationale\": \"Direct eval question using different phrasing\"\n}\n```\n\n**Expectation values:**\n- `must_trigger`: Should always activate\n- `should_not_trigger`: Should never activate\n- `acceptable`: Edge case (excluded from F1 calculation)\n\nBalance positive and negative cases roughly 50/50.\n\n### Methodology Cases\n\nEdit `methodology-rubric.json`:\n\n```json\n{\n  \"id\": \"method-009\",\n  \"prompt\": \"I need to evaluate my coding agent's suggestions\",\n  \"expected_criteria\": {\n    \"metric_selection\": [\"precision\", \"recall\", \"false positive\"],\n    \"failure_modes\": [\"over-activation\", \"under-activation\"]\n  },\n  \"min_score\": 0.70\n}\n```\n\nThe harness uses LLM-as-judge to score responses against the rubric.\n\n## Troubleshooting\n\n### \"No module named 'anthropic'\"\n\nInstall the SDK:\n\n```bash\nuv pip install anthropic\n# or\npip install anthropic\n```\n\nVerify with: `python -c \"import anthropic; print('OK')\"`\n\n### Inconsistent Results\n\nLLMs are stochastic. The harness runs multiple trials and takes majority vote.\n\nIf results vary wildly:\n\n1. Increase trials: `--trials 10`\n2. Check if test cases are ambiguous\n3. Use `--dry-run` to test harness logic separately\n\n### False Positives in Activation Detection\n\nThe harness uses keyword heuristics to detect skill activation. If detection seems wrong, check `run_eval.py` around line 78 for the detection logic.\n\nFor production use, consider switching to the Claude Agent SDK approach (requires `claude_agent_sdk` package).\n\n## What Success Looks Like\n\nBefore shipping the craft-evals plugin:\n\n```bash\npython run_eval.py all\n```\n\nTarget:\n- Activation F1: 85%+\n- Methodology average: 85%+\n- No criterion below 2.0\n\nIf you hit these thresholds, the skill is calibrated for production use.\n"},{slug:"tune-skill-activation",title:"How to Tune Skill Activation",content:"# How to Tune Skill Activation\n\nWhen your skill fires on every prompt or misses the ones that matter, you need to tune its activation behavior. This guide shows you how to measure the problem and fix it.\n\n## The Core Problem\n\nYour skill has two failure modes:\n\n- **False Positive (Noise)**: Activates when it shouldn't, wasting context and diluting relevance\n- **False Negative (Miss)**: Doesn't activate when it should, leaving users without help\n\nYou can't see this without measurement. Raw activation rate tells you nothing.\n\n## Step 1: Run the Activation Eval\n\n```bash\ncd plugins/craft-evals/evals\npython run_eval.py activation\n```\n\nThis produces:\n\n```\nPrecision: 75%    # When it fires, it's right 75% of time\nRecall:    60%    # It catches 60% of relevant prompts\nF1:        67%    # Balanced score\n\nStatus: NEEDS WORK\n```\n\n### What These Numbers Mean\n\n| Metric | Formula | Question |\n|--------|---------|----------|\n| **Precision** | TP / (TP + FP) | When skill activates, is it relevant? |\n| **Recall** | TP / (TP + FN) | When skill should activate, does it? |\n| **F1** | 2×(P×R)/(P+R) | Balanced score |\n\n### The Confusion Matrix\n\n```\n                    ACTUALLY TRIGGERED\n                      Yes         No\n                  +-----------+-----------+\nSHOULD      Yes   |    TP     |    FN     |\nTRIGGER           |  Correct  |  Missed   |\n                  +-----------+-----------+\n            No    |    FP     |    TN     |\n                  |   Noise   |  Correct  |\n                  +-----------+-----------+\n```\n\n## Step 2: Diagnose the Issue\n\nLook at precision and recall together:\n\n| Pattern | Problem | Root Cause |\n|---------|---------|------------|\n| High recall, low precision | Over-activating (FP > 0) | Description too broad |\n| Low recall, high precision | Under-activating (FN > 0) | Description too narrow |\n| Both low | Fundamental mismatch | Description unrelated to actual use |\n\n### Example Diagnosis\n\n```\nPrecision: 60%    # 40% of activations are noise\nRecall:    90%    # Only missing 10% of valid cases\nF1:        72%\n\nProblem: Skill triggers too much - tighten description\n```\n\n## Step 3: Fix the Description\n\n### For Low Precision (Over-Activation)\n\n**Before:**\n```yaml\ndescription: \"Help with testing and metrics\"\n```\n\n**Problem:** This triggers on unit tests, performance benchmarks, business analytics. All noise.\n\n**After:**\n```yaml\ndescription: \"Write evals for LLM agents and AI systems. Use when: building test suites for agents, measuring agent effectiveness, evaluating multi-agent coordination. NOT for: unit tests, integration tests, performance benchmarks, business metrics.\"\n```\n\n**Changes:**\n- Added explicit scope (\"LLM agents and AI systems\")\n- Added exclusions to prevent false positives\n- Used \"Use when:\" pattern for clarity\n\n### For Low Recall (Under-Activation)\n\n**Before:**\n```yaml\ndescription: \"Write evals for LLM agents\"\n```\n\n**Problem:** Misses \"test suite\", \"metrics\", \"measure performance\", \"choose eval framework\".\n\n**After:**\n```yaml\ndescription: \"Write evals for LLM agents, multi-agent systems, skills, MCP servers. Use when: building test suites, measuring effectiveness, evaluating coordination, choosing eval frameworks. Covers: DeepEval, Braintrust, RAGAS, precision, recall, F1, pass@k.\"\n```\n\n**Changes:**\n- Broadened scope (agents, skills, MCP servers)\n- Added user intent triggers (\"Use when:\")\n- Added keyword coverage (framework names, metric terms)\n\n### Intent-Driven, Not Tool-Centric\n\nWrong approach:\n```yaml\n# Triggers only when tool names are mentioned\ndescription: \"Use when: DeepEval, Braintrust, RAGAS\"\n```\n\nRight approach:\n```yaml\n# Triggers on user goals\ndescription: \"Use when: building test suites, measuring agent effectiveness, choosing eval frameworks\"\n```\n\nUsers think in goals (\"I need to test my agent\"), not tools (\"I need DeepEval\").\n\n## Step 4: Rebalance Test Cases\n\nYour test suite should be 50/50 positive and negative cases.\n\n### Check Current Balance\n\n```bash\n# Count test cases\ngrep \"must_trigger\" activation-suite.json | wc -l\ngrep \"should_not_trigger\" activation-suite.json | wc -l\n```\n\n### Add Missing Cases\n\nIf you have 20 positive, 5 negative:\n\n```json\n{\n  \"id\": \"neg-006\",\n  \"prompt\": \"Run unit tests for my Python function\",\n  \"expectation\": \"should_not_trigger\",\n  \"rationale\": \"Unit test, not agent eval\"\n}\n```\n\n### Mark Ambiguous Cases\n\nFor borderline prompts:\n\n```json\n{\n  \"id\": \"edge-001\",\n  \"prompt\": \"Explain F1 score\",\n  \"expectation\": \"acceptable\",\n  \"rationale\": \"Could be educational or eval-related\"\n}\n```\n\nMarking as `acceptable` excludes them from F1 calculation.\n\n## Step 5: Iterate\n\nAfter making changes, re-run the eval:\n\n```bash\n# Before changes\npython run_eval.py activation > before.txt\n\n# Edit description in SKILL.md\n\n# After changes\npython run_eval.py activation > after.txt\n\n# Compare\ndiff before.txt after.txt\n```\n\nTrack progress over time:\n\n| Date | Precision | Recall | F1 | Change |\n|------|-----------|--------|-----|--------|\n| 2026-02-01 | 60% | 90% | 72% | Initial |\n| 2026-02-05 | 85% | 88% | 86% | Tightened description |\n| 2026-02-09 | 90% | 85% | 87% | Added exclusions |\n\n## Thresholds\n\n| F1 Score | Interpretation | Action |\n|----------|----------------|--------|\n| 85%+ | Excellent | No action needed |\n| 70-85% | Good | Minor tuning |\n| 50-70% | Needs work | Review triggers |\n| \u003C50% | Poor | Significant changes needed |\n\n## What Good Looks Like\n\n**Before tuning:**\n```\nDescription: \"Write evals\"\n\nPrecision: 55%\nRecall:    95%\nF1:        70%\n\nProblem: Triggers on \"write tests\", \"measure performance\", \"unit test coverage\"\n```\n\n**After tuning:**\n```\nDescription: \"Write evals for LLM agents, multi-agent systems, skills. Use when: building test suites, measuring agent effectiveness, evaluating coordination. NOT for: unit tests, integration tests, business metrics.\"\n\nPrecision: 90%\nRecall:    88%\nF1:        89%\n\nImprovement: Eliminated unit test noise, still catches eval questions\n```\n\n## When to Stop Tuning\n\nStop when one of these is true:\n\n1. F1 score is 85% or higher\n2. Improvement is less than 2% per iteration\n3. Changes start hurting one metric to help the other\n\nPerfect is the enemy of good. At 85% F1, your effort is better spent elsewhere.\n\n"},{slug:"write-agent-evals",title:"How to Write Evals for an AI Agent",content:"# How to Write Evals for an AI Agent\n\nYou built an agent. Does it work? A single pass rate tells you nothing about failure modes. This guide shows you how to write evals that measure what matters.\n\n## The Core Problem\n\nActivation rate is not an eval:\n\n```python\nactivation_rate = 100%  # Activates on every prompt\n```\n\nThis number is useless. It fires on everything, meaning it adds no signal.\n\nSingle metrics lie. An agent with 90% pass rate could be:\n- Solving 90% of tasks correctly, or\n- Firing on every input, getting 10% false positives you'll never see\n\nYou need to measure both failure modes: misses (recall) and noise (precision).\n\n## Step 1: Identify Your Agent Type\n\nDifferent agent types need different eval approaches.\n\n| Agent Type | Primary Grader | Key Metrics | Example |\n|------------|----------------|-------------|---------|\n| **Coding** | Test suites | Tests pass, no regressions | \"Add factorial function\" → tests pass |\n| **Conversational** | Multi-grader (state + rubric) | Resolution, tone, turn count | \"Book a flight\" → booking confirmed |\n| **Research** | Model-based | Source quality, claim support | \"Why did revenue drop?\" → cited evidence |\n| **Computer Use** | State inspection | GUI state, file system | \"Archive emails\" → folder exists, count correct |\n\nMatch your agent to the pattern. Coding agents get test suites. Conversational agents need state verification plus transcript rubrics.\n\n## Step 2: Select Your Primary Grader\n\nThree types, in order of preference:\n\n### Code-Based (Preferred)\n\nDeterministic checks: string matching, test suites, state inspection.\n\n```python\n# Coding agent: Run tests\ndef grade_coding_task(output):\n    result = subprocess.run([\"pytest\"], capture_output=True)\n    return result.returncode == 0\n\n# Computer use: Check file system\ndef grade_file_task(expected_path):\n    return os.path.exists(expected_path)\n```\n\n**When to use:** Anytime you can check the outcome deterministically. Fast, free, objective.\n\n### Model-Based\n\nLLM judges alignment between task and outcome.\n\n```python\nfrom deepeval.metrics import TaskCompletionMetric\n\nmetric = TaskCompletionMetric(threshold=0.7)\n# LLM scores: \"Did the agent achieve the goal?\"\n```\n\n**When to use:** Open-ended tasks where outcomes vary (writing, research, conversation). Flexible but costs tokens per eval.\n\n### Human\n\nExpert review, crowdsourced judgment, spot-checks.\n\n**When to use:** Gold standard for subjective quality. Expensive and slow, so use for validation, not continuous evals.\n\n**Rule:** Start code-based. Upgrade to model-based only when deterministic checks can't capture success. Use human for spot validation.\n\n## Step 3: Choose Metrics\n\nMatch metrics to what you're measuring.\n\n| What You Need to Know | Metric | Formula |\n|-----------------------|--------|---------|\n| Does it work? | Accuracy | Correct / Total |\n| Does it fire correctly? | Precision | True Positives / (TP + False Positives) |\n| Does it catch everything? | Recall | True Positives / (TP + False Negatives) |\n| Balance of both? | F1 | 2 × (Precision × Recall) / (P + R) |\n| Can it use tools? | Tool Correctness | Expected tools called correctly |\n\n### When to Use F1\n\nUse F1 when you have two failure modes:\n\n```\nAgent fires → Is it relevant?  (Precision)\nShould fire → Does it fire?    (Recall)\n```\n\nExample: A Rust skill should fire on Rust questions (recall) and NOT fire on Python questions (precision).\n\n### When to Use Accuracy\n\nUse accuracy when there's a clear right/wrong answer:\n\n```\nTask: \"Add factorial function\"\nSuccess: Tests pass\nAccuracy = Tasks completed / Total tasks\n```\n\n## Step 4: Build Labeled Test Cases\n\nStart with 20-50 cases from actual failures (Anthropic's \"Step 0\").\n\n### Structure\n\n```json\n{\n  \"id\": \"001\",\n  \"input\": \"Fix authentication bug in auth.py\",\n  \"expected\": {\n    \"type\": \"patch\",\n    \"file\": \"auth.py\",\n    \"tests_pass\": true\n  },\n  \"labels\": [\"must_succeed\"],\n  \"metadata\": {\n    \"difficulty\": \"medium\",\n    \"source\": \"production bug #4521\"\n  }\n}\n```\n\n### Balance Positive and Negative\n\nDon't just test success cases. Include both:\n\n| Label | Tests | Measures |\n|-------|-------|----------|\n| `must_succeed` | Agent should complete this | Recall (misses) |\n| `should_fail` | Agent should recognize this is impossible/invalid | Precision (noise) |\n\nExample for a coding agent:\n\n```json\n[\n  {\"input\": \"Add tests for login\", \"label\": \"must_succeed\"},\n  {\"input\": \"Make this faster without changing behavior\", \"label\": \"must_succeed\"},\n  {\"input\": \"Fix the bug (no bug description)\", \"label\": \"should_fail\"}\n]\n```\n\nOne-sided test sets let agents overfit. The agent that fires on everything gets 100% recall, 0% precision.\n\n### Source from Real Usage\n\nDon't invent hypothetical cases. Pull from:\n- Production bugs that slipped through\n- Support tickets\n- Failed runs from development\n- Edge cases discovered in code review\n\nReal failures are higher signal than imagined ones.\n\n## Step 5: Handle Non-Determinism\n\nLLMs are stochastic. One run tells you nothing.\n\n### Run Multiple Trials\n\nMinimum 5 runs per test case. Track both:\n\n| Metric | Formula | Question |\n|--------|---------|----------|\n| **pass@k** | P(at least 1 success in k trials) | One success is enough? |\n| **pass^k** | P(all k trials succeed) | Needs to be reliable? |\n\nExample: Agent has 75% per-trial success rate.\n\n```\npass@3 = 1 - (1 - 0.75)^3 = 98%   # Exploration mode\npass^3 = 0.75^3 = 42%              # Production reliability\n```\n\n**When to use each:**\n- **pass@k**: Rapid prototyping, research, one-off tasks\n- **pass^k**: Production systems, customer-facing agents\n\n### Example Implementation\n\n```python\ndef eval_agent(agent, test_case, trials=5):\n    results = []\n    for _ in range(trials):\n        output = agent.run(test_case[\"input\"])\n        success = grade(output, test_case[\"expected\"])\n        results.append(success)\n\n    pass_at_k = any(results)\n    pass_all_k = all(results)\n\n    return {\n        \"pass@k\": pass_at_k,\n        \"pass^k\": pass_all_k,\n        \"success_rate\": sum(results) / len(results)\n    }\n```\n\n## Step 6: Interpret Results\n\nLook at both failure modes.\n\n### The Two-by-Two\n\n```\n                    ACTUAL\n                    Fire    Don't Fire\n              +-----------+-----------+\nSHOULD   Fire |    TP     |    FN     |\n             |  Correct  |  Missed   |\n             +-----------+-----------+\n  Don't Fire |    FP     |    TN     |\n             |   Noise   |  Correct  |\n             +-----------+-----------+\n\nPrecision = TP / (TP + FP)   \"When it fires, is it right?\"\nRecall    = TP / (TP + FN)   \"When it should fire, does it?\"\n```\n\n### What Each Pattern Means\n\n| Precision | Recall | Problem | Fix |\n|-----------|--------|---------|-----|\n| High | High | Working well | Expand dataset difficulty |\n| High | Low | Too conservative | Broaden activation triggers |\n| Low | High | Too aggressive | Tighten activation logic |\n| Low | Low | Fundamentally broken | Redesign |\n\nExample:\n\n```\nRust skill results:\n- Precision: 95% (fires on Rust → usually correct)\n- Recall: 60% (should fire on Rust → often misses)\n\nDiagnosis: Too conservative. Fires correctly but misses valid cases.\nAction: Add more trigger examples to training data.\n```\n\n### Look at Specific Failures\n\nDon't just look at aggregate scores. Read failed cases:\n\n```python\nfor case in failed_cases:\n    print(f\"Input: {case['input']}\")\n    print(f\"Expected: {case['expected']}\")\n    print(f\"Actual: {case['actual']}\")\n    print(f\"Why failed: {case['reason']}\\n\")\n```\n\nPatterns emerge: \"Always fails on multi-file changes\" or \"Misses error handling requirements.\"\n\n## Complete Example\n\nEvaluating a coding agent:\n\n```python\nfrom deepeval.metrics import TaskCompletionMetric\nfrom deepeval.test_case import LLMTestCase\nimport subprocess\n\n# Test cases from production bugs\ntest_cases = [\n    {\n        \"input\": \"Add input validation to user signup\",\n        \"context\": {\"repo\": \"auth-service\", \"file\": \"signup.py\"},\n        \"expected\": \"tests pass\",\n        \"label\": \"must_succeed\"\n    },\n    {\n        \"input\": \"Make this faster\",  # Vague request\n        \"context\": {\"repo\": \"auth-service\"},\n        \"expected\": \"clarification requested\",\n        \"label\": \"should_fail\"\n    }\n]\n\n# Code-based grader (primary)\ndef grade_with_tests(output):\n    result = subprocess.run([\"pytest\"], capture_output=True)\n    return result.returncode == 0\n\n# Model-based grader (secondary)\nmetric = TaskCompletionMetric(threshold=0.7)\n\n# Run eval with 5 trials per case\nresults = []\nfor case in test_cases:\n    trial_results = []\n    for _ in range(5):\n        output = agent.run(case[\"input\"], case[\"context\"])\n        success = grade_with_tests(output)\n        trial_results.append(success)\n\n    results.append({\n        \"case\": case[\"input\"],\n        \"pass@5\": any(trial_results),\n        \"pass^5\": all(trial_results),\n        \"rate\": sum(trial_results) / 5\n    })\n\n# Aggregate\ntotal_pass_at_5 = sum(r[\"pass@5\"] for r in results) / len(results)\ntotal_pass_all_5 = sum(r[\"pass^5\"] for r in results) / len(results)\n\nprint(f\"Pass@5: {total_pass_at_5:.1%}\")\nprint(f\"Pass^5: {total_pass_all_5:.1%}\")\n```\n\n## Summary Checklist\n\nBefore running evals:\n\n- [ ] Agent type identified (coding, conversational, research, computer use)\n- [ ] Primary grader selected (code-based preferred, model-based for flexibility)\n- [ ] Metrics chosen (F1 for binary, accuracy for tasks, tool correctness)\n- [ ] 20-50 test cases from actual failures\n- [ ] Test set balanced (positive AND negative cases)\n- [ ] Multiple trials per case (minimum 5)\n- [ ] Both pass@k and pass^k tracked (if non-deterministic)\n- [ ] Precision AND recall measured (not just one)\n- [ ] Failed cases reviewed individually (not just aggregate score)\n\nIf any box is unchecked, your eval will miss critical failure modes.\n"}],tutorials:[{slug:"evaluating-a-coding-agent",title:"Evaluating a Coding Agent",content:"# Evaluating a Coding Agent\n\nA guided walkthrough showing how to measure whether a bug-fixing agent actually works.\n\n## What You'll Learn\n\nBy the end of this tutorial, you'll understand the full evaluation lifecycle for a coding agent: designing test cases from real failures, building deterministic graders, measuring both initial performance and recovery capability, and tracking cost.\n\n## The Scenario\n\nYou've built a coding agent that takes a GitHub issue describing a Python bug, reads the codebase, and generates a patch. Does it work? How reliably? Can it recover when it fails?\n\nYou have 25 real bugs from production to test it against.\n\n## Phase 1: Define Tasks from Actual Failures\n\nDon't invent hypothetical test cases. Use real bugs that cost you time.\n\n### Create Test Cases\n\nEach test case needs three parts:\n\n```json\n{\n  \"id\": \"auth-001\",\n  \"input\": {\n    \"issue\": \"Token expiry check fails in production. Users with expired tokens still get 200 OK.\",\n    \"repo_path\": \"/path/to/repo\"\n  },\n  \"grader\": {\n    \"type\": \"test_suite\",\n    \"command\": \"pytest tests/test_auth.py::test_token_expiry -v\"\n  }\n}\n```\n\n### Why Start from Failures?\n\nProduction failures give you:\n- Real complexity (not toy examples)\n- Known ground truth (you fixed them manually)\n- Representativeness (what users actually hit)\n\nYour first 25 cases should be bugs you've already fixed. You know the correct fix exists because you wrote it.\n\n### Structure Your Dataset\n\n```python\n# test_cases.json\n[\n  {\n    \"id\": \"auth-001\",\n    \"description\": \"Token expiry validation\",\n    \"input\": {\n      \"issue\": \"Token expiry check fails...\",\n      \"repo_path\": \"./repos/auth-001\"\n    },\n    \"grader\": {\n      \"type\": \"test_suite\",\n      \"command\": \"pytest tests/test_auth.py -v\"\n    }\n  },\n  {\n    \"id\": \"db-017\",\n    \"description\": \"Connection pool leak\",\n    \"input\": {\n      \"issue\": \"Database connections not released...\",\n      \"repo_path\": \"./repos/db-017\"\n    },\n    \"grader\": {\n      \"type\": \"test_suite\",\n      \"command\": \"pytest tests/test_db.py -v\"\n    }\n  }\n  // ... 23 more cases\n]\n```\n\nEach repo is a clean checkout at the commit where the bug existed. Your agent generates a patch. The grader checks if tests pass.\n\n## Phase 2: Design Code-Based Graders\n\nGrade what the agent produced, not the path it took.\n\n### Test Suite Grader\n\nThe simplest and most reliable grader for code agents: run the test suite.\n\n```python\nimport subprocess\nimport json\n\ndef grade_code_fix(agent_output, test_case):\n    \"\"\"\n    Grade by running the test suite after applying the patch.\n    \"\"\"\n    repo_path = test_case[\"input\"][\"repo_path\"]\n    patch = agent_output[\"patch\"]\n\n    # Apply patch\n    with open(f\"{repo_path}/agent.patch\", \"w\") as f:\n        f.write(patch)\n\n    result = subprocess.run(\n        [\"git\", \"apply\", \"agent.patch\"],\n        cwd=repo_path,\n        capture_output=True\n    )\n\n    if result.returncode != 0:\n        return {\n            \"passed\": False,\n            \"score\": 0.0,\n            \"reason\": \"Patch failed to apply\",\n            \"details\": result.stderr.decode()\n        }\n\n    # Run tests\n    test_cmd = test_case[\"grader\"][\"command\"]\n    result = subprocess.run(\n        test_cmd.split(),\n        cwd=repo_path,\n        capture_output=True\n    )\n\n    passed = result.returncode == 0\n\n    return {\n        \"passed\": passed,\n        \"score\": 1.0 if passed else 0.0,\n        \"reason\": \"Tests passed\" if passed else \"Tests failed\",\n        \"details\": result.stdout.decode()\n    }\n```\n\n### Why Test Suites?\n\nDeterministic, fast, and objective. No model bias. No token cost for grading.\n\nIf the tests pass, the bug is fixed. If they fail, it's not.\n\n## Phase 3: Run with pass@5\n\nOne trial isn't enough. LLMs are stochastic. Run 5 trials per task.\n\n### Implementation\n\n```python\nimport asyncio\nimport json\n\nasync def run_pass_at_k(agent, test_cases, k=5):\n    \"\"\"\n    Run k independent trials per test case.\n    \"\"\"\n    results = []\n\n    for case in test_cases:\n        print(f\"Evaluating {case['id']}...\")\n\n        trials = []\n        for trial_num in range(k):\n            # Clean environment for each trial\n            reset_repo(case[\"input\"][\"repo_path\"])\n\n            # Run agent\n            output = await agent.run(\n                issue=case[\"input\"][\"issue\"],\n                repo_path=case[\"input\"][\"repo_path\"]\n            )\n\n            # Grade\n            grade = grade_code_fix(output, case)\n\n            trials.append({\n                \"trial\": trial_num + 1,\n                \"passed\": grade[\"passed\"],\n                \"score\": grade[\"score\"],\n                \"details\": grade[\"details\"]\n            })\n\n            print(f\"  Trial {trial_num + 1}: {'PASS' if grade['passed'] else 'FAIL'}\")\n\n        # Calculate metrics\n        successes = sum(1 for t in trials if t[\"passed\"])\n        pass_at_1 = trials[0][\"passed\"]\n        pass_at_k = successes >= 1  # At least one success\n\n        results.append({\n            \"case_id\": case[\"id\"],\n            \"trials\": trials,\n            \"pass@1\": pass_at_1,\n            \"pass@5\": pass_at_k,\n            \"success_count\": successes\n        })\n\n    return aggregate_results(results)\n\ndef aggregate_results(results):\n    \"\"\"Calculate overall metrics.\"\"\"\n    n = len(results)\n    pass_1_count = sum(1 for r in results if r[\"pass@1\"])\n    pass_k_count = sum(1 for r in results if r[\"pass@5\"])\n\n    return {\n        \"total_cases\": n,\n        \"pass@1\": pass_1_count / n,\n        \"pass@5\": pass_k_count / n,\n        \"results\": results\n    }\n```\n\n### Example Output\n\n```\nEvaluating auth-001...\n  Trial 1: FAIL\n  Trial 2: PASS\n  Trial 3: PASS\n  Trial 4: FAIL\n  Trial 5: PASS\n\nEvaluating db-017...\n  Trial 1: PASS\n  Trial 2: PASS\n  Trial 3: PASS\n  Trial 4: PASS\n  Trial 5: PASS\n\n... (23 more)\n\nOverall Results:\n├── pass@1: 60%  (15/25 cases)\n├── pass@5: 92%  (23/25 cases)\n└── Interpretation: Good exploration ceiling. Worth deploying with retry.\n```\n\n### What This Tells You\n\nPass@1 of 60% means baseline capability. Pass@5 of 92% means the agent can solve almost all bugs if you give it multiple attempts.\n\nThe gap between pass@1 and pass@5 tells you whether retry logic helps.\n\n## Phase 4: Add Iterative Evaluation\n\nTraditional pass@5 runs independent trials. Iterative evaluation uses failure feedback.\n\n### Why Iterative?\n\nIf the agent gets 60% pass@1, two questions:\n1. Is 60% its ceiling?\n2. Can it recover if you show it the test failures?\n\nIterative eval answers both.\n\n### Implementation\n\n```python\nasync def iterative_eval(agent, test_case, max_iterations=3):\n    \"\"\"\n    Run with retry and feedback.\n    \"\"\"\n    attempts = []\n    feedback = \"\"\n\n    for iteration in range(max_iterations):\n        # Build prompt with accumulated feedback\n        prompt = f\"Fix: {test_case['input']['issue']}\"\n        if feedback:\n            prompt += f\"\\n\\nPrevious attempt failed. Test output:\\n{feedback}\"\n\n        # Run agent\n        output = await agent.run(\n            issue=prompt,\n            repo_path=test_case[\"input\"][\"repo_path\"]\n        )\n\n        # Grade\n        grade = grade_code_fix(output, test_case)\n\n        attempts.append({\n            \"iteration\": iteration + 1,\n            \"passed\": grade[\"passed\"],\n            \"details\": grade[\"details\"]\n        })\n\n        if grade[\"passed\"]:\n            break\n\n        # Generate feedback for next iteration\n        feedback = grade[\"details\"]\n\n    # Calculate metrics\n    passed_at = next((i+1 for i, a in enumerate(attempts) if a[\"passed\"]), None)\n\n    return {\n        \"case_id\": test_case[\"id\"],\n        \"pass@1\": attempts[0][\"passed\"],\n        \"passed\": attempts[-1][\"passed\"],\n        \"iterations_to_pass\": passed_at,\n        \"iterations_run\": len(attempts),\n        \"attempts\": attempts\n    }\n```\n\n### Run Full Iterative Eval\n\n```python\nasync def full_iterative_eval(agent, test_cases, max_iterations=3):\n    results = []\n\n    for case in test_cases:\n        print(f\"Iterative eval: {case['id']}\")\n        reset_repo(case[\"input\"][\"repo_path\"])\n\n        result = await iterative_eval(agent, case, max_iterations)\n        results.append(result)\n\n        status = \"PASS\" if result[\"passed\"] else \"FAIL\"\n        iters = result[\"iterations_to_pass\"] or \"MAX\"\n        print(f\"  {status} (iterations: {iters})\")\n\n    return aggregate_iterative(results)\n\ndef aggregate_iterative(results):\n    n = len(results)\n\n    pass_1 = sum(1 for r in results if r[\"pass@1\"]) / n\n    pass_k = sum(1 for r in results if r[\"passed\"]) / n\n\n    iterations = [r[\"iterations_to_pass\"] for r in results if r[\"iterations_to_pass\"]]\n    avg_iterations = sum(iterations) / len(iterations) if iterations else None\n\n    recovery_rate = (pass_k - pass_1) / (1 - pass_1) if pass_1 \u003C 1 else 1.0\n\n    return {\n        \"pass@1\": pass_1,\n        \"pass@k_iterative\": pass_k,\n        \"recovery_rate\": recovery_rate,\n        \"avg_iterations_to_pass\": avg_iterations\n    }\n```\n\n### Example Output\n\n```\nIterative eval: auth-001\n  PASS (iterations: 2)\nIterative eval: db-017\n  PASS (iterations: 1)\n... (23 more)\n\nIterative Results:\n├── pass@1: 60%             (baseline)\n├── pass@k: 91%             (with retry + feedback)\n├── recovery_rate: 78%      (78% of failures recovered)\n└── avg_iterations: 1.6\n\nInterpretation: Agent can fix 91% of bugs when given test feedback.\n                Deploy with retry loop, not better prompts.\n```\n\n### Contrast: When Feedback Doesn't Help\n\n```\nIterative Results:\n├── pass@1: 60%\n├── pass@k: 63%             (minimal improvement)\n├── recovery_rate: 8%\n└── avg_iterations: 2.9\n\nInterpretation: Agent at capability ceiling. Retry won't help.\n                Need better model or different approach.\n```\n\nThis tells you retry logic won't save you. The agent needs fundamental improvements.\n\n## Phase 5: Track Cost\n\nEvaluation costs money. Track it.\n\n### Token Counting\n\n```python\nimport tokencost\n\ndef run_with_cost_tracking(agent, test_cases):\n    results = []\n    total_cost = 0.0\n\n    for case in test_cases:\n        # Count input tokens\n        input_cost = tokencost.calculate_prompt_cost(\n            prompt=case[\"input\"][\"issue\"],\n            model=\"claude-3-5-sonnet-20241022\"\n        )\n\n        # Run agent\n        output = await agent.run(case[\"input\"][\"issue\"])\n\n        # Count output tokens\n        output_cost = tokencost.calculate_completion_cost(\n            completion=output[\"patch\"],\n            model=\"claude-3-5-sonnet-20241022\"\n        )\n\n        case_cost = input_cost + output_cost\n        total_cost += case_cost\n\n        results.append({\n            \"case_id\": case[\"id\"],\n            \"cost\": case_cost,\n            \"output\": output\n        })\n\n        print(f\"{case['id']}: ${case_cost:.4f}\")\n\n    return {\n        \"results\": results,\n        \"total_cost\": total_cost,\n        \"avg_cost_per_case\": total_cost / len(test_cases)\n    }\n```\n\n### Budget for pass@k\n\nMultiple trials multiply cost.\n\n```python\n# For pass@5\ncost_per_trial = 0.08  # Average from initial run\ntrials = 5\ncases = 25\n\ntotal_cost = cost_per_trial * trials * cases\n# = 0.08 * 5 * 25\n# = $10.00\n\nprint(f\"Budget for pass@5 eval: ${total_cost:.2f}\")\n```\n\n### Set Guardrails\n\n```python\nMAX_COST_PER_CASE = 0.50  # $0.50 max per case\nMAX_TOTAL_BUDGET = 15.00  # $15 total\n\nrunning_cost = 0.0\n\nfor case in test_cases:\n    if running_cost >= MAX_TOTAL_BUDGET:\n        print(f\"Budget exceeded at ${running_cost:.2f}\")\n        break\n\n    case_cost = run_case_with_tracking(case)\n\n    if case_cost > MAX_COST_PER_CASE:\n        print(f\"Warning: {case['id']} cost ${case_cost:.4f} > limit\")\n\n    running_cost += case_cost\n```\n\n## What's Next\n\nYou've run the full eval lifecycle:\n1. Built 25 test cases from real bugs\n2. Created deterministic graders\n3. Measured pass@5 (exploration ceiling)\n4. Measured iterative capability (recovery with feedback)\n5. Tracked cost per run\n\n### Expand the Eval Suite\n\nAdd more test cases. The SWE-bench pattern uses 500 human-validated bugs. Start with 25. Scale to 100.\n\n### Add Model-Based Graders\n\nTest suites measure correctness. Add graders for code quality:\n\n```python\nfrom deepeval.metrics import GEval\n\ncode_quality = GEval(\n    name=\"Code Quality\",\n    criteria=\"Clean, readable, maintainable. No obvious bugs.\",\n    evaluation_steps=[\n        \"Check variable naming\",\n        \"Check function length\",\n        \"Check code duplication\",\n        \"Assess readability\"\n    ]\n)\n```\n\n### Set Up Continuous Evaluation\n\nRun evals on every commit. Track regressions.\n\n```yaml\n# .github/workflows/eval.yml\nname: Agent Eval\n\non: [push, pull_request]\n\njobs:\n  eval:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run eval suite\n        run: python eval/run_evals.py\n      - name: Check regression\n        run: python eval/check_regression.py\n```\n\n### Measure What Changed\n\nAfter improvements, re-run the eval. Did pass@1 improve? Did recovery rate change? Did cost go up?\n\nThe eval suite tells you whether changes made the agent better or worse.\n"},{slug:"first-eval-suite",title:"Building Your First Eval Suite",content:"# Building Your First Eval Suite\n\nA guided walkthrough showing how to measure skill quality from zero to production-ready.\n\n## What You'll Learn\n\nBy the end of this tutorial, you'll know how to build two-level evaluation for a Claude Code skill: activation testing with F1 scores and methodology rubrics.\n\n## The Scenario\n\nYou've created a Claude Code skill called `rust-patterns` that teaches Rust idioms. It has a `SKILL.md` file with patterns like \"prefer `?` operator for error propagation\" and \"use `impl Trait` for complex return types.\"\n\nThe skill exists. It activates sometimes. But you need answers to two questions:\n\n1. Does it trigger on the right prompts?\n2. When triggered, does it teach the methodology correctly?\n\nWithout measurement, you're blind.\n\n## Phase 1: Write Activation Test Cases\n\nActivation testing answers: \"Does this skill fire when it should, and stay quiet when it shouldn't?\"\n\nYou need a labeled test set. Each case has an expectation:\n- `must_trigger` - should definitely activate\n- `should_not_trigger` - must not activate\n- `acceptable` - either is fine\n\nCreate `plugins/rust-patterns/evals/activation-suite.json`:\n\n```json\n{\n  \"name\": \"rust-patterns-activation\",\n  \"version\": \"1.0.0\",\n  \"skill\": \"rust-patterns\",\n  \"description\": \"Activation test cases for rust-patterns skill\",\n  \"cases\": [\n    {\n      \"id\": \"must-001\",\n      \"prompt\": \"How do I handle errors in Rust?\",\n      \"expectation\": \"must_trigger\",\n      \"rationale\": \"Core Rust pattern question\"\n    },\n    {\n      \"id\": \"must-002\",\n      \"prompt\": \"What's the idiomatic way to return multiple types in Rust?\",\n      \"expectation\": \"must_trigger\",\n      \"rationale\": \"Asking for Rust idiom explicitly\"\n    },\n    {\n      \"id\": \"must-003\",\n      \"prompt\": \"Should I use Box\u003Cdyn Trait> or impl Trait?\",\n      \"expectation\": \"must_trigger\",\n      \"rationale\": \"Specific Rust pattern decision\"\n    },\n    {\n      \"id\": \"must-004\",\n      \"prompt\": \"Best practices for Rust error handling\",\n      \"expectation\": \"must_trigger\",\n      \"rationale\": \"Best practices = patterns\"\n    },\n    {\n      \"id\": \"must-005\",\n      \"prompt\": \"Explain Rust ownership patterns\",\n      \"expectation\": \"must_trigger\",\n      \"rationale\": \"Ownership is core Rust idiom\"\n    },\n    {\n      \"id\": \"not-001\",\n      \"prompt\": \"What's the best IDE for Rust?\",\n      \"expectation\": \"should_not_trigger\",\n      \"rationale\": \"Tooling question, not pattern/idiom\"\n    },\n    {\n      \"id\": \"not-002\",\n      \"prompt\": \"How do I install Rust?\",\n      \"expectation\": \"should_not_trigger\",\n      \"rationale\": \"Installation, not patterns\"\n    },\n    {\n      \"id\": \"not-003\",\n      \"prompt\": \"Write a Python script to parse JSON\",\n      \"expectation\": \"should_not_trigger\",\n      \"rationale\": \"Different language\"\n    },\n    {\n      \"id\": \"not-004\",\n      \"prompt\": \"Debug this TypeScript error\",\n      \"expectation\": \"should_not_trigger\",\n      \"rationale\": \"Different language\"\n    },\n    {\n      \"id\": \"not-005\",\n      \"prompt\": \"Explain the CAP theorem\",\n      \"expectation\": \"should_not_trigger\",\n      \"rationale\": \"Distributed systems theory, not Rust\"\n    },\n    {\n      \"id\": \"edge-001\",\n      \"prompt\": \"How do I write clean code?\",\n      \"expectation\": \"acceptable\",\n      \"rationale\": \"Too generic - could be Rust or general\"\n    }\n  ]\n}\n```\n\n### Why These Choices?\n\n**must_trigger cases**: Direct Rust pattern questions. \"How do I handle errors in Rust?\" is about idioms, the core of your skill. \"What IDE?\" is not.\n\n**should_not_trigger cases**: Installation, tooling, other languages. These filter out noise.\n\n**Balance matters**: 5 positive, 5 negative. Imbalanced sets let skills overfit.\n\n## Phase 2: Run the Activation Eval\n\nCreate `plugins/rust-patterns/evals/run_activation.py`:\n\n```python\nimport json\nimport anthropic\n\ndef check_activation(prompt, skill_description):\n    \"\"\"Check if skill would activate on this prompt.\"\"\"\n    client = anthropic.Anthropic()\n\n    response = client.messages.create(\n        model=\"claude-sonnet-4-5-20250929\",\n        max_tokens=50,\n        messages=[{\n            \"role\": \"user\",\n            \"content\": f\"\"\"You have a skill available: {skill_description}\n\nUser prompt: \"{prompt}\"\n\nWould this skill activate? Answer only YES or NO.\"\"\"\n        }]\n    )\n\n    answer = response.content[0].text.strip().upper()\n    return \"YES\" in answer\n\ndef calculate_metrics(results):\n    \"\"\"Calculate precision, recall, F1.\"\"\"\n    tp = sum(1 for r in results if r[\"should_trigger\"] and r[\"did_trigger\"])\n    fp = sum(1 for r in results if not r[\"should_trigger\"] and r[\"did_trigger\"])\n    fn = sum(1 for r in results if r[\"should_trigger\"] and not r[\"did_trigger\"])\n    tn = sum(1 for r in results if not r[\"should_trigger\"] and not r[\"did_trigger\"])\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"confusion_matrix\": {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn}\n    }\n\nwith open(\"activation-suite.json\") as f:\n    suite = json.load(f)\n\n# Read skill description from SKILL.md frontmatter\nwith open(\"../SKILL.md\") as f:\n    content = f.read()\n    # Extract description (simplified - real version would parse frontmatter)\n    skill_desc = \"Teach Rust idioms and patterns\"\n\nresults = []\nfor case in suite[\"cases\"]:\n    if case[\"expectation\"] == \"acceptable\":\n        continue  # Skip ambiguous cases\n\n    did_trigger = check_activation(case[\"prompt\"], skill_desc)\n    should_trigger = case[\"expectation\"] == \"must_trigger\"\n\n    results.append({\n        \"id\": case[\"id\"],\n        \"should_trigger\": should_trigger,\n        \"did_trigger\": did_trigger\n    })\n\n    status = \"✓\" if (should_trigger == did_trigger) else \"✗\"\n    print(f\"{status} {case['id']}: {case['prompt'][:50]}...\")\n\nmetrics = calculate_metrics(results)\nprint(f\"\\nPrecision: {metrics['precision']:.2%}\")\nprint(f\"Recall:    {metrics['recall']:.2%}\")\nprint(f\"F1:        {metrics['f1']:.2%}\")\n```\n\nRun it:\n\n```bash\ncd plugins/rust-patterns/evals\npython run_activation.py\n```\n\nOutput:\n\n```\n✓ must-001: How do I handle errors in Rust?...\n✓ must-002: What's the idiomatic way to return multiple type...\n✗ must-003: Should I use Box\u003Cdyn Trait> or impl Trait?...\n✓ must-004: Best practices for Rust error handling...\n✗ must-005: Explain Rust ownership patterns...\n✓ not-001: What's the best IDE for Rust?...\n✓ not-002: How do I install Rust?...\n✓ not-003: Write a Python script to parse JSON...\n✓ not-004: Debug this TypeScript error...\n✓ not-005: Explain the CAP theorem...\n\nPrecision: 100.00%\nRecall:    60.00%\nF1:        75.00%\n```\n\n## Phase 3: Diagnose and Fix\n\n### Reading the Results\n\n**Precision 100%**: When it fires, it's always right. No false positives.\n\n**Recall 60%**: It only catches 60% of valid Rust pattern questions. Missing 40%.\n\n**F1 75%**: Balanced score. Not bad, but below the 85% threshold for \"excellent.\"\n\n### The Problem\n\nThe skill is too narrow. \"Should I use Box\u003Cdyn Trait> or impl Trait?\" didn't trigger. Neither did \"Explain Rust ownership patterns.\"\n\nYour skill description is:\n\n```yaml\ndescription: \"Teach Rust idioms and patterns\"\n```\n\nThat's generic. The system doesn't know what \"idioms\" means.\n\n### The Fix\n\nExpand the description with concrete examples:\n\n```yaml\ndescription: |\n  Teach Rust idioms and patterns. Use when: error handling (? operator, Result,\n  custom errors), trait patterns (impl Trait, dyn Trait, associated types),\n  ownership patterns (borrowing, lifetimes, smart pointers), async patterns\n  (futures, tokio idioms), or general Rust best practices.\n```\n\n### Rerun the Eval\n\n```bash\npython run_activation.py\n```\n\nNew output:\n\n```\n✓ must-001: How do I handle errors in Rust?...\n✓ must-002: What's the idiomatic way to return multiple type...\n✓ must-003: Should I use Box\u003Cdyn Trait> or impl Trait?...\n✓ must-004: Best practices for Rust error handling...\n✓ must-005: Explain Rust ownership patterns...\n✓ not-001: What's the best IDE for Rust?...\n✓ not-002: How do I install Rust?...\n✓ not-003: Write a Python script to parse JSON...\n✓ not-004: Debug this TypeScript error...\n✓ not-005: Explain the CAP theorem...\n\nPrecision: 100.00%\nRecall:    100.00%\nF1:        100.00%\n```\n\nThe skill now activates on all valid cases and none of the invalid ones.\n\n## Phase 4: Add Methodology Rubric\n\nActivation testing is Level 1. It tells you IF the skill fires. It doesn't tell you if Claude follows the methodology when the skill is active.\n\nLevel 2 measures: \"Does Claude teach patterns correctly?\"\n\nCreate `plugins/rust-patterns/evals/methodology-rubric.json`:\n\n```json\n{\n  \"name\": \"rust-patterns-methodology\",\n  \"version\": \"1.0.0\",\n  \"skill\": \"rust-patterns\",\n  \"description\": \"Rubric for methodology adherence\",\n  \"rubric\": {\n    \"criteria\": [\n      {\n        \"name\": \"provides_idiom\",\n        \"weight\": 0.4,\n        \"description\": \"Does it provide the idiomatic Rust pattern?\",\n        \"levels\": {\n          \"0\": \"No pattern provided or wrong pattern\",\n          \"1\": \"Generic advice without Rust-specific idiom\",\n          \"2\": \"Correct Rust idiom but incomplete\",\n          \"3\": \"Complete Rust idiom with clear example\"\n        }\n      },\n      {\n        \"name\": \"explains_why\",\n        \"weight\": 0.35,\n        \"description\": \"Does it explain WHY this is idiomatic?\",\n        \"levels\": {\n          \"0\": \"No explanation\",\n          \"1\": \"Says 'this is idiomatic' without reasoning\",\n          \"2\": \"Explains benefit but not tradeoff\",\n          \"3\": \"Explains benefit, tradeoff, and context\"\n        }\n      },\n      {\n        \"name\": \"shows_alternatives\",\n        \"weight\": 0.25,\n        \"description\": \"Does it show when NOT to use this pattern?\",\n        \"levels\": {\n          \"0\": \"No mention of alternatives\",\n          \"1\": \"Mentions alternatives exist\",\n          \"2\": \"Shows alternative with basic comparison\",\n          \"3\": \"Shows when each approach fits\"\n        }\n      }\n    ]\n  }\n}\n```\n\n### Why These Criteria?\n\n**provides_idiom (40%)**: The primary job. If it doesn't teach the pattern, nothing else matters.\n\n**explains_why (35%)**: From cix principles: WHY > HOW. Understanding beats memorization.\n\n**shows_alternatives (25%)**: No pattern is universal. Knowing when NOT to use it is skill.\n\nWeights sum to 1.0.\n\n## Phase 5: Test Methodology\n\nCreate `plugins/rust-patterns/evals/run_methodology.py`:\n\n```python\nimport json\nimport anthropic\n\ndef score_criterion(response_text, criterion):\n    \"\"\"Use LLM to score response against criterion.\"\"\"\n    client = anthropic.Anthropic()\n\n    rubric_text = \"\\n\".join([\n        f\"{score}: {desc}\"\n        for score, desc in criterion[\"levels\"].items()\n    ])\n\n    judge = client.messages.create(\n        model=\"claude-sonnet-4-5-20250929\",\n        max_tokens=100,\n        messages=[{\n            \"role\": \"user\",\n            \"content\": f\"\"\"Score this response on: {criterion['description']}\n\nRubric:\n{rubric_text}\n\nResponse to score:\n{response_text}\n\nReturn only the score (0, 1, 2, or 3).\"\"\"\n        }]\n    )\n\n    score = int(judge.content[0].text.strip())\n    return score\n\nwith open(\"methodology-rubric.json\") as f:\n    rubric = json.load(f)\n\n# Test case: Ask Claude with rust-patterns skill active\ntest_prompt = \"How should I handle errors in Rust?\"\n\nclient = anthropic.Anthropic()\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=1000,\n    messages=[{\"role\": \"user\", \"content\": test_prompt}]\n)\n\nresponse_text = response.content[0].text\n\n# Score against rubric\ntotal_score = 0\nfor criterion in rubric[\"rubric\"][\"criteria\"]:\n    score = score_criterion(response_text, criterion)\n    weighted = score * criterion[\"weight\"]\n    total_score += weighted\n\n    print(f\"{criterion['name']}: {score}/3 (weight {criterion['weight']}) = {weighted:.2f}\")\n\nfinal_score = total_score / 3  # Normalize to 0-1\nprint(f\"\\nFinal Score: {final_score:.2%}\")\n```\n\nRun it:\n\n```bash\npython run_methodology.py\n```\n\nOutput:\n\n```\nprovides_idiom: 3/3 (weight 0.4) = 0.40\nexplains_why: 2/3 (weight 0.35) = 0.23\nshows_alternatives: 1/3 (weight 0.25) = 0.08\n\nFinal Score: 71.00%\n```\n\n### Reading the Results\n\n**provides_idiom**: Perfect. Claude showed the `?` operator pattern with example.\n\n**explains_why**: Good but not great. It explained the benefit (concise error propagation) but didn't mention the tradeoff (requires `From` implementation).\n\n**shows_alternatives**: Weak. It mentioned `match` exists but didn't say when to use it.\n\n71% is \"good\" but below 85% excellence threshold.\n\n### Improving the Skill\n\nThe methodology rubric reveals gaps. Your skill needs to emphasize:\n\n1. Always explain WHY, not just WHAT\n2. Show tradeoffs explicitly\n3. Mention when alternatives fit better\n\nUpdate `SKILL.md`:\n\n```markdown\n## Teaching Pattern\n\nWhen presenting a Rust idiom:\n\n1. Show the pattern with concrete example\n2. Explain WHY it's idiomatic (benefit + tradeoff)\n3. Show when NOT to use it (alternatives and their fit)\n\nExample:\n- Pattern: Use `?` operator for error propagation\n- Why: Concise, compiler-enforced error paths\n- Tradeoff: Requires `From` trait implementation\n- Alternative: `match` when you need custom handling per error\n```\n\nRerun the methodology eval. The score improves.\n\n## What's Next\n\nYou now have two-level evaluation:\n\n**Level 1 (Activation)**: F1 score tracking whether skill triggers correctly\n**Level 2 (Methodology)**: Rubric tracking teaching quality when active\n\n### Continuous Improvement\n\nRun these evals:\n- Before publishing the skill\n- After major changes to `SKILL.md`\n- When activation behavior seems wrong\n- Monthly to catch drift\n\nTrack metrics:\n\n| Date | F1 | Methodology | Notes |\n|------|----|-------------|-------|\n| 2026-02-09 | 100% | 71% | Initial baseline |\n| 2026-02-10 | 100% | 84% | Added tradeoff guidance |\n\n### Expand the Test Suite\n\nStart with 10 cases. Grow over time:\n- Add cases when you find prompts that should/shouldn't trigger\n- Add methodology test cases for different pattern types\n- Aim for 30-50 activation cases eventually\n\n### Version Your Evals\n\n```\nevals/\n├── activation-suite.json      # Current\n├── methodology-rubric.json    # Current\n├── v1/\n│   ├── activation-suite.json  # Previous version\n│   └── methodology-rubric.json\n└── CHANGELOG.md\n```\n\nWhen you change test suites, version them. You need to know if metric changes come from skill improvements or test changes.\n\n## Key Takeaways\n\n**Two-level testing**: Activation (F1) + methodology (rubric). Both matter.\n\n**Balanced test sets**: Equal positive and negative cases prevent overfitting.\n\n**Concrete descriptions**: \"Error handling, trait patterns, ownership\" beats \"Rust idioms.\"\n\n**Metrics reveal gaps**: 60% recall showed under-activation. 71% methodology showed weak alternative coverage.\n\n**Iterate with evidence**: Change description, rerun eval, measure improvement.\n\nThe skill you ship isn't the skill you write. It's the skill the evals prove works.\n"}]},docCount:8}},uses:{params:["slug"]}}}(Array(6)))],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
