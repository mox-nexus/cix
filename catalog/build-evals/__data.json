{"type":"data","nodes":[null,{"type":"data","data":[{"extension":1},{"slug":2,"kind":3,"manifest":4,"tagline":10,"readme":11,"components":12,"variant":15,"tags":16},"build-evals","plugin",{"name":2,"description":5,"version":6,"author":7},"Eval methodology for AI systems. Use when: writing evals for agents, skills, MCP servers, prompts, or measuring AI behavior.","0.2.1",{"name":8,"email":9},"yzavyas","yza.vyas@gmail.com","Eval methodology for AI systems. Write evals that measure what matters.","# build-evals\n\nEval methodology for AI systems. Write evals that measure what matters.\n\n## When to Use\n\n- Building test suites for agents, skills, MCP servers\n- Measuring agent effectiveness\n- Evaluating multi-agent coordination\n- Choosing eval frameworks\n\n## Skills\n\n- **build-eval**: Eval methodology covering grader types, metrics, frameworks\n\n## Key Concepts\n\n- **Three grader types**: Code-based, Model-based, Human\n- **Non-determinism**: pass@k, pass^k for stochastic outputs\n- **Match eval to agent type**: Coding, Conversational, Research, Computer Use, Multi-Agent\n",{"agents":13,"skills":14,"hooks":13,"commands":13},0,1,"emergence",[]],"uses":{"params":["slug"]}}]}
