<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../_app/immutable/assets/0.04mfEt8Y.css" rel="stylesheet">
		<link href="../_app/immutable/assets/CrossLinks.C4T6atsV.css" rel="stylesheet">
		<link href="../_app/immutable/assets/5.Bnuf7QZ4.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.D5y4usZ1.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CEnrcVot.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/iyO_HpW3.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_cEvoN5f.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D0iwhpLH.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Bhs-_H7e.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Cs0QLTHR.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.zelJeFlp.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BPYyHSBz.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/2hgU3PCP.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/C4-dLuTv.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_Dz5Oby6.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DtLL33Bf.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/a8oTfHeT.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CFsY3MpS.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.BxL1EdFy.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BwsKPAvP.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BbhyhsYm.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DZr9k4L0.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/5.yOPt9Hj_.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BFxfVKZg.js"><!--12qhfyh--><meta name="description" content="Extensions that enhance human capability, not replace it."/><!----><!--wzm642--><meta name="description" content="Prompt engineering for reasoning models, deep research, and paper synthesis. Evidence-backed techniques for different model architectures."/><!----><title>craft-prompts — cix</title>
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><a href="#main" class="skip-link">Skip to content</a> <!--[--><nav class="site-nav svelte-qgym72" aria-label="Site navigation"><a href="../" class="nav-wordmark svelte-qgym72">cix</a> <div class="nav-links svelte-qgym72"><!--[--><a href="../ethos" class="nav-link svelte-qgym72">ethos</a><a href="../catalog" class="nav-link svelte-qgym72">catalog</a><a href="../library" class="nav-link svelte-qgym72">library</a><!--]--></div></nav><!--]--> <div class="page svelte-12qhfyh has-nav"><!--[!--><!----><main id="main" class="detail-page svelte-wzm642" style="--variant-color: var(--ci-red)"><nav class="detail-back svelte-wzm642"><a href="../catalog" class="svelte-wzm642">← catalog</a></nav> <header class="detail-header svelte-wzm642"><div class="header-top svelte-wzm642"><h1 class="svelte-wzm642">craft-prompts</h1> <span class="detail-kind svelte-wzm642">plugin</span> <span class="detail-version svelte-wzm642">0.1.0</span></div> <p class="detail-description svelte-wzm642">Prompt engineering for reasoning models, deep research, and paper synthesis. Evidence-backed techniques for different model architectures.</p> <!--[--><div class="detail-inventory svelte-wzm642"><!--[--><span class="inv-item svelte-wzm642">3 skills</span><!--]--></div><!--]--> <!--[--><div class="detail-tags svelte-wzm642"><!--[--><span class="tag svelte-wzm642">prompt-engineering</span><span class="tag svelte-wzm642">reasoning</span><span class="tag svelte-wzm642">research</span><span class="tag svelte-wzm642">synthesis</span><!--]--></div><!--]--></header> <!--[--><nav class="detail-tabs svelte-wzm642" role="tablist"><!--[--><button role="tab" class="tab svelte-wzm642 active" aria-selected="true">README <!--[!--><!--]--></button><button role="tab" class="tab svelte-wzm642" aria-selected="false">Explanation <!--[--><span class="tab-count svelte-wzm642">2</span><!--]--></button><!--]--></nav><!--]--> <article class="detail-content svelte-wzm642 prose"><!--[--><!----><h1>craft-prompts</h1>
<p>Prompt engineering grounded in research. Different model architectures need different techniques.</p>
<h2>Skills</h2>
<table>
<thead>
<tr>
<th>Skill</th>
<th>Use When</th>
</tr>
</thead>
<tbody><tr>
<td><code>deep-reasoning</code></td>
<td>Prompting o1, o3, Gemini Deep Think</td>
</tr>
<tr>
<td><code>deep-research</code></td>
<td>AI-assisted research with citation accuracy</td>
</tr>
<tr>
<td><code>synthesize-papers</code></td>
<td>Multi-paper analysis and literature review</td>
</tr>
</tbody></table>
<h2>Philosophy</h2>
<ul>
<li><strong>Evidence over intuition</strong> — techniques backed by research</li>
<li><strong>Architecture-aware</strong> — what works for Claude ≠ what works for o1</li>
<li><strong>Composable</strong> — each skill is orthogonal</li>
</ul>
<h2>Sources</h2>
<p>Research from 2024-2026 including:</p>
<ul>
<li>Many-Shot ICL (ACL/NeurIPS 2024)</li>
<li>Chain-of-Verification (Meta Research)</li>
<li>Deep reasoning model techniques (o1, o3, Gemini Deep Think)</li>
</ul>
<!----><!--]--></article></main><!----><!--]--><!----></div> <div class="experimental-tag svelte-12qhfyh" aria-hidden="true">experimental</div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_14cdtw8 = {
						base: new URL("..", location).pathname.slice(0, -1),
						assets: "/cix"
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.D5y4usZ1.js"),
						import("../_app/immutable/entry/app.zelJeFlp.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 5],
							data: [null,(function(a){a[0]="prompt-engineering";a[1]="reasoning";a[2]="research";a[3]="synthesis";return {type:"data",data:{extension:{slug:"craft-prompts",kind:"plugin",manifest:{name:"craft-prompts",version:"0.1.0",description:"Prompt engineering for reasoning models, deep research, and paper synthesis. Evidence-backed techniques for different model architectures.",author:{name:"Mox Labs",email:"mox.rnd@gmail.com"},license:"MIT",keywords:a},tagline:"Prompt engineering grounded in research. Different model architectures need different techniques.",readme:"# craft-prompts\n\nPrompt engineering grounded in research. Different model architectures need different techniques.\n\n## Skills\n\n| Skill | Use When |\n|-------|----------|\n| `deep-reasoning` | Prompting o1, o3, Gemini Deep Think |\n| `deep-research` | AI-assisted research with citation accuracy |\n| `synthesize-papers` | Multi-paper analysis and literature review |\n\n## Philosophy\n\n- **Evidence over intuition** — techniques backed by research\n- **Architecture-aware** — what works for Claude ≠ what works for o1\n- **Composable** — each skill is orthogonal\n\n## Sources\n\nResearch from 2024-2026 including:\n- Many-Shot ICL (ACL/NeurIPS 2024)\n- Chain-of-Verification (Meta Research)\n- Deep reasoning model techniques (o1, o3, Gemini Deep Think)\n",components:{agents:0,skills:3,hooks:0,commands:0},variant:"constraint",tags:a,docs:{explanation:[{slug:"methodology",title:"Prompt Engineering: Methodology",content:"# Prompt Engineering: Methodology\n\nWhy these patterns exist and the research behind them.\n\n---\n\n## The Core Problem\n\nDifferent model architectures need different prompting techniques. What works for autoregressive models (Claude, GPT-4) fails for reasoning models (o1, Deep Think), and vice versa.\n\n| Model Type | How It Works | Prompting Approach |\n|------------|--------------|-------------------|\n| Autoregressive | Token-by-token generation | Guide the path (CoT, examples) |\n| Reasoning/RL | Internal search with verification | Constrain the search space |\n\n## Who This Is For\n\nYou're building a skill or prompt that needs to work across model architectures. You've noticed that the same prompt works brilliantly on Claude but fails on o1—or vice versa. This explains *why* that happens so you can pick the right technique for each architecture.\n\n---\n\n## Why Architecture Matters\n\n### Autoregressive Models (Claude, GPT-4)\n\nThese models generate sequentially. Each token depends on previous tokens. They benefit from:\n- Chain-of-thought (shows the path)\n- Examples (demonstrates patterns)\n- Structured output (guides format)\n\nThe prompt **leads** the generation.\n\n### Reasoning Models (o1, Deep Think)\n\nThese models have internal reinforcement learning. They explore solution spaces and verify internally. They need:\n- Destination definition (not path)\n- Hard constraints (electric fences)\n- Negative constraints (remove easy outs)\n\nThe prompt **bounds** the search.\n\n**Key insight:** Telling o1 \"think step by step\" is redundant—it already does. You're wasting tokens on instructions the model doesn't need.\n\n## Before/After: Same Task, Different Architecture\n\n### Autoregressive (Claude, GPT-4)\n\n**Before** (no guidance):\n```\nSummarize this research paper.\n```\n\n**After** (guided path):\n```\nSummarize this research paper. Structure your response as:\n1. Core claim (1 sentence)\n2. Key evidence (3 bullet points with specific findings)\n3. Limitations the authors acknowledge\n4. One thing the paper doesn't address\n```\n\n**Why it works:** Autoregressive models generate token-by-token. The structured prompt *leads* the sequential generation through a specific path. Without it, the model picks its own path—which may not be yours.\n\n### Reasoning (o1, Deep Think)\n\n**Before** (over-guided):\n```\nSummarize this research paper. Think step by step:\n1. First read the abstract\n2. Then identify the methodology\n3. Then extract findings\n4. Then synthesize\n```\n\n**After** (bounded search):\n```\nSummarize this research paper. Requirements:\n- Every claim must cite a specific section or figure\n- Under 200 words\n- No speculation beyond what the paper states\n- Flag if the sample size is under 100\n```\n\n**Why it works:** Reasoning models already think step-by-step internally. Telling them *how* to think is redundant—it wastes tokens and can degrade performance. Instead, define *what success looks like* and let the model find its own path within those bounds.\n\n---\n\n## Evidence Hierarchy\n\nAll technique recommendations follow this hierarchy:\n\n| Level | Source Type | Example |\n|-------|-------------|---------|\n| **Strong** | Peer-reviewed, replicated | Many-shot ICL paper (ACL/NeurIPS 2024) |\n| **Moderate** | Single quality study, converging evidence | Prompt repetition (Google, Dec 2025) |\n| **Weak** | Practitioner consensus, theoretical | Style analysis patterns |\n| **Speculative** | Reasonable inference | Combined technique effects |\n\nWhen evidence is limited, we say so.\n\n---\n\n## Key Research Findings\n\n### Many-Shot ICL (2024)\n\n**Finding:** Performance consistently improves with more examples, often peaking at 50-100 shots rather than the traditional 3-5.\n\n**Implication:** \"Few-shot\" is a misnomer. More examples = better, up to context limits.\n\n**Source:** [Many-Shot In-Context Learning](https://arxiv.org/abs/2404.11018) — ACL/NeurIPS 2024\n\n### Prompt Repetition (Dec 2025)\n\n**Finding:** Simply repeating the prompt (`\u003CQUERY>\u003CQUERY>`) improves accuracy. 47 wins out of 70 benchmark-model tests, 0 losses.\n\n**Why it works:** Causal models process sequentially—earlier tokens can't attend to later ones. Repetition allows full cross-attention.\n\n**Source:** [Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/abs/2512.14982) — Google Research\n\n### Chain-of-Verification (CoVe)\n\n**Finding:** Independent verification of claims reduces hallucination by 50-70%.\n\n**Why it works:** Decoupling verification from generation prevents confirmation bias.\n\n**Source:** Meta Research, ACL 2024\n\n### Style Transfer Limits\n\n**Finding:** LLMs have persistent \"fingerprints\"—noun-heavy, formal, underuse of discourse markers—that prompting can shift but not eliminate entirely.\n\n**Implication:** ~80% style match ceiling with prompting alone. Fine-tuning needed for higher fidelity.\n\n**Source:** Multiple papers on LLM stylistic fingerprints (2024-2025)\n\n---\n\n## Design Principles\n\n### 1. Technique Selection by Architecture\n\nDon't apply autoregressive techniques to reasoning models or vice versa.\n\n| Technique | Autoregressive | Reasoning |\n|-----------|---------------|-----------|\n| Chain-of-thought | Helps | Redundant |\n| Few-shot examples | Helps | Can hurt |\n| Detailed prompts | Helps | Can hurt |\n| Hard constraints | Optional | Essential |\n\n### 2. Evidence Over Intuition\n\nEvery technique claim should have:\n- The claim (what it does)\n- The effect size (how much)\n- The evidence level (how confident)\n- The source (where from)\n\n### 3. Show Uncertainty\n\nWhen evidence is limited:\n```\n✅ \"This is speculative—no direct research, but analogous findings suggest...\"\n❌ \"Research shows...\" (when it doesn't)\n```\n\n### 4. Composability\n\nSkills are orthogonal. Each covers one domain:\n- `deep-reasoning` — Reasoning model constraints\n- `deep-research` — Hallucination reduction, verification\n- `synthesize-papers` — Multi-paper analysis\n\nCombine as needed for complex tasks.\n\n---\n\n## The Verification Imperative\n\nClaims made without verification propagate errors. Every skill includes verification patterns:\n\n- **deep-research**: Chain-of-Verification (CoVe) for factual claims\n- **synthesize-papers**: Dual-LLM cross-critique for extraction\n\nVerification isn't optional—it's how you know the technique worked.\n\n---\n\nSee [sources.md](sources.md) for full bibliography.\n"},{slug:"sources",title:"Sources",content:"# Sources\n\nFull bibliography for craft-prompts techniques.\n\n---\n\n## Core Research\n\n### Many-Shot In-Context Learning\n- **Citation:** Agarwal et al. (2024). \"Many-Shot In-Context Learning.\" ACL/NeurIPS.\n- **URL:** https://arxiv.org/abs/2404.11018\n- **Key finding:** Performance improves consistently up to 50-100+ examples, contradicting \"few-shot\" assumptions.\n- **Used in:** deep-research\n\n### Chain-of-Verification (CoVe)\n- **Citation:** Dhuliawala et al. (2024). \"Chain-of-Verification Reduces Hallucination in Large Language Models.\" ACL.\n- **URL:** https://arxiv.org/abs/2309.11495\n- **Key finding:** 50-70% hallucination reduction through independent verification.\n- **Used in:** deep-research\n\n---\n\n## Reasoning Models\n\n### o1 Prompting Patterns\n- **Citation:** Lenny's Newsletter (2025). \"Prompt Engineering for o1.\"\n- **Key finding:** o1 performs worse with examples; over-detailed prompts counterproductive.\n- **Used in:** deep-reasoning\n\n### Role Prompting Effectiveness\n- **Citation:** Academic meta-analysis (2024).\n- **Key finding:** Role prompting has \"little to no effect on correctness\" for advanced models.\n- **Used in:** deep-reasoning\n\n---\n\n## Paper Synthesis\n\n### Dual-LLM Cross-Critique\n- **Citation:** (2024). Multi-document synthesis research.\n- **Key finding:** 0.94 accuracy on concordant extractions; 51% of discordant responses become concordant after cross-critique.\n- **Used in:** synthesize-papers\n\n### Plan-Based Synthesis (LitLLMs)\n- **Citation:** (2024). LitLLMs paper.\n- **Key finding:** Plan-first synthesis outperforms direct generation, fewer hallucinations.\n- **Used in:** synthesize-papers\n\n### Claimify Pipeline\n- **Citation:** (2024). Claim extraction research.\n- **Key finding:** 99% claim entailment through selection → disambiguation → decomposition.\n- **Used in:** synthesize-papers\n\n---\n\n## Platform Research\n\n### Hallucination Rates\n- **Citation:** Nature Communications (2025), Omniscience Index, Deakin University studies.\n- **Key finding:** 50-90% of LLM responses not fully supported by cited sources.\n- **Used in:** deep-research\n\n### Citation Accuracy\n- **Citation:** Multiple sources (2025).\n- **Key finding:** Claude ~90% precision, Gemini 66% DOI errors, OpenAI 62% claim-sourced.\n- **Used in:** deep-research\n\n---\n\n## Prompt Engineering Guides\n\n### General\n- [Prompt Engineering Guide](https://www.promptingguide.ai/) — Comprehensive techniques reference\n- [Lakera Prompt Engineering Guide 2026](https://www.lakera.ai/blog/prompt-engineering-guide) — Current best practices\n\n### Chain-of-Thought\n- [Chain-of-Thought Prompting](https://www.promptingguide.ai/techniques/cot) — CoT technique documentation\n\n---\n\n## Meta-Research\n\n### 1,500 Papers Analysis\n- **Citation:** Aakash Gupta analysis (2024).\n- **Key finding:** Over-detailed prompts counterproductive with sophisticated models.\n- **Used in:** deep-reasoning\n\n---\n\n*Last updated: February 2026*\n"}],"how-to":[],tutorials:[]},docCount:2}},uses:{params:["slug"]}}}(Array(4)))],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
